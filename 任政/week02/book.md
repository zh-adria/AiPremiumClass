# 线性回归

* y = ax + b ; b 为偏置，a为斜率（直线的倾斜方向，通常是一组矩阵），x是自变量
* 最小二乘法：找到一组参数a跟b，使得真实值与预测值误差平方和最小，那么就得对a和b分别求偏导，另偏导数=0。
* 对于多元线性回归的参数估计：权重参数的矩阵点积自变量的转置 + 偏置

# 逻辑回归

* 将线性回归模型映射为概率【0,1】区间的模型
* 我们使用激活函数sigmoid函数来进行转换
* ```
  def forward(x , theta , bias):
      # 求线性函数的因变量值
      z = np.dot(theta,x.T) + bias
      # 求预测概率值用Sigmoid激活函数  (0,1)区间
      y_hat = 1 / (1 + np.exp(-z))
      return y_hat
  ```

# 概率与似然

* 概率是已知一些概率分布参数的情况下，观测预测值的结果
* 似然 是用于已知某些观测值所得到的结果时，对观测结果的概率分布进行估计
* 当我们讨论具体的样本数据时，每个样本点都有一个自变量x与因变量y，而x为观测值是具体的数值。
* 如果 权重参数已知，x为变量，就叫概率函数 （对于不用的样本点x出现的概率为多少）
* 如果 x为已知，权重参数为变量，就叫似然函数 （对于不用的模型参数， 出现x这个样本点的概率为多少）
* 最大似然估计：就是用已知的x来估计权重参数与偏置的可能性，使得真实值与预测值中的损失最小，通常用对数似然函数来作为损失函数
* ```
  def loss_function(y , y_hat):
      e = 1e-8 # 防止y_hat为0
      return -y * np.log(y_hat + e) - (1 - y) * np.log(1 - y_hat + e)
  ```

# 梯度下降

* 梯度下降就是使损失函数最小，
* 学习率为超参数（需要根据经验手动设置，一般0.1或0.01等），为下降的步长。
* 更新参数时，原参数 - 学习率 * 梯度
* 如果学习率大，下降的快，下降的次数少，但是可能会越过最小值
* 如果学习率小，下降的慢，下降的次数多，消耗的资源会多
* 实际上就是求最合适的权重参数以及偏置
* ```
  # 计算梯度
  def calc_gradient(x,y,y_hat):
      # m取x的最后一维的数量
      m = x.shape[-1]
      # 求平均梯度
      delta_w  = np.dot(y_hat-y,x) / m
      # 计算误差的均值作为偏置的梯度
      delta_b = np.mean(y_hat - y)
      return delta_w , delta_b
  ```