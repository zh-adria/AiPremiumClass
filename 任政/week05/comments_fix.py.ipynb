{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 修复文件  保证书名跟书评是同一行\n",
    "\n",
    "# 创建修复后内容的保存文件\n",
    "fixed = open(\"douban_comments_fixed.txt\" , \"w\" , encoding = \"utf-8\")\n",
    "\n",
    "# 遍历修复前内容文件\n",
    "lines = [line for line in open(\"doubanbook_top250_comments.txt\",\"r\" , encoding = \"utf-8\")]\n",
    "\n",
    "# print(len(lines))\n",
    "\n",
    "for i,line in enumerate(lines):\n",
    "    # 保存标题列\n",
    "    if i == 0 :\n",
    "        fixed.write(line)\n",
    "        prev_line = ' ' #  上一行的书名置为空\n",
    "        continue\n",
    "    # 根据制表符/t 进行拆分  提取书名和评论文本\n",
    "    terms = line.split(\"\\t\")\n",
    "\n",
    "    # 判断当前行的书名 是否等于 上一行的书名\n",
    "    if terms[0] == prev_line.split(\"\\t\")[0]:\n",
    "        if len(prev_line.split(\"\\t\")) == 6:  # 新书评论\n",
    "            # 保存上一行的记录，直接写入 fixed\n",
    "            fixed.write(prev_line + '\\n')\n",
    "            prev_line = line.strip() # 保存当前行\n",
    "        else:\n",
    "            prev_line = \"\"\n",
    "    else:\n",
    "        if len(terms) == 6:\n",
    "            prev_line = line.strip() # 保存当前行\n",
    "        else:\n",
    "            prev_line += line.strip() # 合并当前行与上一行\n",
    "fixed.close()"
   ],
   "id": "bdced50830aac14f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:36:31.767250Z",
     "start_time": "2025-04-05T01:36:31.001481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#  2、使用自定义的文档文本，通过fasttext训练word2vec训练词向量模型，并计算词汇间的相关度。\n",
    "\n",
    "import jieba\n",
    "import fasttext\n",
    "# 文档分词预处理\n",
    "# with open(\"shz.txt\" , \"r\",encoding = \"utf-8\" ) as f :\n",
    "#     lines = f.read()\n",
    "#\n",
    "# with open(\"shz_sp.txt\" ,\"w\", encoding = \"utf-8\") as f :\n",
    "#     f.write(\" \" . join(jieba.cut(lines)))\n",
    "\n",
    "model = fasttext.train_unsupervised(\"shz_sp.txt\",model = \"skipgram\")\n",
    "\n",
    "print(len(model.words))\n",
    "\n",
    "# 获取词向量的最近邻\n",
    "print(model.get_nearest_neighbors(\"武松\"))\n",
    "\n",
    "print(\"-------------\")\n",
    "# 词汇间的类比\n",
    "print(model.get_analogies('宋江' , '鲁智深'  , '林冲'))\n"
   ],
   "id": "6e95d2fe19fc8599",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "[(0.8185369372367859, '，'), (0.8025778532028198, '。'), (0.7752787470817566, '、'), (0.7728549242019653, '梁山'), (0.7717704176902771, '好汉'), (0.7697096467018127, '在'), (0.7623936533927917, '宋江'), (0.7565146684646606, '鲁智深'), (0.7500678896903992, '的'), (0.6883617043495178, '人')]\n",
      "-------------\n",
      "[(0.4294202923774719, '在'), (0.40036338567733765, '。'), (0.3922351896762848, '的'), (0.38388118147850037, '也'), (0.3833412826061249, '武松'), (0.38171130418777466, '，'), (0.3669948875904083, '了'), (0.3530404567718506, '梁山'), (0.3516506552696228, '、'), (0.3049568235874176, '人')]\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T00:33:22.740876Z",
     "start_time": "2025-04-05T00:33:15.739172Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[65], line 34\u001B[0m\n\u001B[0;32m     31\u001B[0m stop_words \u001B[38;5;241m=\u001B[39m [line\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstopwords.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m , \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m , encoding \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)]\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# 加载图书评论信息\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m book_comments \u001B[38;5;241m=\u001B[39m load_data(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdouban_comments_fixed.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# print(len(book_comments))\u001B[39;00m\n\u001B[0;32m     36\u001B[0m \n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m# 提取书名\u001B[39;00m\n\u001B[0;32m     38\u001B[0m book_names \u001B[38;5;241m=\u001B[39m []\n",
      "Cell \u001B[1;32mIn[65], line 16\u001B[0m, in \u001B[0;36mload_data\u001B[1;34m(filename)\u001B[0m\n\u001B[0;32m     13\u001B[0m reader \u001B[38;5;241m=\u001B[39m csv\u001B[38;5;241m.\u001B[39mDictReader(f , delimiter \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m )\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# 提取书名与评论信息 并且将评论信息进行分词\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m reader:\n\u001B[0;32m     17\u001B[0m     book \u001B[38;5;241m=\u001B[39m item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbook\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     18\u001B[0m     comment \u001B[38;5;241m=\u001B[39m item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32mD:\\Code\\Conda\\py312\\envs\\Mathematics_of_Deep_Learning___Using_Python\\Lib\\csv.py:116\u001B[0m, in \u001B[0;36mDictReader.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mline_num \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;66;03m# Used only for its side effect.\u001B[39;00m\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfieldnames\n\u001B[1;32m--> 116\u001B[0m row \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreader)\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mline_num \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreader\u001B[38;5;241m.\u001B[39mline_num\n\u001B[0;32m    119\u001B[0m \u001B[38;5;66;03m# unlike the basic reader, we prefer not to return blanks,\u001B[39;00m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;66;03m# because we will typically wind up with a dict full of None\u001B[39;00m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;66;03m# values\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Code\\Conda\\py312\\envs\\Mathematics_of_Deep_Learning___Using_Python\\Lib\\codecs.py:319\u001B[0m, in \u001B[0;36mBufferedIncrementalDecoder.decode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_buffer_decode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, errors, final):\n\u001B[0;32m    315\u001B[0m     \u001B[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001B[39;00m\n\u001B[0;32m    316\u001B[0m     \u001B[38;5;66;03m# and return an (output, length consumed) tuple\u001B[39;00m\n\u001B[0;32m    317\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m\n\u001B[1;32m--> 319\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    320\u001B[0m     \u001B[38;5;66;03m# decode input (taking the buffer into account)\u001B[39;00m\n\u001B[0;32m    321\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer \u001B[38;5;241m+\u001B[39m \u001B[38;5;28minput\u001B[39m\n\u001B[0;32m    322\u001B[0m     (result, consumed) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer_decode(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merrors, final)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 65,
   "source": [
    "# 计算 TF-IDF并通过预选相似度给出的推荐列表\n",
    "import csv\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer   # 特征提取\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # 余弦相似度\n",
    "import numpy as np\n",
    "\n",
    "def load_data(filename):\n",
    "    # 图书评论信息集合{书名：\"评论1+评论2+。。。。\"}\n",
    "    book_comments = {}\n",
    "    with open(filename , 'r' , encoding = 'utf-8') as f :\n",
    "        # 识别格式文本中的标题列 csv.DictReader , delimiter   以制表符作为文件的分隔符\n",
    "        reader = csv.DictReader(f , delimiter = \"\\t\" )\n",
    "\n",
    "        # 提取书名与评论信息 并且将评论信息进行分词\n",
    "        for item in reader:\n",
    "            book = item['book']\n",
    "            comment = item['body']\n",
    "            comment_words = jieba.lcut(comment)\n",
    "\n",
    "            if book == ' ' : continue # 跳过空书名\n",
    "\n",
    "            # 将图书评论收集，一个书名对应该书名的全部评论\n",
    "            book_comments[book] = book_comments.get(book , [])\n",
    "            # 加上后续评论\n",
    "            book_comments[book].extend(comment_words)\n",
    "    return book_comments\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 加载停用词列表\n",
    "    stop_words = [line.strip() for line in open(\"stopwords.txt\" , 'r' , encoding = \"utf-8\")]\n",
    "\n",
    "    # 加载图书评论信息\n",
    "    book_comments = load_data(\"douban_comments_fixed.txt\")\n",
    "    # print(len(book_comments))\n",
    "\n",
    "    # 提取书名\n",
    "    book_names = []\n",
    "    book_comms = []\n",
    "    for book , comments in book_comments.items():\n",
    "        book_names.append(book)\n",
    "        book_comms.append(comments)\n",
    "    # 构建TF-IDF特征矩阵\n",
    "    vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "    tfidf_matrix = vectorizer.fit_transform([\" \" . join(comms) for comms in book_comms])\n",
    "\n",
    "    # 计算图书之间的余弦相似度\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "\n",
    "    # 输入要推荐的图书名称\n",
    "    book_list = list(book_comments.keys())\n",
    "    print(book_list)\n",
    "\n",
    "    book_name = input(\"请输入图书名称：\")\n",
    "    book_idx = book_names.index(book_name) # 获取图书索引\n",
    "\n",
    "    # 获取与输入图书最相似的图书 输入图书除外（第一本）\n",
    "    # argsort  返回升序索引值，所以similarity_matrix 前面加一个符号，用于降序\n",
    "    recommend_book_index = np.argsort(-similarity_matrix[book_idx])[1:11]\n",
    "    print(f\"推荐的十种书：\")\n",
    "    # 输出推荐的图书\n",
    "    for idx in recommend_book_index:\n",
    "        print(f\"《{book_names[idx]}》 \\t 相似度：{similarity_matrix[book_idx][idx]:.4f}\")\n",
    "    print()\n"
   ],
   "id": "4ef7b425c3b20eb2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T06:11:41.922731Z",
     "start_time": "2025-04-06T06:11:14.490081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 作业1\n",
    "import csv\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from bm25_code import bm25\n",
    "\n",
    "# 对评论进行分词并且收集所有书名的评论汇总到一起\n",
    "def load_data(filename):\n",
    "    book_comments = {}\n",
    "\n",
    "    with open(filename , 'r' , encoding = \"utf-8\") as f:\n",
    "        # 识别格式\n",
    "        reader = csv.DictReader(f , delimiter = \"\\t\",)\n",
    "        for item in reader:\n",
    "            book = item['book']\n",
    "            comments = item['body']\n",
    "            # 利用jieba的lcut 进行分词处理\n",
    "            comments_words = jieba.lcut(comments)\n",
    "            if book == '' : continue  # 跳过空书名\n",
    "            # 评论集合\n",
    "            book_comments[book] = book_comments.get(book,[])\n",
    "            book_comments[book].extend(comments_words)\n",
    "    return book_comments\n",
    "\n",
    "# 判断用模型训练tfidf 还是 bm25\n",
    "def comments_vectors_similarity(book_comms , method = 'tfidf'):\n",
    "    global matrix\n",
    "    if method == 'tfidf':\n",
    "        # 构建TF-IDF特征矩阵\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        matrix = vectorizer.fit_transform([''.join(comms) for comms in book_comms])\n",
    "    if method == 'dm25':\n",
    "        matrix = bm25(book_comms)\n",
    "    # 计算图书余弦相似度\n",
    "    similarity_matrix = cosine_similarity(matrix)\n",
    "    return similarity_matrix\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # 加载停用词列表\n",
    "    stop_words = [line.strip() for line in open(\"stopwords.txt\" , 'r' , encoding = \"utf-8\")]\n",
    "\n",
    "\n",
    "    # 加载图书评论信息\n",
    "    book_comments = load_data('douban_comments_fixed.txt')\n",
    "\n",
    "    # 提取书名和评论文本\n",
    "    book_names = []\n",
    "    book_comms = []\n",
    "    for book,comments in book_comments.items():\n",
    "        book_names.append(book)\n",
    "        book_comms.append(comments)\n",
    "\n",
    "    # TF-IDF 算法得到的相似度矩阵\n",
    "    tfidf_matrix = comments_vectors_similarity(book_comms , method = 'tfidf')\n",
    "\n",
    "    bm25_matrix = comments_vectors_similarity(book_comms , method = 'bm25')\n",
    "\n",
    "\n",
    "    # 输入要推荐的图书名称\n",
    "    book_list = list(book_comments.keys())\n",
    "    print(book_list)\n",
    "    book_name = input(\"请输入图书名称：\")\n",
    "    book_idx = book_names.index(book_name)\n",
    "\n",
    "    # 获取与输入图书最相似的图书\n",
    "    print(f\"TF-IDF算法推荐的图书：\\n\")\n",
    "    recommend_book_index = np.argsort(-tfidf_matrix[book_idx])[1:11]\n",
    "    # 输出推荐的图书\n",
    "    for idx in recommend_book_index:\n",
    "        print(f\"《{book_names[idx]}》 \\t 相似度：{tfidf_matrix[book_idx][idx]:.4f}\")\n",
    "    print()\n",
    "\n",
    "    print(f\"BM25算法推荐的图书：\\n\")\n",
    "    recommend_book_index = np.argsort(-bm25_matrix[book_idx])[1:11]\n",
    "    # 输出推荐的图书\n",
    "    for idx in recommend_book_index:\n",
    "        print(f\"《{book_names[idx]}》 \\t 相似度：{bm25_matrix[book_idx][idx]:.4f}\")\n",
    "\n"
   ],
   "id": "bf688bfd63c04c0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['天才在左 疯子在右', '1Q84 BOOK 1', '悲伤逆流成河', '恶意', 'Harry Potter and the Deathly Hallows', '长安乱', '苏菲的世界', '许三观卖血记', '1995-2005夏至未至', '盗墓笔记', '霍乱时期的爱情', '三生三世 十里桃花', '基督山伯爵', '小时代1.0折纸时代', '洛丽塔', '1Q84 BOOK 2', '第一次的亲密接触', '神雕侠侣', '一座城池', '茶花女', '当我谈跑步时我谈些什么', '明朝那些事儿（贰）', '人类简史', '一個人住第5年', '明朝那些事儿（肆）', '寻路中国', '我们台湾这些年', '1Q84 BOOK 3', '摆渡人', '明朝那些事儿（伍）', '骆驼祥子', '盗墓笔记3', '麦琪的礼物', '格林童话全集', '水仙已乘鲤鱼去', '历史深处的忧虑', '金锁记', '草样年华', '刀锋', '飞鸟集', '七夜雪', '最初的爱情 最后的仪式', '拆掉思维里的墙', '明朝那些事儿（陆）', '追风筝的人', '小王子', '围城', '解忧杂货店', '活着', '白夜行', '挪威的森林', '嫌疑人X的献身', '三体', '不能承受的生命之轻', '红楼梦', '梦里花落知多少', '达·芬奇密码', '看见', '百年孤独', '1988：我想和这个世界谈谈', '何以笙箫默', '平凡的世界（全三部）', '简爱', '哈利·波特与魔法石', '三体Ⅱ', '飘', '送你一颗子弹', '三体Ⅲ', '傲慢与偏见', '倾城之恋', '三重门', '杜拉拉升职记', '明朝那些事儿（壹）', '哈利·波特与阿兹卡班的囚徒', '目送', '情人', '哈利·波特与密室', '万历十五年', '我们仨', '幻城', '致我们终将逝去的青春', '狼图腾', '微微一笑很倾城', '莲花', '哈利·波特与火焰杯', '边城', '月亮和六便士', '向左走·向右走', '穆斯林的葬礼', '从你的全世界路过', '天龙八部', '放学后', '哈利·波特与混血王子', '一个人的好天气', '哈利·波特与凤凰社', '喜宝', '海边的卡夫卡', '文化苦旅', '窗边的小豆豆', '三国演义（全二册）', '黄金时代', '悟空传', '兄弟（上）', '呼啸山庄', '笑傲江湖（全四册）', '少有人走的路', '民主的细节', '亲爱的安德烈', '灿烂千阳', '老人与海', '遇见未知的自己', '一九八四·动物农场', '牧羊少年奇幻之旅', '福尔摩斯探案全集（上中下）', '素年锦时', '情书', '他的国', '彼岸花', '西决', '东方快车谋杀案', '这些都是你给我的爱', '这些人，那些事', '八月未央', '清醒纪', '一个陌生女人的来信', '蔡康永的说话之道', '偷影子的人', '陪安东尼度过漫长岁月', '沉默的大多数', '白鹿原', '芒果街上的小屋', '羊脂球', '鲁滨逊漂流记', '灌篮高手31', '撒哈拉的故事', '巴黎圣母院', '肖申克的救赎', '麦田里的守望者', '无声告白', '山楂树之恋', '华胥引（全二册）', '地下铁', '且听风吟', '钢铁是怎样炼成的', '红玫瑰与白玫瑰', '人生若只如初见', '人间失格', '鬼吹灯之精绝古城', '安徒生童话故事集', '呐喊', '小团圆', '泡沫之夏', '会有天使替我爱你', '1984', '年华是无效信', '幻夜', '在路上', '射雕英雄传（全四册）', '明朝那些事儿（1-9）', '月亮忘記了', '明朝那些事儿（叁）', '哭泣的骆驼', '原来你还在这里', '半生缘', '此间的少年', '货币战争', '佳期如梦', '无人生还', '了不起的盖茨比', '时间旅行者的妻子', '告别薇安', '常识', '爱你就像爱生命', '步步惊心', '皮囊', '二三事', '兄弟（下）', '孤独六讲', '乌合之众', '盗墓笔记2', '失恋33天', '动物农场', '左耳', '鹿鼎记（全五册）', '荆棘鸟', '左手倒影，右手年华。', '零下一度', '像少年啦飞驰', '被窝是青春的坟墓', '关于莉莉周的一切', '机器猫哆啦A梦23', '阿Q正传', '乖，摸摸头', '大地之灯', '如何阅读一本书', '当我们谈论爱情时我们在谈论什么', '尘埃落定', '东霓', '海贼王', '那些回不去的年少时光', '孩子你慢慢来', '橙', '悲惨世界（上中下）', '盗墓笔记4', '巴别塔之犬', '香水', '一只特立独行的猪', '局外人', '一个人的朝圣', '史蒂夫·乔布斯传', '看不见的城市', '长恨歌', '匆匆那年（上下）', '蔷薇岛屿', '我的路', '菊与刀', '球状闪电', '谁动了我的奶酪？', '曾有一个人，爱我如生命', '那些年，我们一起追的女孩', '伊豆的舞女', '世界尽头与冷酷仙境', '鬼吹灯之云南虫谷', '明朝那些事儿（柒）：大结局', '把时间当作朋友', '秘密', '天使与魔鬼', '佛祖在一号线', '倚天屠龙记(共四册)', '阿狸·梦之城堡', '杜拉拉2华年似水', '不朽']\n",
      "TF-IDF算法推荐的图书：\n",
      "\n",
      "《三体》 \t 相似度：0.0211\n",
      "《球状闪电》 \t 相似度：0.0144\n",
      "《三体Ⅲ》 \t 相似度：0.0110\n",
      "《万历十五年》 \t 相似度：0.0107\n",
      "《三体Ⅱ》 \t 相似度：0.0101\n",
      "《天才在左 疯子在右》 \t 相似度：0.0096\n",
      "《寻路中国》 \t 相似度：0.0088\n",
      "《苏菲的世界》 \t 相似度：0.0080\n",
      "《拆掉思维里的墙》 \t 相似度：0.0075\n",
      "《孤独六讲》 \t 相似度：0.0070\n",
      "\n",
      "BM25算法推荐的图书：\n",
      "\n",
      "《三体》 \t 相似度：0.0211\n",
      "《球状闪电》 \t 相似度：0.0144\n",
      "《三体Ⅲ》 \t 相似度：0.0110\n",
      "《万历十五年》 \t 相似度：0.0107\n",
      "《三体Ⅱ》 \t 相似度：0.0101\n",
      "《天才在左 疯子在右》 \t 相似度：0.0096\n",
      "《寻路中国》 \t 相似度：0.0088\n",
      "《苏菲的世界》 \t 相似度：0.0080\n",
      "《拆掉思维里的墙》 \t 相似度：0.0075\n",
      "《孤独六讲》 \t 相似度：0.0070\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T01:03:06.162942Z",
     "start_time": "2025-04-05T01:02:58.739475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. 使用课堂示例cooking.stackexchange.txt，使用fasttext训练文本分类模型。\n",
    "\n",
    "import fasttext\n",
    "\n",
    "model = fasttext.train_supervised('cooking.stackexchange.txt',epoch = 30 , dim = 100)\n",
    "\n",
    "# print(model.words)\n",
    "\n",
    "# 返回预测概率最高的标签\n",
    "print(model.predict('What kind of tea do you boil for 45minutes?'))\n",
    "print(model.predict('Good breads for evening cooking?'))\n",
    "\n",
    "# 保存模型\n",
    "model.save_model('model_file.bin' )\n",
    "\n",
    "# 获取模型\n",
    "model = fasttext.load_model('model_file.bin')"
   ],
   "id": "2a0510f38574ac9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__tea',), array([0.38441685]))\n",
      "(('__label__bread',), array([0.85953915]))\n"
     ]
    }
   ],
   "execution_count": 66
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
