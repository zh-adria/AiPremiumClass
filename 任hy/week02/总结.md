# 逻辑回归

机器学习算法中的监督式学习可以分两类：

[^监督式学习]: 所谓监督式学习，就是模型从带有标签的数据中学习输入（特征）与输出（标签）之间的映射关系，如输入房屋面积、位置、房龄，输出房价（回归任务）或房屋类型（分类任务）。此外还有无监督（数据无标签），半监督（部分有标签），强化学习（数据通过试错生成）

1. 分类模型：目标变量是分类变量（离散值）
2. 回归模型：目标变量是连续性数值变量

逻辑回归用于解决**分类问题**，即01模型。逻辑回归模型得到的计算结果是介于0~1之间的连续数字，可以称他为可能性，因此实际上过程上是“回归”的，但是人为限制某个阈值来将其变成“分类”的（类似代码中的`np.round`）。

例如：贷款不还的可能性大于0.5，将其标签定位违约客户（1），而小于0.5的标签定位不违约客户（0）

因此我们可以看到，逻辑回归的第一步是对样本的特征进行线性回归分析

### 线性回归

线性回归结果的选择标准是由损失函数决定的，即为什么选了A作为线性回归方程而不选B，就是因为A比B的损失函数小，对于样本的拟合度更高。

在这里还是先把线性回归的知识过一下。一元线性回归的方程损失函数通过残差平方和计算。

[^残差平方和]: ![image-20250306170220835](C:\Users\Renhy\AppData\Roaming\Typora\typora-user-images\image-20250306170220835.png)。这是一个二元二次方程，是一个凸函数，存在全局最小值。因此对每一对(xi,yi)，β0和β1求偏导＝0，即可求出损失函数最小的对应的β0和β1

那么多元的情况呢？多元线性回归方程的一般形式是：**Y = X.β** ,其中**Y**是标签列向量，**X**是首列为1的样本特征向量，**β**是每个特征对应的权重。首列之所以为1是因为回归方程一般要加一个截距（偏置），允许模型在特征全0时也能输出一个非零值。使得预测结果适应不经过原点的数据分布。在实际生活中，不经过原点是较为常见的情况

那么多元线性回归方程的**β**怎么选择呢，和一元一样，也是通过损失函数来选择。但是我们这里不能用残差平方和，变量太多太复杂而且不是凸函数，我们使用的是损失函数和梯度下降法。这里后续再讲，我们要引入另一个函数，sigmoid函数

### sigmoid函数

逻辑回归模型（LR）是基于sigmoid函数实现的。为什么我们要选sigmoid函数呢？原因如下：

sigmoid函数值域（0，1），满足概率要求，单调上升。数学上符合要求。大多数情况下，没有办法知道未知事件的概率分布形式，而正态分布是所有概率分布中最可能的形式，sigmoid函数和正态分布的积分形式非常类似，计算积分函数计算量更小，因此选sigmoid函数。还有一点原因是最重要的，如下：

[从【为什么要用sigmoid函数】到真的懂【逻辑回归】-CSDN博客](file:///F:/下载/从【为什么要用sigmoid函数】到真的懂【逻辑回归】.html) （目前看不懂，改天再细看）



sigmoid函数的主要作用是把线性回归转化为逻辑回归，即**把回归转化为分类问题的函数**。



落实到代码中，就是把我们通过正态分布（用正态分布是因为比较常见）得到的模型权重参数和特征矩阵（样本）进行乘积，再加上截距得到的多元线性回归方程带入sigmoid函数，构造了逻辑函数

```python
def forward(x,bias,theta):
    z = np.dot(theta,x.T) + bias # 多元线性回归
    y_hat = 1/(1+np.exp(-z)) # sigmoid函数
    return y_hat # y_hat是预测值（概率）
```

### 似然函数和损失函数

  现在我们得到了逻辑函数，里面有未知量`theta`和`bias`，而样本点`x`是确定的，因此我们求的是似然函数，即改变未知量`theta`和`bias`的值得到的模型，函数（`y_hat`）表示的是该模型下`x`出现的概率。我们这里讨论的是二分类问题，这样的分类满足伯努利分布，将`y_hat`带入伯努利分布，则得到了一个似然函数（此函数一般会取个对数方便运算）。然后取反，则求的是损失函数（反过来就是改变未知量`theta`和`bias`的值，得不到x的概率）

损失函数代码：

```python
def loss(y,y_hat): # y是训练集的特征
    e = 1e-8 # 定义一个极小值防止log里出现0
    return -y*np.log(y_hat+e)-(1-y)*np.log(1-y_hat+e)
```

### 计算梯度和梯度下降

有了损失函数，我们就可以求这个函数的最小值（最小值是不准确的，因为这个函数是不确定的，参数在变化。我们要求的是这个函数的最小化，即损失最小）。这里用的就是梯度下降。

想让梯度下降首先要求梯度，梯度是比较好求的，对两个未知量`theta`和`bias`求偏导就行：

```python
def calc_gradient(x,y,y_hat):
    m = x.shape[-1] # 拿到x的最后一个维度，表示10个特征
    delta_theta = np.dot((y_hat - y), x) / m
    delta_bias = np.mean(y_hat - y)
    return delta_theta,delta_bias
```

接下来就是模型训练，计算逻辑函数，计算损失函数，对损失函数进行梯度下降

```python
for i in range(3000): 
    y_hat = forward(X_train,theta,bias)
    loss_val = loss(y_train,y_hat)
    delta_theta,delta_bias = calc_gradient(x_train,y_train,y_hat)
    theta = theta - lr*delta_theta
    bias = bias - lr*delta_bias
    
```

