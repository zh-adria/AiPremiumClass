{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5944,"sourceType":"datasetVersion","datasetId":3759}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:04:18.799358Z","iopub.execute_input":"2025-04-03T08:04:18.799665Z","iopub.status.idle":"2025-04-03T08:04:19.737138Z","shell.execute_reply.started":"2025-04-03T08:04:18.799630Z","shell.execute_reply":"2025-04-03T08:04:19.736286Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/weatherww2/Summary of Weather.csv\n/kaggle/input/weatherww2/Weather Station Locations.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.utils.data import Dataset, Subset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nclass WeatherRNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.rnn = nn.RNN(\n            input_size=1,\n            hidden_size=64,\n            num_layers=2,\n            batch_first=True,\n            dropout=0.2\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 5)  \n        )\n\n    def forward(self, x):\n        out, _ = self.rnn(x)\n        return self.fc(out[:, -1, :])\n\nif __name__ == \"__main__\":\n    df = pd.read_csv('/kaggle/input/weatherww2/Summary of Weather.csv',\n                    usecols=['Date', 'MaxTemp'],\n                    parse_dates=['Date'])\n\n    df['MaxTemp'] = df['MaxTemp'].ffill().bfill()\n    scaler = MinMaxScaler()\n    data = scaler.fit_transform(df[['MaxTemp']])\n    data = data.astype(np.float32)\n\n    seq_len = 30\n    pred_days = 5\n    X, y = [], []\n    for i in range(len(data)-seq_len-pred_days):\n        X.append(data[i:i+seq_len])\n        y.append(data[i+seq_len:i+seq_len+pred_days].flatten())\n    X, y = np.array(X), np.array(y)\n\n    X_tensor = torch.tensor(X, dtype=torch.float32)\n    y_tensor = torch.tensor(y, dtype=torch.float32)\n    dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n    split = int(0.8 * len(X))\n    train_set, test_set = torch.utils.data.random_split(dataset, [split, len(X) - split])\n    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n    test_loader = DataLoader(test_set, batch_size=64)\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = WeatherRNN().to(device)\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    writer = SummaryWriter()\n\n    num_epochs = 20\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)  \n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n            total_loss += loss.item()\n            writer.add_scalar('train loss', loss.item(), epoch * len(train_loader) + batch_idx)\n            avg_train_loss = total_loss / len(train_loader)\n            if batch_idx % 200 == 0:\n                print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n                writer.add_scalar('avg train loss', avg_train_loss, epoch)\n        model.eval()\n        with torch.no_grad():\n            val_loss = 0\n            for inputs, targets in test_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                val_loss += criterion(outputs, targets).item()\n            avg_val = val_loss / len(test_loader)\n            print(f'Epoch [{epoch + 1}/{num_epochs}], Val Loss: {avg_val:.4f}')  \n            writer.add_scalar('val loss', avg_val, epoch)\n\n    torch.save(model.state_dict(), 'weather_rnn.pth')\n    writer.close()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:11:08.144668Z","iopub.execute_input":"2025-04-03T08:11:08.145078Z","iopub.status.idle":"2025-04-03T08:12:50.888375Z","shell.execute_reply.started":"2025-04-03T08:11:08.145046Z","shell.execute_reply":"2025-04-03T08:12:50.887471Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/20], Loss: 0.5031\nEpoch [1/20], Loss: 0.0129\nEpoch [1/20], Loss: 0.0010\nEpoch [1/20], Loss: 0.0010\nEpoch [1/20], Loss: 0.0009\nEpoch [1/20], Loss: 0.0020\nEpoch [1/20], Loss: 0.0013\nEpoch [1/20], Loss: 0.0034\nEpoch [1/20], Val Loss: 0.0013\nEpoch [2/20], Loss: 0.0008\nEpoch [2/20], Loss: 0.0007\nEpoch [2/20], Loss: 0.0028\nEpoch [2/20], Loss: 0.0020\nEpoch [2/20], Loss: 0.0008\nEpoch [2/20], Loss: 0.0010\nEpoch [2/20], Loss: 0.0014\nEpoch [2/20], Loss: 0.0010\nEpoch [2/20], Val Loss: 0.0013\nEpoch [3/20], Loss: 0.0010\nEpoch [3/20], Loss: 0.0038\nEpoch [3/20], Loss: 0.0012\nEpoch [3/20], Loss: 0.0009\nEpoch [3/20], Loss: 0.0018\nEpoch [3/20], Loss: 0.0015\nEpoch [3/20], Loss: 0.0014\nEpoch [3/20], Loss: 0.0020\nEpoch [3/20], Val Loss: 0.0013\nEpoch [4/20], Loss: 0.0010\nEpoch [4/20], Loss: 0.0008\nEpoch [4/20], Loss: 0.0007\nEpoch [4/20], Loss: 0.0007\nEpoch [4/20], Loss: 0.0010\nEpoch [4/20], Loss: 0.0008\nEpoch [4/20], Loss: 0.0009\nEpoch [4/20], Loss: 0.0005\nEpoch [4/20], Val Loss: 0.0014\nEpoch [5/20], Loss: 0.0014\nEpoch [5/20], Loss: 0.0011\nEpoch [5/20], Loss: 0.0014\nEpoch [5/20], Loss: 0.0006\nEpoch [5/20], Loss: 0.0007\nEpoch [5/20], Loss: 0.0012\nEpoch [5/20], Loss: 0.0009\nEpoch [5/20], Loss: 0.0043\nEpoch [5/20], Val Loss: 0.0018\nEpoch [6/20], Loss: 0.0014\nEpoch [6/20], Loss: 0.0005\nEpoch [6/20], Loss: 0.0014\nEpoch [6/20], Loss: 0.0046\nEpoch [6/20], Loss: 0.0019\nEpoch [6/20], Loss: 0.0011\nEpoch [6/20], Loss: 0.0011\nEpoch [6/20], Loss: 0.0007\nEpoch [6/20], Val Loss: 0.0014\nEpoch [7/20], Loss: 0.0020\nEpoch [7/20], Loss: 0.0008\nEpoch [7/20], Loss: 0.0022\nEpoch [7/20], Loss: 0.0011\nEpoch [7/20], Loss: 0.0012\nEpoch [7/20], Loss: 0.0006\nEpoch [7/20], Loss: 0.0012\nEpoch [7/20], Loss: 0.0017\nEpoch [7/20], Val Loss: 0.0013\nEpoch [8/20], Loss: 0.0016\nEpoch [8/20], Loss: 0.0010\nEpoch [8/20], Loss: 0.0006\nEpoch [8/20], Loss: 0.0009\nEpoch [8/20], Loss: 0.0009\nEpoch [8/20], Loss: 0.0010\nEpoch [8/20], Loss: 0.0009\nEpoch [8/20], Loss: 0.0031\nEpoch [8/20], Val Loss: 0.0014\nEpoch [9/20], Loss: 0.0008\nEpoch [9/20], Loss: 0.0011\nEpoch [9/20], Loss: 0.0020\nEpoch [9/20], Loss: 0.0010\nEpoch [9/20], Loss: 0.0008\nEpoch [9/20], Loss: 0.0006\nEpoch [9/20], Loss: 0.0044\nEpoch [9/20], Loss: 0.0019\nEpoch [9/20], Val Loss: 0.0014\nEpoch [10/20], Loss: 0.0015\nEpoch [10/20], Loss: 0.0011\nEpoch [10/20], Loss: 0.0009\nEpoch [10/20], Loss: 0.0008\nEpoch [10/20], Loss: 0.0010\nEpoch [10/20], Loss: 0.0019\nEpoch [10/20], Loss: 0.0010\nEpoch [10/20], Loss: 0.0016\nEpoch [10/20], Val Loss: 0.0013\nEpoch [11/20], Loss: 0.0008\nEpoch [11/20], Loss: 0.0010\nEpoch [11/20], Loss: 0.0009\nEpoch [11/20], Loss: 0.0010\nEpoch [11/20], Loss: 0.0031\nEpoch [11/20], Loss: 0.0012\nEpoch [11/20], Loss: 0.0007\nEpoch [11/20], Loss: 0.0012\nEpoch [11/20], Val Loss: 0.0012\nEpoch [12/20], Loss: 0.0008\nEpoch [12/20], Loss: 0.0014\nEpoch [12/20], Loss: 0.0007\nEpoch [12/20], Loss: 0.0014\nEpoch [12/20], Loss: 0.0009\nEpoch [12/20], Loss: 0.0008\nEpoch [12/20], Loss: 0.0008\nEpoch [12/20], Loss: 0.0006\nEpoch [12/20], Val Loss: 0.0012\nEpoch [13/20], Loss: 0.0008\nEpoch [13/20], Loss: 0.0013\nEpoch [13/20], Loss: 0.0009\nEpoch [13/20], Loss: 0.0009\nEpoch [13/20], Loss: 0.0007\nEpoch [13/20], Loss: 0.0016\nEpoch [13/20], Loss: 0.0009\nEpoch [13/20], Loss: 0.0008\nEpoch [13/20], Val Loss: 0.0013\nEpoch [14/20], Loss: 0.0018\nEpoch [14/20], Loss: 0.0013\nEpoch [14/20], Loss: 0.0022\nEpoch [14/20], Loss: 0.0006\nEpoch [14/20], Loss: 0.0020\nEpoch [14/20], Loss: 0.0008\nEpoch [14/20], Loss: 0.0010\nEpoch [14/20], Loss: 0.0020\nEpoch [14/20], Val Loss: 0.0013\nEpoch [15/20], Loss: 0.0008\nEpoch [15/20], Loss: 0.0007\nEpoch [15/20], Loss: 0.0007\nEpoch [15/20], Loss: 0.0010\nEpoch [15/20], Loss: 0.0006\nEpoch [15/20], Loss: 0.0046\nEpoch [15/20], Loss: 0.0007\nEpoch [15/20], Loss: 0.0011\nEpoch [15/20], Val Loss: 0.0013\nEpoch [16/20], Loss: 0.0017\nEpoch [16/20], Loss: 0.0010\nEpoch [16/20], Loss: 0.0052\nEpoch [16/20], Loss: 0.0008\nEpoch [16/20], Loss: 0.0010\nEpoch [16/20], Loss: 0.0007\nEpoch [16/20], Loss: 0.0015\nEpoch [16/20], Loss: 0.0006\nEpoch [16/20], Val Loss: 0.0013\nEpoch [17/20], Loss: 0.0009\nEpoch [17/20], Loss: 0.0015\nEpoch [17/20], Loss: 0.0007\nEpoch [17/20], Loss: 0.0015\nEpoch [17/20], Loss: 0.0014\nEpoch [17/20], Loss: 0.0007\nEpoch [17/20], Loss: 0.0009\nEpoch [17/20], Loss: 0.0006\nEpoch [17/20], Val Loss: 0.0012\nEpoch [18/20], Loss: 0.0006\nEpoch [18/20], Loss: 0.0011\nEpoch [18/20], Loss: 0.0013\nEpoch [18/20], Loss: 0.0044\nEpoch [18/20], Loss: 0.0007\nEpoch [18/20], Loss: 0.0008\nEpoch [18/20], Loss: 0.0014\nEpoch [18/20], Loss: 0.0012\nEpoch [18/20], Val Loss: 0.0013\nEpoch [19/20], Loss: 0.0010\nEpoch [19/20], Loss: 0.0014\nEpoch [19/20], Loss: 0.0008\nEpoch [19/20], Loss: 0.0009\nEpoch [19/20], Loss: 0.0023\nEpoch [19/20], Loss: 0.0009\nEpoch [19/20], Loss: 0.0008\nEpoch [19/20], Loss: 0.0010\nEpoch [19/20], Val Loss: 0.0012\nEpoch [20/20], Loss: 0.0007\nEpoch [20/20], Loss: 0.0012\nEpoch [20/20], Loss: 0.0011\nEpoch [20/20], Loss: 0.0010\nEpoch [20/20], Loss: 0.0011\nEpoch [20/20], Loss: 0.0012\nEpoch [20/20], Loss: 0.0006\nEpoch [20/20], Loss: 0.0033\nEpoch [20/20], Val Loss: 0.0012\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from tensorboard import notebook\nimport os\nprint(os.listdir(\"/kaggle/working/runs\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:19:30.133603Z","iopub.execute_input":"2025-04-03T08:19:30.133954Z","iopub.status.idle":"2025-04-03T08:19:30.138715Z","shell.execute_reply.started":"2025-04-03T08:19:30.133928Z","shell.execute_reply":"2025-04-03T08:19:30.138006Z"}},"outputs":[{"name":"stdout","text":"['Apr03_08-04-45_e51fdfc42c67', 'Apr03_08-05-59_e51fdfc42c67', 'Apr03_08-11-08_e51fdfc42c67']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"%load_ext tensorboard\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:19:49.879871Z","iopub.execute_input":"2025-04-03T08:19:49.880176Z","iopub.status.idle":"2025-04-03T08:19:49.885067Z","shell.execute_reply.started":"2025-04-03T08:19:49.880154Z","shell.execute_reply":"2025-04-03T08:19:49.884228Z"}},"outputs":[{"name":"stdout","text":"The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"%reload_ext tensorboard","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:27:54.186285Z","iopub.execute_input":"2025-04-03T08:27:54.186624Z","iopub.status.idle":"2025-04-03T08:27:54.191907Z","shell.execute_reply.started":"2025-04-03T08:27:54.186602Z","shell.execute_reply":"2025-04-03T08:27:54.191247Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"%tensorboard --logdir /kaggle/working/runs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:28:01.711564Z","iopub.execute_input":"2025-04-03T08:28:01.711906Z","iopub.status.idle":"2025-04-03T08:28:01.720666Z","shell.execute_reply.started":"2025-04-03T08:28:01.711877Z","shell.execute_reply":"2025-04-03T08:28:01.719992Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Reusing TensorBoard on port 6006 (pid 91), started 0:10:21 ago. (Use '!kill 91' to kill it.)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    "},"metadata":{}}],"execution_count":20}]}
