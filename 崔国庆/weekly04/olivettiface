import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from sklearn.datasets import fetch_olivetti_faces
from sklearn.model_selection import train_test_split

class TorchNN(nn.Module):
    # 初始化
    def __init__(self):  # self 指代新创建模型对象
        super().__init__()

        self.linear1 = nn.Linear(4096, 512)
        self.bn1 = nn.BatchNorm1d(512)
        self.linear2 = nn.Linear(512, 512)
        self.bn2 = nn.BatchNorm1d(512)
        # 修改输出类别数为40
        self.linear3 = nn.Linear(512, 40)
        self.drop = nn.Dropout(p=0.3)
        self.act = nn.ReLU()

    # forward 前向运算 (nn.Module方法重写)
    def forward(self, input_tensor):
        out = self.linear1(input_tensor)
        out = self.bn1(out)
        out = self.act(out)
        out = self.drop(out)
        out = self.linear2(out)
        out = self.bn2(out)
        out = self.act(out)
        out = self.drop(out)
        final = self.linear3(out)

        return final

if __name__ == '__main__':
    # 测试代码
    print('模型测试')
    model = TorchNN()  # 创建模型对象

    input_data = torch.randn((10, 4096))
    final = model(input_data)
    print(final.shape)

# 定义超参数
LR = 1e-3
epochs = 20
BATCH_SIZE = 128

# 数据集加载
olivetti_faces = fetch_olivetti_faces(data_home='./face_data', shuffle=True)

X = olivetti_faces.data
y = olivetti_faces.target
train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)

train_X = torch.tensor(train_X, dtype=torch.float32)
train_y = torch.tensor(train_y, dtype=torch.long)
test_X = torch.tensor(test_X, dtype=torch.float32)
test_y = torch.tensor(test_y, dtype=torch.long)

train_dl = DataLoader(list(zip(train_X, train_y)), batch_size=BATCH_SIZE, shuffle=True)
test_dl = DataLoader(list(zip(test_X, test_y)), batch_size=BATCH_SIZE)

model = TorchNN()
print(model)

# 损失函数&优化器
loss_fn = nn.CrossEntropyLoss()  # 交叉熵损失函数
# 优化器（模型参数更新）
optimizer = torch.optim.AdamW(model.parameters(), lr=LR)

model.train()  # 正则化&归一化生效
for epoch in range(epochs):
    running_loss = 0.0
    for data, target in train_dl:
        # 前向运算
        output = model(data.reshape(-1, 4096))
        # 计算损失
        loss = loss_fn(output, target)
        # 反向传播
        model.zero_grad()  # 所有参数梯度清零
        loss.backward()     # 计算梯度（参数.grad）
        optimizer.step()    # 更新参数
        running_loss += loss.item()

    print(f'Epoch:{epoch} Loss: {running_loss / len(train_dl)}')

# 测试
correct = 0
total = 0
model.eval()  # train(False)
with torch.no_grad():  # 不计算梯度
    for data, target in test_dl:
        output = model(data.reshape(-1, 4096))
        _, predicted = torch.max(output, 1)  # 返回每行最大值和索引
        total += target.size(0)  # size(0) 等效 shape[0]
        correct += (predicted == target).sum().item()

print(f'Accuracy: {correct / total * 100}%')
