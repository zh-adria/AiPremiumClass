{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n",
      "True\n",
      "12.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)          # PyTorch 版本\n",
    "print(torch.cuda.is_available())  # 输出 True 才表示 GPU 可用\n",
    "print(torch.version.cuda)         # 查看 PyTorch 关联的 CUDA 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "date1 = torch.tensor([[1,2],[3,4]])\n",
    "date1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_arry = np.array([[1,2],[3,4]])\n",
    "\n",
    "date2 = torch.from_numpy(np_arry)\n",
    "date2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3096, 0.9067],\n",
       "        [0.3248, 0.2996]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过已知张量维度创建新张量\n",
    "date3 = torch.rand_like(date2,dtype=torch.float)                \n",
    "date3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.6162, 0.4772, 0.9226],\n",
      "        [0.6612, 0.3033, 0.0458]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "from torch import rand\n",
    "\n",
    "\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)        # 随机张量\n",
    "ones_tensor = torch.ones(shape)        # 全1张量\n",
    "zeros_tensor = torch.zeros(shape)      # 全0张量\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")     \n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "tensor([[0.5273, 0.9480, 0.9029],\n",
      "        [0.0018, 0.3779, 0.6514]])\n",
      "------------------\n",
      "tensor([[ 0.1786, -0.6784,  0.8847],\n",
      "        [-1.1851,  0.1136,  0.6099]])\n",
      "------------------\n",
      "tensor([[ 2.0954,  0.9864,  0.5260],\n",
      "        [-2.9542, -1.0217,  1.0707]])\n",
      "------------------\n",
      "tensor([  0.,  25.,  50.,  75., 100.])\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# 基于现有tensor构建，使用新的数值填充\n",
    "from traceback import print_list\n",
    "\n",
    "\n",
    "m = torch.ones(5,3,dtype=torch.double)\n",
    "n = torch.rand_like(m,dtype=torch.float)\n",
    "\n",
    "# 获取tensor的大小\n",
    "print(m.size())     # torch.Size([5, 3])\n",
    "\n",
    "# 均匀分布(0,1)\n",
    "u = torch.rand(2,3)\n",
    "print(u)\n",
    "print(\"------------------\")\n",
    "\n",
    "# 正态分布\n",
    "n = torch.randn(2,3)\n",
    "print(n)\n",
    "print(\"------------------\")\n",
    "\n",
    "# 离散正态分布\n",
    "d = torch.normal(mean=.0,std=1.0,size=(2,3))\n",
    "print(d)\n",
    "print(\"------------------\")\n",
    "\n",
    "# 线性间隔向量（返回的是一个1维张量，包含在区间start和end上均匀间隔的steps个点）\n",
    "l = torch.linspace(start=0,end=100,steps=5)\n",
    "print(l)\n",
    "print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor  = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")        \n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU count: 1\n",
      "tensor([[0.1924, 0.9186, 0.4298, 0.2169],\n",
      "        [0.1631, 0.5299, 0.1965, 0.6310],\n",
      "        [0.0666, 0.3281, 0.4804, 0.2398]], device='cuda:0')\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 检查 GPU 数量\n",
    "print(\"GPU count:\", torch.cuda.device_count())  \n",
    "\n",
    "# 检查pytorch是否支持GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")          # a CUDA device object\n",
    "    tensor = tensor.to(device)             \n",
    "\n",
    "print(tensor)\n",
    "print(tensor.device)\n",
    "\n",
    "\n",
    "\n",
    "# mac上没有GPU，使用M系列芯片\n",
    "# if torch.backends.mps.is_available():\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1., 1., 1., 1.])\n",
      "First column:  tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print('First row: ',tensor[0])             \n",
    "print('Last column:', tensor[..., -1])     \n",
    "tensor[:,1] = 0         # 将第二列都置为0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.,  7.,  8.,  6.,  7.,  8.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11.,  9., 10., 11.,  9., 10., 11.],\n",
      "        [12., 13., 14., 12., 13., 14., 12., 13., 14.]])\n",
      "torch.Size([3, 9])\n",
      "tensor([[ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.],\n",
      "        [12., 13., 14.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.],\n",
      "        [12., 13., 14.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.],\n",
      "        [12., 13., 14.]])\n",
      "torch.Size([9, 3])\n",
      "tensor([[[ 6.,  7.,  8.],\n",
      "         [ 6.,  7.,  8.],\n",
      "         [ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.],\n",
      "         [ 9., 10., 11.],\n",
      "         [ 9., 10., 11.]],\n",
      "\n",
      "        [[12., 13., 14.],\n",
      "         [12., 13., 14.],\n",
      "         [12., 13., 14.]]])\n",
      "torch.Size([3, 3, 3])\n",
      "tensor([[[ 6.,  7.,  8.],\n",
      "         [ 9., 10., 11.],\n",
      "         [12., 13., 14.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.],\n",
      "         [ 9., 10., 11.],\n",
      "         [12., 13., 14.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.],\n",
      "         [ 9., 10., 11.],\n",
      "         [12., 13., 14.]]])\n",
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch.cat功能类似torch.stack，一个是拼接（左右），一个是堆叠（上下）\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)      # 按列拼接张量，dim=1表示按列拼接\n",
    "print(t1)\n",
    "print(t1.shape)\n",
    "t2 = torch.cat([tensor, tensor, tensor], dim=0)      # 按行拼接张量，dim=0表示按行拼接\n",
    "print(t2)\n",
    "print(t2.shape)\n",
    "\n",
    "t3 = torch.stack([tensor, tensor, tensor], dim=1)    # 按列堆叠张量，dim=1表示按列堆叠\n",
    "print(t3)\n",
    "print(t3.shape)\n",
    "\n",
    "t4 = torch.stack([tensor, tensor, tensor], dim=0)    # 按行堆叠张量，dim=0表示按行堆叠\n",
    "print(t4)\n",
    "print(t4.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  4.,  9.],\n",
       "        [16., 25., 36.],\n",
       "        [49., 64., 81.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "tensor = torch.arange(1, 10,dtype=torch.float32).reshape(3,3)\n",
    "\n",
    "# 计算两个张量之间矩阵乘法的几种方法，结果都是一样的 dot\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "# print(y1)\n",
    "# print(y2)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "# print(y3)\n",
    "\n",
    "\n",
    "# 计算张量逐元素相乘的几种方法，结果都是一样的 \n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "# print(z1)\n",
    "# print(z2)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "# print(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()         \n",
    "agg_item = agg.item()           # 将张量中的元素转换为Python数值\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.],\n",
       "       [49., 64., 81.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr = z1.numpy()           # 将张量转换为numpy数组\n",
    "np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]]) \n",
      "\n",
      "tensor([[ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.],\n",
      "        [12., 13., 14.]])\n"
     ]
    }
   ],
   "source": [
    "# inplace操作\n",
    "print(tensor,\"\\n\")\n",
    "tensor.add_(5)\n",
    "# tensor = tensor + 5\n",
    "# tensor+=5\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 张量转换为numpy数组\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m n \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# 张量转换为numpy数组\n",
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy数组转换为张量\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sgp\\AppData\\Local\\Temp\\ipykernel_17332\\253383709.py:11: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3729.)\n",
      "  result = torch.matmul(A,x.T) + torch.dot(b,x) + c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'expression.png'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# 定义矩阵A，向量b和常数c\n",
    "A = torch.randn(10,10,requires_grad=True)       # requires_grad=True表示需要计算梯度\n",
    "b = torch.randn(10,requires_grad=True)\n",
    "c = torch.randn(1,requires_grad=True)\n",
    "x = torch.randn(10,requires_grad=True)\n",
    "\n",
    "# 计算x^T A x + b^T x + c\n",
    "result = torch.matmul(A,x.T) + torch.dot(b,x) + c\n",
    "\n",
    "# 生成计算图节点\n",
    "dot = make_dot(result, params={\"A\":A, \"b\":b, \"c\":c, \"x\":x})\n",
    "\n",
    "# 绘制计算图\n",
    "dot.render('expression', format='png', cleanup=True, view=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
