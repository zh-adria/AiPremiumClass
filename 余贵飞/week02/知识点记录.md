为什么线性回归中使用最小二乘法进行参数估计？
最小二乘法是一种数学优化技术，其核心思想是通过最小化观测数据与模型预测值之间误差的平方和， 寻找数据的最佳函数匹配‌.
技术本质‌
该方法属于曲线拟合技术，不仅适用于线性回归，还可扩展至其他函数形式（如多项式、指数函数等）‌。
其数学基础是通过简化计算逼近不可知的真实值，确保估计值与实际数据的整体偏差最小‌.
‌应用前提‌
在使用最小二乘法进行线性拟合时，需先通过散点图观察数据是否呈现线性趋势；若为非线性关系，则需选择其他拟合方法‌

最小二乘法与最大似然估计的关系？
1. 当测量误差服从高斯分布时，最小二乘法等价于最大似然估计。
2. 因为在高斯分布假设下，最大化似然函数相当于最小化平方误差，数学推导可能涉及对数似然函数和平方项的关系。
3. 最小二乘法不依赖分布假设，而最大似然需要已知分布
最小二乘法：
假设条件：仅需模型为线性关系，不依赖误差分布假设‌
适用范围：任意线性/非线性模型参数估计
最大似然估计：
假设条件：需明确误差的分布形式（如高斯分布），否则无法构建似然函数‌
适用范围：已知数据生成分布模型的参数估计

最小二乘法‌：直接从几何角度最小化预测值与观测值的距离，无需概率解释‌。
‌最大似然估计‌：基于概率框架，强调参数在统计意义上的“最可能”性，需依赖分布假设‌

最小二乘法是最大似然估计在‌高斯误差假设下的特例‌，两者在特定条件下等价，但核心思想与应用前提存在本质差异‌


数据集适用逻辑回归的判断标准
1. ‌因变量类型‌
逻辑回归要求因变量为‌分类变量‌（如二分类、多分类或有序分类）。例如：
    二分类：是否违约（0/1）‌12
    多分类：疾病类型（A/B/C）‌67
    有序分类：满意度评分（低/中/高）‌
    若因变量为连续型数据（如销售额），则需改用线性回归‌。

因变量是否为分类变量？  
  │  
  ├─ 否 → 改用线性回归或其他模型  
  │  
  └─ 是 → 检查自变量与logit值的线性关系  
          │  
          ├─ 非线性 → 添加交互项/多项式项或换模型  
          │  
          └─ 线性 → 处理多重共线性、缺失值、异常值  
                     │  
                     ├─ 数据质量达标 → 特征编码与标准化  
                     │  
                     └─ 样本量充足且类别平衡 → 适用逻辑回归

逻辑回归适用于以下场景：
    ‌分类预测‌：如用户流失预测、疾病诊断‌
    ‌驱动力分析‌：识别影响事件发生的关键因素（如广告点击率的影响因素）‌
    ‌概率估计‌：输出事件发生的概率（如贷款违约概率）‌