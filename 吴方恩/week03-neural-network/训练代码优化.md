# 神经网络超参数调试实验（优化版）

## 一、优化后的完整代码
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.datasets import KMNIST
from torchvision.transforms import ToTensor
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

# 实验配置
class Config:
    # 超参数搜索空间
    HIDDEN_SIZES = [64, 128, 256]      # 隐藏层神经元数
    HIDDEN_LAYERS = [1, 2, 3]         # 隐藏层数量
    LEARNING_RATES = [1e-2, 1e-3]     # 学习率
    BATCH_SIZES = [32, 64, 128]       # 批次大小
    EPOCHS = 20                       # 训练轮次
    INPUT_SIZE = 28*28                # 输入维度
    NUM_CLASSES = 10                  # 输出类别

# 模型构建器
def build_model(hidden_sizes, num_layers):
    layers = []
    in_features = Config.INPUT_SIZE
    
    # 构建隐藏层
    for i in range(num_layers):
        out_features = hidden_sizes[i] if i < len(hidden_sizes) else hidden_sizes[-1]
        layers += [
            nn.Linear(in_features, out_features),
            nn.ReLU(),                # 改用ReLU激活函数
            nn.BatchNorm1d(out_features)  # 添加批标准化
        ]
        in_features = out_features
    
    # 输出层
    layers.append(nn.Linear(in_features, Config.NUM_CLASSES))
    return nn.Sequential(*layers)

# 训练与评估函数
def train_evaluate(params):
    # 数据加载
    train_set = KMNIST(root='./data', train=True, transform=ToTensor(), download=True)
    test_set = KMNIST(root='./data', train=False, transform=ToTensor(), download=True)
    
    train_loader = DataLoader(train_set, batch_size=params['batch_size'], shuffle=True)
    test_loader = DataLoader(test_set, batch_size=params['batch_size'], shuffle=False)
    
    # 模型初始化
    model = build_model([params['hidden_size']]*params['num_layers'], params['num_layers'])
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=params['lr'])  # 改用Adam优化器
    
    # 训练记录
    history = {'train_loss': [], 'test_acc': []}
    
    # 训练循环
    for epoch in range(Config.EPOCHS):
        # 训练阶段
        model.train()
        epoch_loss = 0
        for data, target in train_loader:
            optimizer.zero_grad()
            output = model(data.view(-1, Config.INPUT_SIZE))
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        
        # 评估阶段
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for data, target in test_loader:
                output = model(data.view(-1, Config.INPUT_SIZE))
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
        
        # 记录指标
        history['train_loss'].append(epoch_loss/len(train_loader))
        history['test_acc'].append(100 * correct / total)
        
        print(f"Epoch {epoch+1}/{Config.EPOCHS} | "
              f"Loss: {history['train_loss'][-1]:.4f} | "
              f"Acc: {history['test_acc'][-1]:.2f}%")
    
    return history

# 超参数搜索
results = []
for batch_size in Config.BATCH_SIZES:
    for lr in Config.LEARNING_RATES:
        for num_layers in Config.HIDDEN_LAYERS:
            for hidden_size in Config.HIDDEN_SIZES:
                params = {
                    'batch_size': batch_size,
                    'lr': lr,
                    'num_layers': num_layers,
                    'hidden_size': hidden_size
                }
                print(f"\n当前参数组合：{params}")
                history = train_evaluate(params)
                results.append({
                    'params': params,
                    'final_acc': history['test_acc'][-1],
                    'loss_curve': history['train_loss'],
                    'acc_curve': history['test_acc']
                })

# 结果可视化
def plot_results(results):
    plt.figure(figsize=(15, 10))
    
    # 准确率对比
    plt.subplot(2, 1, 1)
    x_labels = [str(r['params']) for r in results]
    acc_values = [r['final_acc'] for r in results]
    plt.bar(range(len(results)), acc_values)
    plt.xticks(range(len(results)), x_labels, rotation=90)
    plt.title('Test Accuracy Comparison')
    plt.ylabel('Accuracy (%)')
    
    # 损失曲线对比
    plt.subplot(2, 1, 2)
    for r in results:
        plt.plot(r['loss_curve'], label=str(r['params']))
    plt.title('Training Loss Curves')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    plt.tight_layout()
    plt.show()

plot_results(results)
```

## 二、代码优化说明

### 1. 主要改进点
- **模块化设计**：将配置、模型构建、训练流程分离
- **参数搜索系统**：支持多参数组合自动实验
- **性能提升**：
  - 使用ReLU代替Sigmoid（缓解梯度消失）
  - 添加Batch Normalization（加速收敛）
  - 改用Adam优化器（自适应学习率）
- **可视化系统**：自动生成准确率对比和损失曲线图

### 2. 参数影响分析维度
| 参数类型   | 测试值       | 分析指标          |
| ---------- | ------------ | ----------------- |
| 批次大小   | 32, 64, 128  | 训练速度/内存消耗 |
| 学习率     | 0.01, 0.001  | 收敛稳定性        |
| 隐藏层数   | 1, 2, 3      | 模型复杂度        |
| 神经元数量 | 64, 128, 256 | 模型容量          |

## 三、预期实验结果

### 1. 参数影响规律预测
| 参数         | 增大时的典型影响                         |
| ------------ | ---------------------------------------- |
| **批次大小** | - 训练速度加快<br>- 可能降低泛化能力     |
| **学习率**   | - 收敛速度加快<br>- 可能震荡不收敛       |
| **隐藏层数** | - 模型容量增加<br>- 训练难度增大         |
| **神经元数** | - 模型表达能力增强<br>- 计算资源消耗增加 |

### 2. 典型实验结果示例
```python
# 预期输出形式
当前参数组合：{'batch_size': 32, 'lr': 0.01, 'num_layers': 1, 'hidden_size': 64}
Epoch 1/20 | Loss: 1.2345 | Acc: 65.43%
...
Epoch 20/20 | Loss: 0.1234 | Acc: 89.12%

当前参数组合：{'batch_size': 128, 'lr': 0.001, 'num_layers': 3, 'hidden_size': 256}
Epoch 1/20 | Loss: 2.3456 | Acc: 32.15%
...
Epoch 20/20 | Loss: 0.4567 | Acc: 85.67%
```

## 四、结果分析方法

### 1. 可视化分析要点
- **准确率对比图**：直方图显示不同参数组合的最终测试准确率
- **损失曲线图**：对比不同参数组合的训练收敛情况

### 2. 分析维度建议
1. **学习率与批次大小的平衡**：
   - 大batch_size（128）需要更小的学习率
   - 小batch_size（32）可以承受更大学习率

2. **网络深度与宽度的权衡**：
   - 深层网络（3层）需要更多训练时间
   - 宽层网络（256神经元）需要更多内存

3. **收敛稳定性分析**：
   - 震荡较大的曲线可能需要降低学习率
   - 长期不下降的曲线可能需要增加模型容量

> **提示**：实际运行时建议先减少参数组合进行快速验证（例如设置EPOCHS=5），确定有潜力的参数组合后再进行完整训练。