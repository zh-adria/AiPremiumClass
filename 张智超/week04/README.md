#### 第四周作业
1. 搭建的神经网络，使用olivettiface数据集进行训练。
2. 结合归一化和正则化来优化网络模型结构，观察对比loss结果。
3. 尝试不同optimizer对模型进行训练，观察对比loss结果。
4. 注册kaggle并尝试激活Accelerator，使用GPU加速模型训练。

#### 知识点总结：全连接神经网络模型结构
##### 1. 激活函数：
引入非线性特性、控制输出范围、增强模型表达能力。

* Sigmoid函数：输出值范围(0,1)，容易导致梯度消失，且输出均值不为0；
* Tanh函数：输出值范围(-1,1)，均值为0，仍容易导致梯度消失；
* ReLU函数：输出值范围(0,∞)，存在神经元死亡现象（输入为负数时梯度为0导致权重无法更新，即死亡神经元）；
* Leaky ReLU函数：对ReLU函数的改进，当输入为负数时保留小梯度（例如0.01），解决了ReLU函数的神经元死亡问题；
* Swish函数：Swish(x) = x ⋅ σ(x)，比ReLU函数更平滑，适合深层网络；

##### 2. 归一化：
稳定分布、加速收敛、提高泛化性。

* BatchNorm：对每个batch计算均值和方差，然后进行归一化（均值为0，方差为1），然后引入参数γ和β，对归一化后的数据进行缩放和平移；

##### 3. 正则化：
随机关闭部分神经元，防止过拟合、提高泛化性。

* Dropout：随机丢弃部分神经元，防止过拟合；


##### 4. 优化器Optimizer：
更新神经网络参数。
* SGD：随机梯度下降，每次更新权重时只考虑当前样本的梯度；
    * 带动量（momentum）的随机梯度下降；
* RMSProp：保持梯度平方的滑动平均；
* Adam：自适应矩估计，计算⼀阶矩估计与⼆阶矩估计的⽐值，其核⼼⽬的是实现⾃适应学习率调整；
* AdamW：Adam的变种，对参数θ增加了权重衰减项，防止过拟合；

##### 5. 神经网络输出层和模型任务：
* 单一输出节点：回归预测；
* 多输出节点：分类预测；

##### 6. 回归预测的损失函数：
* MSE：均方误差，又称L2损失；
* MAE：平均绝对误差，又称L1损失；
* Huber：结合了MSE和MAE，当预测值与真实值差距较大时，采用MAE，否则采用MSE；

##### 7. 分类预测的损失函数：
* CrossEntropyLoss：交叉熵损失；
    * 对输入进行softmax；
    * 计算交叉熵；
* binary_cross_entropy：二分类（0或1）的交叉熵损失，实际是CrossEntropyLoss的一种特殊情况；
