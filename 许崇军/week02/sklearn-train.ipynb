{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n"
     ]
    }
   ],
   "source": [
    "X,y = load_iris(return_X_y=True)\n",
    "X1= X[:100]  # 取前100个数据\n",
    "y1 = y[:100]  # 取前100个标签(0,1)\n",
    "print(X1.shape)\n",
    "# print(y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 4)\n",
      "(80,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, theta, bias):\n",
    "    # 2.1线性运算  这里的z就是y_hat\n",
    "    z = np.dot(theta, x.T) + bias \n",
    "    # 2.2 sigmoid公式\n",
    "    y_hat = 1 / (1 + np.exp(-z))  \n",
    "    return y_hat\n",
    "\n",
    "def loss(y, y_hat):   \n",
    "    e = 1e-8\n",
    "    return - y * np.log(y_hat + e) - (1 - y) * np.log(1 - y_hat + e)\n",
    "\n",
    "\n",
    "def calc_gradient(x,y,y_hat):\n",
    "    # 计算梯度\n",
    "    m = x.shape[-1]    \n",
    "    # theta梯度计算\n",
    "    delta_theta = np.dot((y_hat - y), x) / m\n",
    "    # bias梯度计算\n",
    "    delta_bias = np.mean(y_hat - y)   #mean就是求平均值\n",
    "    # 返回梯度\n",
    "    return delta_theta, delta_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.66578165  1.1287311  -0.23937512  0.60853903]]\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.randn(1,4)  # shape (1, 10)\n",
    "#因为每个样本有10个特征值，所以生成10个θ\n",
    "bias = 0\n",
    "lr = 0.05  # 超参数\n",
    "epochs = 3000  # 训练次数\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.1405939762192185, acc: 0.075\n",
      "epoch: 100, loss: 0.002347904296020625, acc: 1.0\n",
      "epoch: 200, loss: 0.0018926032160967408, acc: 1.0\n",
      "epoch: 300, loss: 0.0015903646661001823, acc: 1.0\n",
      "epoch: 400, loss: 0.0013744490623260664, acc: 1.0\n",
      "epoch: 500, loss: 0.0012121415138048334, acc: 1.0\n",
      "epoch: 600, loss: 0.0010854754223733462, acc: 1.0\n",
      "epoch: 700, loss: 0.0009837438195812284, acc: 1.0\n",
      "epoch: 800, loss: 0.0009001595907537679, acc: 1.0\n",
      "epoch: 900, loss: 0.0008302070428129494, acc: 1.0\n",
      "epoch: 1000, loss: 0.0007707622404736964, acc: 1.0\n",
      "epoch: 1100, loss: 0.0007195935147794826, acc: 1.0\n",
      "epoch: 1200, loss: 0.0006750630999307163, acc: 1.0\n",
      "epoch: 1300, loss: 0.000635941231460741, acc: 1.0\n",
      "epoch: 1400, loss: 0.0006012861062795458, acc: 1.0\n",
      "epoch: 1500, loss: 0.0005703639607759148, acc: 1.0\n",
      "epoch: 1600, loss: 0.0005425944268438056, acc: 1.0\n",
      "epoch: 1700, loss: 0.0005175122912033229, acc: 1.0\n",
      "epoch: 1800, loss: 0.0004947401780447159, acc: 1.0\n",
      "epoch: 1900, loss: 0.00047396867434194265, acc: 1.0\n",
      "epoch: 2000, loss: 0.0004549416309896699, acc: 1.0\n",
      "epoch: 2100, loss: 0.00043744513000380043, acc: 1.0\n",
      "epoch: 2200, loss: 0.00042129909179956516, acc: 1.0\n",
      "epoch: 2300, loss: 0.0004063508125006656, acc: 1.0\n",
      "epoch: 2400, loss: 0.00039246993167759363, acc: 1.0\n",
      "epoch: 2500, loss: 0.0003795444736299889, acc: 1.0\n",
      "epoch: 2600, loss: 0.0003674777037178676, acc: 1.0\n",
      "epoch: 2700, loss: 0.00035618561010761, acc: 1.0\n",
      "epoch: 2800, loss: 0.0003455948701670666, acc: 1.0\n",
      "epoch: 2900, loss: 0.0003356411958734187, acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    " for i in range(epochs):  \n",
    "    # 前向计算\n",
    "    y_hat = forward(X_train, theta, bias)\n",
    "    # 计算损失差\n",
    "    loss_val = loss(y_train, y_hat)\n",
    "    # 计算梯度\n",
    "    delta_theta, delta_bias = calc_gradient(X_train, y_train, y_hat)\n",
    "    # 更新参数，替换变量内容\n",
    "    theta = theta - lr * delta_theta\n",
    "    bias = bias - lr * delta_bias\n",
    "    #计算准确率\n",
    "    if i % 100 == 0:\n",
    "      acc = np.mean(np.round(y_hat) == y_train)  \n",
    "      print(f\"epoch: {i}, loss: {np.mean(loss_val)}, acc: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # 保存模型参数\n",
    "np.savez('lris.npz', theta=theta, bias=bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
