
逻辑回归是一种分类算法，尤其是用于二分类问题，就是用sigmoid函数把线性回归的结果映射到0和1之间，从而把模型的输出值转换为概率值。

将模型带⼊观测值会计算出⼀个输出，输出与真实的数据标签输出对比会产生⼀个误差，然后根据误差会设定⼀个损失函数，要想知道某⼀个权重参数对损失函数的影响，那么就要求它的偏导数。我们就可以使用梯度下降法来寻找损失函数极⼩值。梯度下降是⼀种基于搜索的最优化的方法，梯度下降法的作用就是：最小化⼀个损失函数。

梯度：函数在某⼀点的梯度是这样⼀个向量，它的方向与取得最大方向导数的方向⼀致，而它的模为方向导数的最大值。

学习率：⼀个合适的学习率能够让我们的损失函数在合适的时间内收敛到局部最小值。当学习率设定太小时，收敛过程将变得十分缓慢并且可能不收敛。
反而学习率设置的过大时，梯度可能会在最小值附近来回震荡，甚至可能收敛。学习率的大小直接影响到梯度更新的步数。学习率越小，theta下降的步长越小。所需要的步数(计算次数)也就越多。

逻辑回归训练步骤：
1. 数据准备（scikit-learn库）
    1. 生成数据
    2. 拆分数据，拆分为训练集和测试集（局部样本训练模型会导致过拟合，测试预测不好泛化差）
    3. 准备参数（权重参数，超参数学习率、迭代轮数）
2. 定义模型运算函数
    1. 线性运算函数
    2. sigmoid函数，代入输入项转换成概率值
3. 计算损失函数
4. 计算梯度的函数
5. 模型训练循环迭代流程
    1. 前向计算
    2. 计算损失。模型运算出的结果和真实结果比对
    3. 计算梯度
    4. 更新参数
    5. 计算准确率。
6. 模型推理（测试集）
