{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-20T10:40:57.829968Z",
     "iopub.status.busy": "2025-03-20T10:40:57.829528Z",
     "iopub.status.idle": "2025-03-20T10:40:57.837440Z",
     "shell.execute_reply": "2025-03-20T10:40:57.836698Z",
     "shell.execute_reply.started": "2025-03-20T10:40:57.829939Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/olivetti-faces/olivetti_faces_target.npy\n",
      "/kaggle/input/olivetti-faces/olivetti_faces.npy\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T10:40:57.838842Z",
     "iopub.status.busy": "2025-03-20T10:40:57.838544Z",
     "iopub.status.idle": "2025-03-20T10:40:57.887773Z",
     "shell.execute_reply": "2025-03-20T10:40:57.887044Z",
     "shell.execute_reply.started": "2025-03-20T10:40:57.838820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LR = 1e-3\n",
    "epochs = 50\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "olivetti_faces = fetch_olivetti_faces(data_home='./', shuffle=True)\n",
    "X = olivetti_faces.data\n",
    "y = olivetti_faces.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "x_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = [(img, lbl) for img, lbl in zip(x_train, y_train)]\n",
    "test_dataset = [(img, lb1) for img, lb1 in zip(x_test, y_test)]\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T10:40:57.889460Z",
     "iopub.status.busy": "2025-03-20T10:40:57.889231Z",
     "iopub.status.idle": "2025-03-20T10:40:57.894130Z",
     "shell.execute_reply": "2025-03-20T10:40:57.893308Z",
     "shell.execute_reply.started": "2025-03-20T10:40:57.889440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(4096, 12288),\n",
    "        nn.BatchNorm1d(12288),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(12288, 8192),\n",
    "        nn.BatchNorm1d(8192),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.6),\n",
    "        nn.Linear(8192, 2048),\n",
    "        nn.BatchNorm1d(2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(2048, 40)\n",
    "    ).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T10:40:57.895440Z",
     "iopub.status.busy": "2025-03-20T10:40:57.895225Z",
     "iopub.status.idle": "2025-03-20T10:40:59.465504Z",
     "shell.execute_reply": "2025-03-20T10:40:59.464856Z",
     "shell.execute_reply.started": "2025-03-20T10:40:57.895421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T10:40:59.466575Z",
     "iopub.status.busy": "2025-03-20T10:40:59.466363Z",
     "iopub.status.idle": "2025-03-20T10:42:17.717694Z",
     "shell.execute_reply": "2025-03-20T10:42:17.716784Z",
     "shell.execute_reply.started": "2025-03-20T10:40:59.466557Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sgd optimizer\n",
      "Epoch 0 Loss: 3.3289413452148438 with sgd\n",
      "Epoch 1 Loss: 1.847638726234436 with sgd\n",
      "Epoch 2 Loss: 1.195692777633667 with sgd\n",
      "Epoch 3 Loss: 0.471047967672348 with sgd\n",
      "Epoch 4 Loss: 0.5231450200080872 with sgd\n",
      "Epoch 5 Loss: 0.29533660411834717 with sgd\n",
      "Epoch 6 Loss: 0.3562431335449219 with sgd\n",
      "Epoch 7 Loss: 0.20333795249462128 with sgd\n",
      "Epoch 8 Loss: 0.15340350568294525 with sgd\n",
      "Epoch 9 Loss: 0.09753087162971497 with sgd\n",
      "Epoch 10 Loss: 0.16551896929740906 with sgd\n",
      "Epoch 11 Loss: 0.14894574880599976 with sgd\n",
      "Epoch 12 Loss: 0.06936604529619217 with sgd\n",
      "Epoch 13 Loss: 0.11914755403995514 with sgd\n",
      "Epoch 14 Loss: 0.07478485256433487 with sgd\n",
      "Epoch 15 Loss: 0.05491945892572403 with sgd\n",
      "Epoch 16 Loss: 0.05964955687522888 with sgd\n",
      "Epoch 17 Loss: 0.058710455894470215 with sgd\n",
      "Epoch 18 Loss: 0.08189171552658081 with sgd\n",
      "Epoch 19 Loss: 0.07231154292821884 with sgd\n",
      "Epoch 20 Loss: 0.08317961543798447 with sgd\n",
      "Epoch 21 Loss: 0.02951253019273281 with sgd\n",
      "Epoch 22 Loss: 0.02651090919971466 with sgd\n",
      "Epoch 23 Loss: 0.022839687764644623 with sgd\n",
      "Epoch 24 Loss: 0.06869833171367645 with sgd\n",
      "Epoch 25 Loss: 0.020092928782105446 with sgd\n",
      "Epoch 26 Loss: 0.03633194416761398 with sgd\n",
      "Epoch 27 Loss: 0.03128135949373245 with sgd\n",
      "Epoch 28 Loss: 0.03721637278795242 with sgd\n",
      "Epoch 29 Loss: 0.0245523601770401 with sgd\n",
      "Epoch 30 Loss: 0.024848658591508865 with sgd\n",
      "Epoch 31 Loss: 0.038496118038892746 with sgd\n",
      "Epoch 32 Loss: 0.06503312289714813 with sgd\n",
      "Epoch 33 Loss: 0.02550431713461876 with sgd\n",
      "Epoch 34 Loss: 0.01927567645907402 with sgd\n",
      "Epoch 35 Loss: 0.020923485979437828 with sgd\n",
      "Epoch 36 Loss: 0.013692975044250488 with sgd\n",
      "Epoch 37 Loss: 0.044120751321315765 with sgd\n",
      "Epoch 38 Loss: 0.018030287697911263 with sgd\n",
      "Epoch 39 Loss: 0.029402781277894974 with sgd\n",
      "Epoch 40 Loss: 0.01921146735548973 with sgd\n",
      "Epoch 41 Loss: 0.02426302619278431 with sgd\n",
      "Epoch 42 Loss: 0.017782263457775116 with sgd\n",
      "Epoch 43 Loss: 0.03377315402030945 with sgd\n",
      "Epoch 44 Loss: 0.021866388618946075 with sgd\n",
      "Epoch 45 Loss: 0.02542661502957344 with sgd\n",
      "Epoch 46 Loss: 0.029104242101311684 with sgd\n",
      "Epoch 47 Loss: 0.014437267556786537 with sgd\n",
      "Epoch 48 Loss: 0.021360833197832108 with sgd\n",
      "Epoch 49 Loss: 0.02936643362045288 with sgd\n",
      "Using adam optimizer\n",
      "Epoch 0 Loss: 2.8752856254577637 with adam\n",
      "Epoch 1 Loss: 1.470510482788086 with adam\n",
      "Epoch 2 Loss: 0.44825491309165955 with adam\n",
      "Epoch 3 Loss: 0.4505835473537445 with adam\n",
      "Epoch 4 Loss: 0.13466231524944305 with adam\n",
      "Epoch 5 Loss: 0.019492056220769882 with adam\n",
      "Epoch 6 Loss: 0.2791755795478821 with adam\n",
      "Epoch 7 Loss: 0.29550406336784363 with adam\n",
      "Epoch 8 Loss: 0.009977603331208229 with adam\n",
      "Epoch 9 Loss: 0.07627006620168686 with adam\n",
      "Epoch 10 Loss: 0.4321337640285492 with adam\n",
      "Epoch 11 Loss: 0.06302230805158615 with adam\n",
      "Epoch 12 Loss: 0.04263658821582794 with adam\n",
      "Epoch 13 Loss: 0.013459394685924053 with adam\n",
      "Epoch 14 Loss: 0.02819485031068325 with adam\n",
      "Epoch 15 Loss: 0.021024001762270927 with adam\n",
      "Epoch 16 Loss: 0.5573112964630127 with adam\n",
      "Epoch 17 Loss: 0.30462902784347534 with adam\n",
      "Epoch 18 Loss: 0.01624794490635395 with adam\n",
      "Epoch 19 Loss: 0.03985648602247238 with adam\n",
      "Epoch 20 Loss: 0.09785536676645279 with adam\n",
      "Epoch 21 Loss: 0.015410658903419971 with adam\n",
      "Epoch 22 Loss: 0.0034809187054634094 with adam\n",
      "Epoch 23 Loss: 0.20334458351135254 with adam\n",
      "Epoch 24 Loss: 0.02087864652276039 with adam\n",
      "Epoch 25 Loss: 0.006301424000412226 with adam\n",
      "Epoch 26 Loss: 0.0017480459064245224 with adam\n",
      "Epoch 27 Loss: 0.000787617638707161 with adam\n",
      "Epoch 28 Loss: 0.0020518989767879248 with adam\n",
      "Epoch 29 Loss: 0.0021494433749467134 with adam\n",
      "Epoch 30 Loss: 0.008984396234154701 with adam\n",
      "Epoch 31 Loss: 0.0016062650829553604 with adam\n",
      "Epoch 32 Loss: 0.006010940298438072 with adam\n",
      "Epoch 33 Loss: 0.0007890945416875184 with adam\n",
      "Epoch 34 Loss: 0.00029689708026126027 with adam\n",
      "Epoch 35 Loss: 0.0028848042711615562 with adam\n",
      "Epoch 36 Loss: 0.0015077400021255016 with adam\n",
      "Epoch 37 Loss: 0.0038353397976607084 with adam\n",
      "Epoch 38 Loss: 0.0002659335150383413 with adam\n",
      "Epoch 39 Loss: 0.0007936400361359119 with adam\n",
      "Epoch 40 Loss: 0.001792841823771596 with adam\n",
      "Epoch 41 Loss: 0.0007487893453799188 with adam\n",
      "Epoch 42 Loss: 0.0004446010570973158 with adam\n",
      "Epoch 43 Loss: 0.00022840856399852782 with adam\n",
      "Epoch 44 Loss: 0.0008484696154482663 with adam\n",
      "Epoch 45 Loss: 0.013495313003659248 with adam\n",
      "Epoch 46 Loss: 0.00030388712184503675 with adam\n",
      "Epoch 47 Loss: 0.003978424239903688 with adam\n",
      "Epoch 48 Loss: 0.0010782995959743857 with adam\n",
      "Epoch 49 Loss: 0.022884204983711243 with adam\n",
      "Using rmsprop optimizer\n",
      "Epoch 0 Loss: 1.3478219509124756 with rmsprop\n",
      "Epoch 1 Loss: 0.2630293667316437 with rmsprop\n",
      "Epoch 2 Loss: 0.09268435835838318 with rmsprop\n",
      "Epoch 3 Loss: 0.018122076988220215 with rmsprop\n",
      "Epoch 4 Loss: 0.03978423401713371 with rmsprop\n",
      "Epoch 5 Loss: 0.009090492501854897 with rmsprop\n",
      "Epoch 6 Loss: 0.0760294646024704 with rmsprop\n",
      "Epoch 7 Loss: 0.004121706355363131 with rmsprop\n",
      "Epoch 8 Loss: 0.06346443295478821 with rmsprop\n",
      "Epoch 9 Loss: 0.07666734606027603 with rmsprop\n",
      "Epoch 10 Loss: 0.08267296105623245 with rmsprop\n",
      "Epoch 11 Loss: 0.07669620215892792 with rmsprop\n",
      "Epoch 12 Loss: 0.00282862177118659 with rmsprop\n",
      "Epoch 13 Loss: 0.0382736399769783 with rmsprop\n",
      "Epoch 14 Loss: 0.006909726653248072 with rmsprop\n",
      "Epoch 15 Loss: 0.007609816733747721 with rmsprop\n",
      "Epoch 16 Loss: 0.002388706197962165 with rmsprop\n",
      "Epoch 17 Loss: 0.00996552873402834 with rmsprop\n",
      "Epoch 18 Loss: 1.87148916721344 with rmsprop\n",
      "Epoch 19 Loss: 0.0035963417030870914 with rmsprop\n",
      "Epoch 20 Loss: 0.0005021976539865136 with rmsprop\n",
      "Epoch 21 Loss: 0.007450888864696026 with rmsprop\n",
      "Epoch 22 Loss: 0.05185065418481827 with rmsprop\n",
      "Epoch 23 Loss: 0.22652645409107208 with rmsprop\n",
      "Epoch 24 Loss: 0.061431415379047394 with rmsprop\n",
      "Epoch 25 Loss: 0.008584858849644661 with rmsprop\n",
      "Epoch 26 Loss: 0.030367080122232437 with rmsprop\n",
      "Epoch 27 Loss: 0.001010419917292893 with rmsprop\n",
      "Epoch 28 Loss: 0.0015071321977302432 with rmsprop\n",
      "Epoch 29 Loss: 0.00011516806262079626 with rmsprop\n",
      "Epoch 30 Loss: 1.4292094707489014 with rmsprop\n",
      "Epoch 31 Loss: 0.02798784151673317 with rmsprop\n",
      "Epoch 32 Loss: 0.052102625370025635 with rmsprop\n",
      "Epoch 33 Loss: 0.02940678596496582 with rmsprop\n",
      "Epoch 34 Loss: 0.04263873025774956 with rmsprop\n",
      "Epoch 35 Loss: 0.0008949503535404801 with rmsprop\n",
      "Epoch 36 Loss: 0.4184674620628357 with rmsprop\n",
      "Epoch 37 Loss: 0.017781395465135574 with rmsprop\n",
      "Epoch 38 Loss: 0.1082894578576088 with rmsprop\n",
      "Epoch 39 Loss: 0.004679164849221706 with rmsprop\n",
      "Epoch 40 Loss: 0.006806317716836929 with rmsprop\n",
      "Epoch 41 Loss: 0.0010165341664105654 with rmsprop\n",
      "Epoch 42 Loss: 0.109329953789711 with rmsprop\n",
      "Epoch 43 Loss: 0.005216502584517002 with rmsprop\n",
      "Epoch 44 Loss: 0.024824876338243484 with rmsprop\n",
      "Epoch 45 Loss: 0.002530676545575261 with rmsprop\n",
      "Epoch 46 Loss: 0.0018823236459866166 with rmsprop\n",
      "Epoch 47 Loss: 0.0023485964629799128 with rmsprop\n",
      "Epoch 48 Loss: 0.0013059636112302542 with rmsprop\n",
      "Epoch 49 Loss: 0.23487024009227753 with rmsprop\n",
      "Using adagrad optimizer\n",
      "Epoch 0 Loss: 0.0001829168468248099 with adagrad\n",
      "Epoch 1 Loss: 0.0006665755645371974 with adagrad\n",
      "Epoch 2 Loss: 0.00021660930360667408 with adagrad\n",
      "Epoch 3 Loss: 0.0001460748171666637 with adagrad\n",
      "Epoch 4 Loss: 3.1947874958859757e-06 with adagrad\n",
      "Epoch 5 Loss: 0.0020585572347044945 with adagrad\n",
      "Epoch 6 Loss: 0.0007235679076984525 with adagrad\n",
      "Epoch 7 Loss: 0.00010376631689723581 with adagrad\n",
      "Epoch 8 Loss: 3.677648419397883e-05 with adagrad\n",
      "Epoch 9 Loss: 0.02168143168091774 with adagrad\n",
      "Epoch 10 Loss: 2.7835130822495557e-06 with adagrad\n",
      "Epoch 11 Loss: 2.4981380192912184e-05 with adagrad\n",
      "Epoch 12 Loss: 1.5019782949821092e-05 with adagrad\n",
      "Epoch 13 Loss: 3.177239705109969e-05 with adagrad\n",
      "Epoch 14 Loss: 0.0004638508544303477 with adagrad\n",
      "Epoch 15 Loss: 7.73769206716679e-05 with adagrad\n",
      "Epoch 16 Loss: 0.0006477978313341737 with adagrad\n",
      "Epoch 17 Loss: 1.4619688954553567e-05 with adagrad\n",
      "Epoch 18 Loss: 0.0004545138799585402 with adagrad\n",
      "Epoch 19 Loss: 0.0005768089322373271 with adagrad\n",
      "Epoch 20 Loss: 1.3666714949067682e-05 with adagrad\n",
      "Epoch 21 Loss: 7.872621063143015e-05 with adagrad\n",
      "Epoch 22 Loss: 0.0001434438454452902 with adagrad\n",
      "Epoch 23 Loss: 6.295515049714595e-05 with adagrad\n",
      "Epoch 24 Loss: 0.00016793003305792809 with adagrad\n",
      "Epoch 25 Loss: 6.758811650797725e-06 with adagrad\n",
      "Epoch 26 Loss: 0.00020599337585736066 with adagrad\n",
      "Epoch 27 Loss: 0.00018000339332502335 with adagrad\n",
      "Epoch 28 Loss: 7.909123814897612e-05 with adagrad\n",
      "Epoch 29 Loss: 0.0009522325126454234 with adagrad\n",
      "Epoch 30 Loss: 6.759747338946909e-05 with adagrad\n",
      "Epoch 31 Loss: 4.696759788203053e-06 with adagrad\n",
      "Epoch 32 Loss: 8.344639468305104e-07 with adagrad\n",
      "Epoch 33 Loss: 7.223378634080291e-05 with adagrad\n",
      "Epoch 34 Loss: 0.00013127413694746792 with adagrad\n",
      "Epoch 35 Loss: 1.27248740682262e-05 with adagrad\n",
      "Epoch 36 Loss: 7.374531560344622e-05 with adagrad\n",
      "Epoch 37 Loss: 0.00013514715828932822 with adagrad\n",
      "Epoch 38 Loss: 8.046323273447342e-06 with adagrad\n",
      "Epoch 39 Loss: 9.822521860769484e-06 with adagrad\n",
      "Epoch 40 Loss: 0.00050025136442855 with adagrad\n",
      "Epoch 41 Loss: 7.076140900608152e-05 with adagrad\n",
      "Epoch 42 Loss: 2.6643042474461254e-06 with adagrad\n",
      "Epoch 43 Loss: 9.583334758644924e-05 with adagrad\n",
      "Epoch 44 Loss: 0.0004720145370811224 with adagrad\n",
      "Epoch 45 Loss: 4.863551293965429e-05 with adagrad\n",
      "Epoch 46 Loss: 1.8879669369198382e-05 with adagrad\n",
      "Epoch 47 Loss: 7.980898226378486e-05 with adagrad\n",
      "Epoch 48 Loss: 0.00020644029427785426 with adagrad\n",
      "Epoch 49 Loss: 6.008020591252716e-06 with adagrad\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizers = {\n",
    "        'sgd': optim.SGD(model.parameters(), lr=LR, momentum=0.9),\n",
    "        'adam': optim.Adam(model.parameters(), lr=LR),\n",
    "        'rmsprop': optim.RMSprop(model.parameters(), lr=LR),\n",
    "        'adagrad': optim.Adagrad(model.parameters(), lr=LR)\n",
    "    }\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "for optimizer_name, optimizer in optimizers.items():\n",
    "    print(f\"Using {optimizer_name} optimizer\")\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_history.append(loss.item())\n",
    "        print(f'Epoch {epoch} Loss: {loss.item()} with {optimizer_name}')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 11928,
     "sourceId": 16420,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
