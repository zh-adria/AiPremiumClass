{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch搭建神经网络model\n",
    "    训练模型，实现给定图片就能识别种类\n",
    "    分析：\n",
    "        60000张图片去训练模型，每张图片获取一个最优的参数\n",
    "        1、输入层：60000-gray图片，28*28像素， X.shape=(60000,784)\n",
    "        2、隐藏层：20个神经元 参数矩阵shape=(784,20) 偏置bias shape=(20,)\n",
    "        3、输出层：10个神经元(10个类别) 参数矩阵shape=(10,5) 偏置bias-shape=(5,) Y输出shape=(,5)\n",
    "        4、损失函数：交叉熵损失函数\n",
    "    备注：\n",
    "        1、每张图片都会被隐藏层中的所有20个神经元处理 每一列对应一个神经元的参数矩阵 -> (20,)\n",
    "        2、​隐藏层的20个神经元共同为每张图片生成一个20维的特征向量\n",
    "        3、综上，对所有样本进行损失求导和参数优化会带来巨大的计算量和内存需求，因此需要采用批量梯度下降法\n",
    "            基本思想是将整个数据集分成若干个小批量，每次迭代时只计算一个小批量的损失和梯度，然后更新参数。这样可以大大减少计算量和内存需求，同时也可以提高训练的效率和稳定性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms.v2 import ToTensor\n",
    "from torchvision.datasets import FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = FashionMNIST(\"./data\", train=True, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "epochs = 1000\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隐藏层\n",
    "# 线性层\n",
    "linear = nn.Linear(in_features=784, out_features=10, bias=True)\n",
    "# 激活函数\n",
    "act = nn.Sigmoid()\n",
    "\n",
    "# 隐藏层\n",
    "# 线性层\n",
    "linear2 = nn.Linear(in_features=10, out_features=5, bias=True)\n",
    "# 激活函数\n",
    "act2 = nn.Sigmoid()\n",
    "\n",
    "# 模拟输出\n",
    "x = torch.randn(10, 784)\n",
    "out = linear(x)\n",
    "out2 = act(out)\n",
    "\n",
    "x2 = linear2(out2)\n",
    "out3 = act2(x2)\n",
    "\n",
    "# 每个类别的概率值-未归一化-取值区间宽泛-每个类别概率占比-总和不为1，适合二分类问题\n",
    "print(out3)\n",
    "# 转化为概率分布的函数-全部类别概率占比-总和1-适合多分类问题\n",
    "softmax = nn.Softmax(dim=1)\n",
    "final = softmax(out3)\n",
    "print(final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型结构串联在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隐藏层\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 10),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(10, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定制模型损失函数和优化器 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # 交叉熵损失函数\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr) # 优化器-完成梯度更新\n",
    "# [parm for parm in model.parameters()] # 模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练并观察超参数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
