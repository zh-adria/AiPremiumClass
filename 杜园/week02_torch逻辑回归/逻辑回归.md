# 逻辑回归
## 1. 简介
    逻辑回归是一种分类算法，其输出为0或1。
## 2. 背景
    逻辑回归学习背景：神经网络是深度学习的基础，是当前主流网络模型应用最多的网络架构；学习神经网络入门方法就是逻辑回归。
    逻辑回归的应用场景举例：用户是否会购买、房价是否会上涨。
## 2. 原理
    逻辑回归的原理是将输入数据映射到0或1的概率上。
    逻辑回归的损失函数为交叉熵损失函数。
    逻辑回归的优化算法为梯度下降算法。

## 3. 线性回归
### 3.1 线性回归目标
    线性回归的目标是找到一条直线，使得直线与数据点的误差最小。
### 3.2 线性回归损失函数
    线性回归的损失函数为均方误差损失函数(最小二乘法-假设误差是独立同分布的)。
### 3.3 线性回归最优拟合曲线求取过程
    ① 对于给定的一组数据点，找到一条直线或曲线，使得这些数据点到这条直线或曲线的垂直距离的平方和最小。
    ② 通过对平方和求关于w和b的偏导数(损失函数)，得到w和b的更新公式。
    ③ 利用更新公式，不断迭代，直到w和b的变化很小，即达到收敛条件(上一个w和b带入损失函数得到的值和本次差值小于某个极小的数）。
    ④ 得到w和b的值，即直线或曲线的参数。
    多元线性回归的参数估计：
    y = wx + b
    w = (x.T * x)^-1 * x.T * y
    b = y - w * x
### 3.4 最小二乘法缺点
    1、对异常值敏感，即对异常值的影响很大。
    2、是对非线性数据敏感，即对非线性数据的拟合效果不好。
    3、对数据量敏感，即对数据量的要求较高。
    4、对数据分布敏感，即对数据分布的要求较高。
    为此，引入逻辑回归模型。

## 4. 逻辑回归
### 4.1 逻辑回归目标
    逻辑回归的目标将线性回归模型映射为概率模型，即将输入数据映射到0或1的概率上。
    举例：求n个样本同时发生概率最大值，即损失函数损失函数最小的时候求取θ向量最优解
### 4.2 逻辑回归损失函数
    逻辑回归的损失函数为交叉熵损失函数。
    原预测函数带入sigmoid函数求事件发生概率最大值（取反即求取损失函数最小值）,得到损失函数。
### 4.3 逻辑回归最优拟合曲线求取过程
    ① 针对二分类逻辑回归问题假设函数。
    ② 将假设函数带入Sigmoid函数映射到0~1之间。
    ③ 基于损失函数，计算损失函数的梯度。
    ④ 利用梯度下降算法，更新参数。
    ⑤ 重复③、④，直到损失函数收敛。
    举例：
        求n个样本同时发生概率最大值，即损失函数损失函数最小的时候求取θ向量最优解
        分析：
            ① 原型函数：假设函数为符合正态分布的概率密度函数；
            ② 损失函数：假设函数带入Sigmoid函数映射到0~1之间，得到概率密度函数；概率密度函数的乘积就是n个样本同时发生的概率（乘积没有求和简单，过程中损失函数映射为对数函数，求对数函数最大值相当于：对数函数加个负号求取损失函数最小值-因为是n个样本，所以损失函数J(θ)/n）；
            ③ 损失函数的梯度：损失函数分别对θ、bias求偏导；
            ④ 利用梯度下降算法，更新参数；
            ⑤ 重复③、④，直到损失函数收敛。
        求解：
            ① 初始化参数(样本数据X_train、学习率α、迭代次数epochs、初始化特征向量参数β和截距bias)；
            ② 定义损失函数（分析中求得的）
            ③ 定义模型计算函数（原函数导入Sigmoid后得到的函数-目的求取概率密度函数预测值）
            ④ 定义梯度下降函数（损失函数对θ求偏导，损失函数对bias求导）
            ⑤ 定义训练函数（循环迭代epochs次，每次迭代更新参数）
            ⑥ 调用训练函数，得到最优参数。
