CBOW：给定⼀组（相邻）单词，猜测可能与这组单词⼀起出现的单个单词。
CBOW模型的输⼊是某⼀个特定词的上下⽂相关的词对应的词向量。这段上下⽂中关注词的
前后各对应3个词，数值3也称为窗⼝⼤⼩(window size)。上下⽂词汇共6个，模型的输⼊
是6个词的词向量。输出是所有词617个)的softmax概率。训练的⽬标是训练样本中特定
词对应的softmax概率最⼤化。
通过深度神经⽹络的反向传播算法，可以训练出模型的参数，也就得到了所有的词对应的词
向量。
Skip-gram：根据正在分析的单个单词猜测潜在的相邻单词。
Skip-Gram模型和CBOW的训练思路正好相反，输⼊的是特定的⼀个词的词向量，⽽输出是
特定词对应的上下⽂词向量。还是上⾯的例⼦，我们的上下⽂⼤⼩取值为3， 特定的这个
词"糊涂"是模型的输⼊，⽽这6个上下⽂词是模型的输出。
在这个Skip-Gram的例⼦⾥，我们的输⼊是词， 输出是softmax概率排前6的6个词，对应的
Skip-Gram神经⽹络模型输⼊层有1个神经元，输出层有词汇表⼤⼩个神经元。训练的⽬标是
训练样本对应softmax概率⼤⼩排前n个的特定词所对应的词。
通过深度神经⽹络的反向传播算法，可以训练出模型的参数，同时得到了所有的词对应的词
向量。