CBOW ： 给定 ⼀ 组 （ 相邻 ） 单词 ， 猜测 可能 与 这组 单词 ⼀ 起 出现 的 单个 单词 。
CBOW 模型 的 输 ⼊ 是 某 ⼀ 个 特定 词 的 上下 ⽂ 相关 的 词 对应 的 词 向量 。 这 段 上下 ⽂ 中 关注 词 的
前后 各 对应 3 个 词 ， 数值 3 也 称为 窗 ⼝ ⼤ ⼩ ( window   size ) 。 上下 ⽂ 词汇 共 6 个 ， 模型 的 输 ⼊
是 6 个 词 的 词 向量 。 输出 是 所有 词  6  1  7 个 ) 的 softmax 概率 。 训练 的 ⽬ 标是 训练样本 中 特定
词 对应 的 softmax 概率 最 ⼤ 化 。
通过 深度 神经 ⽹ 络 的 反向 传播 算法 ， 可以 训练 出 模型 的 参数 ， 也 就 得到 了 所有 的 词 对应 的 词
向量 。
Skip - gram ： 根据 正在 分析 的 单个 单词 猜测 潜在 的 相邻 单词 。
Skip - Gram 模型 和 CBOW 的 训练 思路 正好 相反 ， 输 ⼊ 的 是 特定 的 ⼀ 个 词 的 词 向量 ， ⽽ 输出 是
特定 词 对应 的 上下 ⽂ 词 向量 。 还是 上 ⾯ 的 例 ⼦ ， 我们 的 上下 ⽂ ⼤ ⼩ 取值 为 3 ，   特定 的 这个
词 " 糊涂 " 是 模型 的 输 ⼊ ， ⽽ 这 6 个 上下 ⽂ 词是 模型 的 输出 。
在 这个 Skip - Gram 的 例 ⼦ ⾥ ， 我们 的 输 ⼊ 是 词 ，   输出 是 softmax 概率 排前 6 的 6 个 词 ， 对应 的
Skip - Gram 神经 ⽹ 络 模型 输 ⼊ 层有 1 个 神经元 ， 输出 层有 词汇表 ⼤ ⼩ 个 神经元 。 训练 的 ⽬ 标是
训练样本 对应 softmax 概率 ⼤ ⼩ 排前 n 个 的 特定 词所 对应 的 词 。
通过 深度 神经 ⽹ 络 的 反向 传播 算法 ， 可以 训练 出 模型 的 参数 ， 同时 得到 了 所有 的 词 对应 的 词
向量 。
