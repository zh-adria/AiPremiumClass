{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pytorch搭建神经网络模型，尝试调整模型结构（变更神经元数量，增加隐藏层）来提升模型预测的准确率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入包\n",
    "import torch\n",
    "from torchvision.datasets import KMNIST\n",
    "from torchvision.transforms import ToTensor \n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义超参数\n",
    "LR = 1e-3\n",
    "epochs = 100 #训练轮次，100轮左右损失率降到了1以下，1000轮降到0.3左右，Accuracy: 76.03%\n",
    "BATCH_SIZE = 128  #每批64张\n",
    "\n",
    "\n",
    "#数据集加载\n",
    "train_data = KMNIST(root='./kmnist_data', train=True, download=True, transform=ToTensor())\n",
    "test_data = KMNIST(root='./kmnist_data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "#数据分批次处理\n",
    "train_dl = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True) # shuffle：随机打乱数据\n",
    "\n",
    "\n",
    "# 模型创建 \n",
    "# 神经元增加 （训练100轮，神经元256，Accuracy: 60.11%, 神经元1000，Accuracy: 61.39%）=》增加神经元对损失率影响不大\n",
    "# 增加1个隐藏层（隐藏层2层，训练100轮，神经元256，激活函数选ReLU,Accuracy: 73.64%）sigmoid函数增加隐藏层，损失基本不变\n",
    "# 隐藏层仅1层，使用ReLU函数，神经元256个，Accuracy: 72.8%\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 256),  #输入层\n",
    "    nn.ReLU(),     #隐藏层\n",
    "    nn.Linear(256, 128),  # 新增的隐藏层\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)  #输出层\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss:2.2180464267730713\n",
      "Epoch:1, Loss:2.1495659351348877\n",
      "Epoch:2, Loss:2.0579543113708496\n",
      "Epoch:3, Loss:1.884940266609192\n",
      "Epoch:4, Loss:1.771724820137024\n",
      "Epoch:5, Loss:1.5104624032974243\n",
      "Epoch:6, Loss:1.4477630853652954\n",
      "Epoch:7, Loss:1.4398494958877563\n",
      "Epoch:8, Loss:1.4162606000900269\n",
      "Epoch:9, Loss:1.3857475519180298\n",
      "Epoch:10, Loss:1.2009378671646118\n",
      "Epoch:11, Loss:1.078429102897644\n",
      "Epoch:12, Loss:1.199627161026001\n",
      "Epoch:13, Loss:0.9566478729248047\n",
      "Epoch:14, Loss:0.9635658264160156\n",
      "Epoch:15, Loss:0.9904351830482483\n",
      "Epoch:16, Loss:1.065567135810852\n",
      "Epoch:17, Loss:0.9593588709831238\n",
      "Epoch:18, Loss:0.796842634677887\n",
      "Epoch:19, Loss:0.7784572243690491\n",
      "Epoch:20, Loss:0.8787466883659363\n",
      "Epoch:21, Loss:0.8522575497627258\n",
      "Epoch:22, Loss:0.7846777439117432\n",
      "Epoch:23, Loss:0.9622738361358643\n",
      "Epoch:24, Loss:0.6354860663414001\n",
      "Epoch:25, Loss:0.8893347382545471\n",
      "Epoch:26, Loss:0.8809974193572998\n",
      "Epoch:27, Loss:0.7500686645507812\n",
      "Epoch:28, Loss:0.6516140699386597\n",
      "Epoch:29, Loss:0.6952914595603943\n",
      "Epoch:30, Loss:0.6180683970451355\n",
      "Epoch:31, Loss:0.8115725517272949\n",
      "Epoch:32, Loss:0.7563114762306213\n",
      "Epoch:33, Loss:0.687819242477417\n",
      "Epoch:34, Loss:0.6194580793380737\n",
      "Epoch:35, Loss:0.7095237374305725\n",
      "Epoch:36, Loss:0.6186735033988953\n",
      "Epoch:37, Loss:0.6801381707191467\n",
      "Epoch:38, Loss:0.8736951351165771\n",
      "Epoch:39, Loss:0.6135116219520569\n",
      "Epoch:40, Loss:0.4916589558124542\n",
      "Epoch:41, Loss:0.6347216963768005\n",
      "Epoch:42, Loss:0.5828831195831299\n",
      "Epoch:43, Loss:0.5788007974624634\n",
      "Epoch:44, Loss:0.6424964070320129\n",
      "Epoch:45, Loss:0.5822351574897766\n",
      "Epoch:46, Loss:0.6223438382148743\n",
      "Epoch:47, Loss:0.7938445210456848\n",
      "Epoch:48, Loss:0.7515661716461182\n",
      "Epoch:49, Loss:0.6139587163925171\n",
      "Epoch:50, Loss:0.4756089746952057\n",
      "Epoch:51, Loss:0.7570062279701233\n",
      "Epoch:52, Loss:0.7295376658439636\n",
      "Epoch:53, Loss:0.8101315498352051\n",
      "Epoch:54, Loss:0.5489792227745056\n",
      "Epoch:55, Loss:0.4514876902103424\n",
      "Epoch:56, Loss:0.7521713376045227\n",
      "Epoch:57, Loss:0.8570248484611511\n",
      "Epoch:58, Loss:0.5469586849212646\n",
      "Epoch:59, Loss:0.7493560314178467\n",
      "Epoch:60, Loss:0.6356157660484314\n",
      "Epoch:61, Loss:0.5164125561714172\n",
      "Epoch:62, Loss:0.5989291071891785\n",
      "Epoch:63, Loss:0.42757749557495117\n",
      "Epoch:64, Loss:0.6294258236885071\n",
      "Epoch:65, Loss:0.49609628319740295\n",
      "Epoch:66, Loss:0.5155745148658752\n",
      "Epoch:67, Loss:0.47468045353889465\n",
      "Epoch:68, Loss:0.5647097229957581\n",
      "Epoch:69, Loss:0.4175620377063751\n",
      "Epoch:70, Loss:0.7453136444091797\n",
      "Epoch:71, Loss:0.5970003008842468\n",
      "Epoch:72, Loss:0.5542365908622742\n",
      "Epoch:73, Loss:0.7523362636566162\n",
      "Epoch:74, Loss:0.38898658752441406\n",
      "Epoch:75, Loss:0.5336686968803406\n",
      "Epoch:76, Loss:0.4486648738384247\n",
      "Epoch:77, Loss:0.7013669610023499\n",
      "Epoch:78, Loss:0.4690190851688385\n",
      "Epoch:79, Loss:0.593167781829834\n",
      "Epoch:80, Loss:0.39806362986564636\n",
      "Epoch:81, Loss:0.6011973023414612\n",
      "Epoch:82, Loss:0.5329095125198364\n",
      "Epoch:83, Loss:0.49939072132110596\n",
      "Epoch:84, Loss:0.6119036078453064\n",
      "Epoch:85, Loss:0.5843225121498108\n",
      "Epoch:86, Loss:0.36435481905937195\n",
      "Epoch:87, Loss:0.5074822306632996\n",
      "Epoch:88, Loss:0.4738713204860687\n",
      "Epoch:89, Loss:0.45027968287467957\n",
      "Epoch:90, Loss:0.6053585410118103\n",
      "Epoch:91, Loss:0.46977493166923523\n",
      "Epoch:92, Loss:0.48850229382514954\n",
      "Epoch:93, Loss:0.4072517454624176\n",
      "Epoch:94, Loss:0.4948232173919678\n",
      "Epoch:95, Loss:0.43041709065437317\n",
      "Epoch:96, Loss:0.6657861471176147\n",
      "Epoch:97, Loss:0.7440062165260315\n",
      "Epoch:98, Loss:0.4736603796482086\n",
      "Epoch:99, Loss:0.4034760892391205\n"
     ]
    }
   ],
   "source": [
    "#损失函数和优化器\n",
    "loss_fn = nn.CrossEntropyLoss() #交叉熵损失函数\n",
    "\n",
    "#优化器（模型参数更新）\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR) #SGD优化器\n",
    "\n",
    "###加上训练轮次\n",
    "# 训练加速，数据加载器，分批次\n",
    "for epoch in range(epochs):\n",
    "    for data, target in train_dl:\n",
    "        #前向运算\n",
    "        output = model(data.reshape(-1, 784))\n",
    "        #计算损失\n",
    "        loss = loss_fn(output, target) #计算梯度\n",
    "        #反向传播\n",
    "        optimizer.zero_grad() #所有参数梯度清零\n",
    "        loss.backward()  #计算梯度（参数.grad）\n",
    "        optimizer.step() #更新参数\n",
    "    print(f'Epoch:{epoch}, Loss:{loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.8%\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "test_dl = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad(): #不计算梯度\n",
    "    for data, target in test_dl:\n",
    "        output = model(data.reshape(-1, 784))\n",
    "        _, predicted = torch.max(output, 1) #返回第一个维度的最大值张量\n",
    "        total += target.size(0) #返回张量的样本量\n",
    "        correct += (predicted == target).sum().item()\n",
    "print(f'Accuracy: {correct/total*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
