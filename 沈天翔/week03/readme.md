## 1.第三周作业

1. 使用pytorch搭建神经网络模型，实现对KMNIST数据集的训练。
2. 尝试调整模型结构（变更神经元数量，增加隐藏层）来提升模型预测的准确率。
3. 调试超参数，观察学习率和批次大小对训练的影响。

## 2.训练数据

### 1.1.超参数影响
1. 学习率设为 1e-3，批次设为 128，训练 loss 从 2.2914392948150635 降为 1.9529975652694702，训练时间 2min28s
2. 学习率设为 1e-3，批次设为 1234，训练 loss 从 2.3243138790130615 降为 2.278895616531372，训练时间 1min55s
3. 学习率设为 1e-2，批次设为 1234，训练 loss 从 2.3009696006774902 降为 1.9860433340072632，训练时间 1min55s
4. 学习率设为 1e-2，批次设为 128，训练 loss 从 2.177506923675537 降为 0.7405412793159485，训练时间 2min28s
5. 学习率设为 1e-1，批次设为 128，训练 loss 从 1.0824363231658936 降为 0.20801061391830444，训练时间 2min32s

### 1.2.模型结构影响
1. 由上述可知，学习率设为 1e-1，批次设为 128，效果最好，在此基础上调整模型结构
2. 变更神经元数量从 64 增加为 128，训练 loss 从 0.894407331943512 降为 0.2868447005748749，训练时间 2min37s
3. 变更神经元数量从 64 减少为 32，训练 loss 从 0.9547157287597656 降为 0.4009294807910919，训练时间 2min30s
4. 由上述可知，神经元数量为64时效果最好，于是增加隐藏层，继续训练
5. model = nn.Sequential(
    nn.Linear(784, 512),
    nn.Sigmoid(),
    nn.Linear(512, 128),
    nn.Linear(128, 64),
    nn.Linear(64, 32),
    nn.Linear(32, 10)
), 新增三个隐藏层，可以减少数据失真，通过不断测试，将学习率设为0.2，训练 loss 从 0.8577246069908142 降为  0.09024099260568619，训练时间 2min46s，极大提高了准确率

## 3.实验结论

1. 适当增大学习率，会加快 loss 值变小的速度
2. 增大批次，会增加训练时间且减慢 loss 值变小的速度
3. 增加神经元的个数，同时增加隐藏层数量，以减少数据失真，对 loss 值的变化影响极大，但是会增加改变训练的时间，此时可以适当加大学习率，加快 loss 值变小的速度的同时可以平衡模型的稳定性
4. 最终模型参数为：学习率设为 0.2，批次设为 128，神经元数量64，模型隐藏层为model = nn.Sequential( nn.Linear(784, 512), nn.Sigmoid(), nn.Linear(512, 128), nn.Linear(128, 64), nn.Linear(64, 32), nn.Linear(32, 10) )，此时模型效果最好，准确率为85.97%
