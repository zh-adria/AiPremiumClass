{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 调整学习率，观察模型训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 生成训练数据\n",
    "X,y = load_iris(return_X_y=True)\n",
    "\n",
    "X = X[:100]\n",
    "y = y[:100]\n",
    "\n",
    "# 数据拆分\n",
    "# 局部样本训练模型（过拟合模型）测试预测不好\n",
    "# 新样本数据模型表现不好（泛化能力差）\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 超参数\n",
    "lrs = [0.1, 0.01, 0.001]  # 学习率\n",
    "epochs = 500  # 训练次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:0.1 epoch: 0, loss: 6.737876089832445, acc: 0.5\n",
      "lr:0.1 epoch: 10, loss: 0.0035842303535322403, acc: 1.0\n",
      "lr:0.1 epoch: 20, loss: 0.003320793191357164, acc: 1.0\n",
      "lr:0.1 epoch: 30, loss: 0.003099982151563609, acc: 1.0\n",
      "lr:0.1 epoch: 40, loss: 0.0029081560928105018, acc: 1.0\n",
      "lr:0.1 epoch: 50, loss: 0.002739870040358993, acc: 1.0\n",
      "lr:0.1 epoch: 60, loss: 0.002590975880900283, acc: 1.0\n",
      "lr:0.1 epoch: 70, loss: 0.002458252794282741, acc: 1.0\n",
      "lr:0.1 epoch: 80, loss: 0.0023391608459611473, acc: 1.0\n",
      "lr:0.1 epoch: 90, loss: 0.0022316693854515625, acc: 1.0\n",
      "lr:0.1 epoch: 100, loss: 0.002134134980100507, acc: 1.0\n",
      "lr:0.1 epoch: 110, loss: 0.0020452129397813858, acc: 1.0\n",
      "lr:0.1 epoch: 120, loss: 0.001963792102702569, acc: 1.0\n",
      "lr:0.1 epoch: 130, loss: 0.001888946034557314, acc: 1.0\n",
      "lr:0.1 epoch: 140, loss: 0.0018198960070110117, acc: 1.0\n",
      "lr:0.1 epoch: 150, loss: 0.0017559825606817043, acc: 1.0\n",
      "lr:0.1 epoch: 160, loss: 0.0016966434124230738, acc: 1.0\n",
      "lr:0.1 epoch: 170, loss: 0.0016413961116984739, acc: 1.0\n",
      "lr:0.1 epoch: 180, loss: 0.0015898242939561707, acc: 1.0\n",
      "lr:0.1 epoch: 190, loss: 0.001541566688067343, acc: 1.0\n",
      "lr:0.1 epoch: 200, loss: 0.00149630825364618, acc: 1.0\n",
      "lr:0.1 epoch: 210, loss: 0.0014537729809018566, acc: 1.0\n",
      "lr:0.1 epoch: 220, loss: 0.001413717999475318, acc: 1.0\n",
      "lr:0.1 epoch: 230, loss: 0.0013759287262287809, acc: 1.0\n",
      "lr:0.1 epoch: 240, loss: 0.0013402148438889211, acc: 1.0\n",
      "lr:0.1 epoch: 250, loss: 0.0013064069488271008, acc: 1.0\n",
      "lr:0.1 epoch: 260, loss: 0.0012743537413117973, acc: 1.0\n",
      "lr:0.1 epoch: 270, loss: 0.001243919658292116, acc: 1.0\n",
      "lr:0.1 epoch: 280, loss: 0.0012149828693052743, acc: 1.0\n",
      "lr:0.1 epoch: 290, loss: 0.001187433572003789, acc: 1.0\n",
      "lr:0.1 epoch: 300, loss: 0.0011611725362001587, acc: 1.0\n",
      "lr:0.1 epoch: 310, loss: 0.0011361098550669846, acc: 1.0\n",
      "lr:0.1 epoch: 320, loss: 0.001112163869827039, acc: 1.0\n",
      "lr:0.1 epoch: 330, loss: 0.001089260240389118, acc: 1.0\n",
      "lr:0.1 epoch: 340, loss: 0.0010673311392795707, acc: 1.0\n",
      "lr:0.1 epoch: 350, loss: 0.0010463145501570068, acc: 1.0\n",
      "lr:0.1 epoch: 360, loss: 0.0010261536553790277, acc: 1.0\n",
      "lr:0.1 epoch: 370, loss: 0.0010067962996755607, acc: 1.0\n",
      "lr:0.1 epoch: 380, loss: 0.0009881945190938829, acc: 1.0\n",
      "lr:0.1 epoch: 390, loss: 0.0009703041261109737, acc: 1.0\n",
      "lr:0.1 epoch: 400, loss: 0.0009530843432340372, acc: 1.0\n",
      "lr:0.1 epoch: 410, loss: 0.00093649747858933, acc: 1.0\n",
      "lr:0.1 epoch: 420, loss: 0.0009205086379778, acc: 1.0\n",
      "lr:0.1 epoch: 430, loss: 0.0009050854686919277, acc: 1.0\n",
      "lr:0.1 epoch: 440, loss: 0.0008901979310711001, acc: 1.0\n",
      "lr:0.1 epoch: 450, loss: 0.0008758180943452354, acc: 1.0\n",
      "lr:0.1 epoch: 460, loss: 0.0008619199537995486, acc: 1.0\n",
      "lr:0.1 epoch: 470, loss: 0.0008484792667008115, acc: 1.0\n",
      "lr:0.1 epoch: 480, loss: 0.0008354734047719286, acc: 1.0\n",
      "lr:0.1 epoch: 490, loss: 0.0008228812212947194, acc: 1.0\n",
      "**************************************************\n",
      "lr:0.01 epoch: 0, loss: 4.559739982081287, acc: 0.5\n",
      "lr:0.01 epoch: 10, loss: 0.5282447779996542, acc: 0.5\n",
      "lr:0.01 epoch: 20, loss: 0.17735727579631605, acc: 1.0\n",
      "lr:0.01 epoch: 30, loss: 0.11708649430480059, acc: 1.0\n",
      "lr:0.01 epoch: 40, loss: 0.08727395612451738, acc: 1.0\n",
      "lr:0.01 epoch: 50, loss: 0.06961344495939707, acc: 1.0\n",
      "lr:0.01 epoch: 60, loss: 0.057958626317844784, acc: 1.0\n",
      "lr:0.01 epoch: 70, loss: 0.049696821245819194, acc: 1.0\n",
      "lr:0.01 epoch: 80, loss: 0.04353526776438351, acc: 1.0\n",
      "lr:0.01 epoch: 90, loss: 0.038762819189922006, acc: 1.0\n",
      "lr:0.01 epoch: 100, loss: 0.034956476843705155, acc: 1.0\n",
      "lr:0.01 epoch: 110, loss: 0.03184906319901816, acc: 1.0\n",
      "lr:0.01 epoch: 120, loss: 0.029263593393417255, acc: 1.0\n",
      "lr:0.01 epoch: 130, loss: 0.027078205580678156, acc: 1.0\n",
      "lr:0.01 epoch: 140, loss: 0.025206260467260273, acc: 1.0\n",
      "lr:0.01 epoch: 150, loss: 0.023584475994479874, acc: 1.0\n",
      "lr:0.01 epoch: 160, loss: 0.022165553331735074, acc: 1.0\n",
      "lr:0.01 epoch: 170, loss: 0.020913429584545305, acc: 1.0\n",
      "lr:0.01 epoch: 180, loss: 0.019800127280835068, acc: 1.0\n",
      "lr:0.01 epoch: 190, loss: 0.01880360752652483, acc: 1.0\n",
      "lr:0.01 epoch: 200, loss: 0.017906272709125114, acc: 1.0\n",
      "lr:0.01 epoch: 210, loss: 0.01709390051989644, acc: 1.0\n",
      "lr:0.01 epoch: 220, loss: 0.016354870997119603, acc: 1.0\n",
      "lr:0.01 epoch: 230, loss: 0.015679596740412086, acc: 1.0\n",
      "lr:0.01 epoch: 240, loss: 0.015060096606225698, acc: 1.0\n",
      "lr:0.01 epoch: 250, loss: 0.014489672426576108, acc: 1.0\n",
      "lr:0.01 epoch: 260, loss: 0.013962660825088014, acc: 1.0\n",
      "lr:0.01 epoch: 270, loss: 0.013474240532801685, acc: 1.0\n",
      "lr:0.01 epoch: 280, loss: 0.013020281241030251, acc: 1.0\n",
      "lr:0.01 epoch: 290, loss: 0.012597223904077828, acc: 1.0\n",
      "lr:0.01 epoch: 300, loss: 0.012201985110620817, acc: 1.0\n",
      "lr:0.01 epoch: 310, loss: 0.0118318800584049, acc: 1.0\n",
      "lr:0.01 epoch: 320, loss: 0.011484560040898867, acc: 1.0\n",
      "lr:0.01 epoch: 330, loss: 0.011157961351760306, acc: 1.0\n",
      "lr:0.01 epoch: 340, loss: 0.010850263244808114, acc: 1.0\n",
      "lr:0.01 epoch: 350, loss: 0.010559853129876614, acc: 1.0\n",
      "lr:0.01 epoch: 360, loss: 0.010285297591254183, acc: 1.0\n",
      "lr:0.01 epoch: 370, loss: 0.010025318122418672, acc: 1.0\n",
      "lr:0.01 epoch: 380, loss: 0.009778770704735582, acc: 1.0\n",
      "lr:0.01 epoch: 390, loss: 0.009544628537501942, acc: 1.0\n",
      "lr:0.01 epoch: 400, loss: 0.009321967365819509, acc: 1.0\n",
      "lr:0.01 epoch: 410, loss: 0.009109952961217143, acc: 1.0\n",
      "lr:0.01 epoch: 420, loss: 0.00890783039504498, acc: 1.0\n",
      "lr:0.01 epoch: 430, loss: 0.008714914811882366, acc: 1.0\n",
      "lr:0.01 epoch: 440, loss: 0.008530583463617963, acc: 1.0\n",
      "lr:0.01 epoch: 450, loss: 0.008354268807553907, acc: 1.0\n",
      "lr:0.01 epoch: 460, loss: 0.008185452506195661, acc: 1.0\n",
      "lr:0.01 epoch: 470, loss: 0.008023660194106785, acc: 1.0\n",
      "lr:0.01 epoch: 480, loss: 0.007868456899709924, acc: 1.0\n",
      "lr:0.01 epoch: 490, loss: 0.007719443028272359, acc: 1.0\n",
      "**************************************************\n",
      "lr:0.001 epoch: 0, loss: 2.2375986623373705, acc: 0.5\n",
      "lr:0.001 epoch: 10, loss: 1.0812813740749547, acc: 0.4625\n",
      "lr:0.001 epoch: 20, loss: 0.895909423531102, acc: 0.2625\n",
      "lr:0.001 epoch: 30, loss: 0.7778132256826809, acc: 0.4\n",
      "lr:0.001 epoch: 40, loss: 0.6797204005637351, acc: 0.5375\n",
      "lr:0.001 epoch: 50, loss: 0.5983055383330583, acc: 0.7375\n",
      "lr:0.001 epoch: 60, loss: 0.5306853366968916, acc: 0.925\n",
      "lr:0.001 epoch: 70, loss: 0.4743188817820931, acc: 0.9875\n",
      "lr:0.001 epoch: 80, loss: 0.42707090185615826, acc: 1.0\n",
      "lr:0.001 epoch: 90, loss: 0.3871990885098709, acc: 1.0\n",
      "lr:0.001 epoch: 100, loss: 0.3533055482877126, acc: 1.0\n",
      "lr:0.001 epoch: 110, loss: 0.32427807176475376, acc: 1.0\n",
      "lr:0.001 epoch: 120, loss: 0.29923412908656644, acc: 1.0\n",
      "lr:0.001 epoch: 130, loss: 0.27747269751698445, acc: 1.0\n",
      "lr:0.001 epoch: 140, loss: 0.25843498546492505, acc: 1.0\n",
      "lr:0.001 epoch: 150, loss: 0.24167333600926, acc: 1.0\n",
      "lr:0.001 epoch: 160, loss: 0.22682699290012476, acc: 1.0\n",
      "lr:0.001 epoch: 170, loss: 0.21360335898991156, acc: 1.0\n",
      "lr:0.001 epoch: 180, loss: 0.20176353935259753, acc: 1.0\n",
      "lr:0.001 epoch: 190, loss: 0.19111118148563885, acc: 1.0\n",
      "lr:0.001 epoch: 200, loss: 0.18148383612272218, acc: 1.0\n",
      "lr:0.001 epoch: 210, loss: 0.1727462411374555, acc: 1.0\n",
      "lr:0.001 epoch: 220, loss: 0.1647850739359325, acc: 1.0\n",
      "lr:0.001 epoch: 230, loss: 0.15750482829744286, acc: 1.0\n",
      "lr:0.001 epoch: 240, loss: 0.15082455566775513, acc: 1.0\n",
      "lr:0.001 epoch: 250, loss: 0.1446752742103698, acc: 1.0\n",
      "lr:0.001 epoch: 260, loss: 0.13899789640146826, acc: 1.0\n",
      "lr:0.001 epoch: 270, loss: 0.13374156153779915, acc: 1.0\n",
      "lr:0.001 epoch: 280, loss: 0.1288622862294794, acc: 1.0\n",
      "lr:0.001 epoch: 290, loss: 0.12432186604352254, acc: 1.0\n",
      "lr:0.001 epoch: 300, loss: 0.12008697664107656, acc: 1.0\n",
      "lr:0.001 epoch: 310, loss: 0.11612843426512753, acc: 1.0\n",
      "lr:0.001 epoch: 320, loss: 0.11242058421198282, acc: 1.0\n",
      "lr:0.001 epoch: 330, loss: 0.10894079264370955, acc: 1.0\n",
      "lr:0.001 epoch: 340, loss: 0.105669022276717, acc: 1.0\n",
      "lr:0.001 epoch: 350, loss: 0.10258747649014469, acc: 1.0\n",
      "lr:0.001 epoch: 360, loss: 0.0996802995170922, acc: 1.0\n",
      "lr:0.001 epoch: 370, loss: 0.09693332282181119, acc: 1.0\n",
      "lr:0.001 epoch: 380, loss: 0.09433384968447914, acc: 1.0\n",
      "lr:0.001 epoch: 390, loss: 0.09187047153114607, acc: 1.0\n",
      "lr:0.001 epoch: 400, loss: 0.08953291075022489, acc: 1.0\n",
      "lr:0.001 epoch: 410, loss: 0.08731188569734419, acc: 1.0\n",
      "lr:0.001 epoch: 420, loss: 0.08519899436025473, acc: 1.0\n",
      "lr:0.001 epoch: 430, loss: 0.08318661377537688, acc: 1.0\n",
      "lr:0.001 epoch: 440, loss: 0.08126781278890567, acc: 1.0\n",
      "lr:0.001 epoch: 450, loss: 0.07943627616255255, acc: 1.0\n",
      "lr:0.001 epoch: 460, loss: 0.07768623835605004, acc: 1.0\n",
      "lr:0.001 epoch: 470, loss: 0.07601242559041607, acc: 1.0\n",
      "lr:0.001 epoch: 480, loss: 0.07441000501943293, acc: 1.0\n",
      "lr:0.001 epoch: 490, loss: 0.07287454002115154, acc: 1.0\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# 2. 模型计算函数\n",
    "def forward(x, theta, bias):\n",
    "    # 线性运算\n",
    "    z = np.dot(theta, x.T) + bias \n",
    "    # sigmoid\n",
    "    y_hat = 1 / (1 + np.exp(-z))  \n",
    "    return y_hat\n",
    "\n",
    "# 3. 计算损失函数\n",
    "def loss(y, y_hat):\n",
    "    e = 1e-8\n",
    "    return - y * np.log(y_hat + e) - (1 - y) * np.log(1 - y_hat + e)\n",
    "\n",
    "# 4. 计算梯度\n",
    "def calc_gradient(x,y,y_hat):\n",
    "    # 计算梯度\n",
    "    m = x.shape[-1]\n",
    "    # theta梯度计算\n",
    "    delta_theta = np.dot((y_hat - y), x) / m\n",
    "    # bias梯度计算\n",
    "    delta_bias = np.mean(y_hat - y)\n",
    "    # 返回梯度\n",
    "    return delta_theta, delta_bias\n",
    "\n",
    "# 5. 模型训练\n",
    "for lr in lrs:\n",
    "    # 权重参数\n",
    "    theta = np.random.randn(1,4)  # shape (1, 4)\n",
    "    bias = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # 前向计算\n",
    "        y_hat = forward(X_train, theta, bias)\n",
    "        # 计算损失\n",
    "        loss_val = loss(y_train, y_hat)\n",
    "        # 计算梯度\n",
    "        delta_theta, delta_bias = calc_gradient(X_train, y_train, y_hat)\n",
    "        # 更新参数\n",
    "        theta = theta - lr * delta_theta\n",
    "        bias = bias - lr * delta_bias\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            # 计算准确率\n",
    "            acc = np.mean(np.round(y_hat) == y_train)  # [False,True,...,False] -> [0,1,...,0]\n",
    "            print(f\"lr:{lr} epoch: {i}, loss: {np.mean(loss_val)}, acc: {acc}\")\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: 0, predict: [0.]\n"
     ]
    }
   ],
   "source": [
    "# 模型推理\n",
    "idx = np.random.randint(len(X_test)) # 随机选择一个测试样本索引\n",
    "x = X_test[idx]\n",
    "y = y_test[idx]\n",
    "\n",
    "predict = np.round(forward(x, theta, bias))\n",
    "print(f\"y: {y}, predict: {predict}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
