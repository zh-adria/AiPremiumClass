{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.86620108  1.39220954 -0.45701521 ... -0.59335549  3.23283078\n",
      "  -0.04614431]\n",
      " [-1.12562431  0.16774721 -2.32432281 ... -1.37803707 -0.36103203\n",
      "   1.69385927]\n",
      " [ 0.78394694  0.19479814  1.28669471 ...  0.92218457 -0.87182627\n",
      "  -1.31392749]\n",
      " ...\n",
      " [ 0.95543713  0.18293643  0.38938175 ... -0.76720622 -0.41349083\n",
      "   0.89042898]\n",
      " [-0.09429599  0.15682787 -0.20856018 ... -1.71300632 -1.65744221\n",
      "   2.13596829]\n",
      " [-0.28741514 -1.37163152  0.22404159 ...  1.08102809 -1.77022738\n",
      "  -0.60026016]]\n",
      "[0 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1\n",
      " 1 1 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1\n",
      " 1 1 1 0 1 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0\n",
      " 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1\n",
      " 1 0]\n",
      "(150, 10)\n",
      "(150,)\n",
      "------------\n",
      "(array([[ 0.68318095,  0.26344311,  0.64761349, ..., -1.99677609,\n",
      "        -0.8529396 , -0.10392982],\n",
      "       [ 0.38068753,  0.1645282 , -2.10962049, ..., -1.58614868,\n",
      "         0.41737829, -0.03416084],\n",
      "       [-0.52555003,  0.60541408,  0.89137679, ...,  1.02851003,\n",
      "         0.93533332, -0.59989384],\n",
      "       ...,\n",
      "       [-0.23691124, -0.63367286, -0.83898707, ..., -0.49352177,\n",
      "        -0.84494936,  0.32389068],\n",
      "       [-1.3958436 ,  0.09491578, -0.27345605, ...,  0.01191354,\n",
      "        -0.54094783, -1.15351659],\n",
      "       [ 0.04334476, -0.2139739 , -0.27632903, ..., -0.87161283,\n",
      "        -1.10885122, -0.14799972]]), array([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
      "       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
      "       0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
      "       0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "#1数据准备\n",
    "X,y=make_classification(n_samples=150, n_features=10)\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "print(X.shape)#一百五十个样本，每个样本有30个特征\n",
    "print(y.shape)#该样本的类别\n",
    "print('------------')\n",
    "\n",
    "result=make_classification()\n",
    "print(result)\n",
    "\n",
    "#局部样本训练模型（过拟合模型）（还有欠拟合）\n",
    "#然后用新样本，模型测试表现不好（泛化能力差）\n",
    "\n",
    "#训练集和测试集进行拆分，只搞一部分,例如0.3为只拿30%的数据进行测试\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 10)\n",
      "(45, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)#训练集为150的70%--训练用\n",
    "print(X_test.shape)#测试集为150的30%--测试用\n",
    "#数据集的拆分环节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#权重参数\n",
    "theta=np.random.randn(1,10)#一行十列\n",
    "#偏置\n",
    "bias=0\n",
    "#超参数\n",
    "lr=0.1\n",
    "epochs=3000 #训练次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2模型运算\n",
    "\n",
    "#为什么用点积运算？\n",
    "#因为点积运算可以将两个向量的每个元素相乘，然后将结果相加，得到一个标量\n",
    "# \n",
    "#假设X是shape(3,3)\n",
    "#[[x1,x2,x3],\n",
    "# [x4,x5,x6],\n",
    "# [x7,x8,x9]]\n",
    "# \n",
    "#所以X.T是shape(3,3)\n",
    "#[[x1,x4,x7],\n",
    "# [x2,x5,x8],\n",
    "# [x3,x6,x9]]\n",
    "#\n",
    "#再假设theta模型是shape(1,3)\n",
    "#[[W1,W2,W3]]\n",
    "#\n",
    "#得到结果theta*X.T是shape(1,3)\n",
    "#y1=W1*x1+W2*x4+W3*x7\n",
    "#y2=W1*x2+W2*x5+W3*x8\n",
    "#y3=W1*x3+W2*x6+W3*x9\n",
    "#\n",
    "#y=[y1,y2,y3] 一行三列 \n",
    "def forward(X,theta,bias):\n",
    "    #线性回归的运算\n",
    "    z=np.dot(theta,X.T)+bias #shape(105,10)\n",
    "    #sigmoid\n",
    "    y_hat=1/(1+np.exp(-z))#shape(105,10)\n",
    "\n",
    "    return y_hat\n",
    "#forward 向前传播\n",
    "#X 是输入特征矩阵，通常是一个二维数组，形状为 (样本数量, 特征数量)。\n",
    "#theta 是模型的权重向量，通常是一个二维数组，形状为 (1, 特征数量)。\n",
    "#bias 是模型的偏置项，通常是一个标量。\n",
    "\n",
    "#线性回归运算\n",
    "#np.dot(theta, X.T)：使用 numpy 的 dot 函数计算 theta 和 X 的转置的点积。X.T 是 X 的转置矩阵，形状为 (特征数量, 样本数量)。点积的结果 z 是一个二维数组，形状为 (1, 样本数量)。\n",
    "#+ bias：将偏置项 bias 加到点积结果上，得到线性回归的输出 z\n",
    "\n",
    "#sigmoid 激活函数\n",
    "#np.exp(-z)：计算 z 的负指数，即 e 的 -z 次方。\n",
    "#1 + np.exp(-z)：将负指数结果加 1。\n",
    "#1 / (1 + np.exp(-z))：计算 Sigmoid 函数的值，将线性回归的输出 z 转换为概率值 y_hat。y_hat 的形状与 z 相同，为 (1, 样本数量)。\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3计算损失函数\n",
    "def loss(y,y_hat):\n",
    "    e=1e-8\n",
    "    #\\拼接两行代码\n",
    "    return -y*np.log(y_hat+e) \\\n",
    "    -(1-y)*np.log(1-y_hat+e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4计算梯度\n",
    "def calc_gradient(X,y,y_hat):\n",
    "    m=X.shape[0]\n",
    "    delta_theta=np.dot((y_hat-y),X)/m\n",
    "    delta_bias=np.mean(y_hat-y)\n",
    "    return delta_theta,delta_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss: 1.0017441029962224, acc: 0.638095238095238\n",
      "epoch:100, loss: 0.3043040138583721, acc: 0.8761904761904762\n",
      "epoch:200, loss: 0.25528099346940997, acc: 0.9142857142857143\n",
      "epoch:300, loss: 0.24327641267662917, acc: 0.9238095238095239\n",
      "epoch:400, loss: 0.23893834965896849, acc: 0.9047619047619048\n",
      "epoch:500, loss: 0.23713608034424535, acc: 0.9047619047619048\n",
      "epoch:600, loss: 0.23632183684695737, acc: 0.9142857142857143\n",
      "epoch:700, loss: 0.2359245037264958, acc: 0.9142857142857143\n",
      "epoch:800, loss: 0.23571543916401436, acc: 0.9142857142857143\n",
      "epoch:900, loss: 0.23559771316021083, acc: 0.9142857142857143\n",
      "epoch:1000, loss: 0.23552761958217405, acc: 0.9142857142857143\n",
      "epoch:1100, loss: 0.23548405899185118, acc: 0.9142857142857143\n",
      "epoch:1200, loss: 0.2354561172828424, acc: 0.9142857142857143\n",
      "epoch:1300, loss: 0.23543777837782415, acc: 0.9142857142857143\n",
      "epoch:1400, loss: 0.2354255413059102, acc: 0.9142857142857143\n",
      "epoch:1500, loss: 0.23541727756383227, acc: 0.9142857142857143\n",
      "epoch:1600, loss: 0.23541164821496724, acc: 0.9142857142857143\n",
      "epoch:1700, loss: 0.2354077888488031, acc: 0.9142857142857143\n",
      "epoch:1800, loss: 0.23540513039016797, acc: 0.9142857142857143\n",
      "epoch:1900, loss: 0.23540329266747617, acc: 0.9142857142857143\n",
      "epoch:2000, loss: 0.2354020189071472, acc: 0.9142857142857143\n",
      "epoch:2100, loss: 0.23540113424933526, acc: 0.9142857142857143\n",
      "epoch:2200, loss: 0.23540051887813376, acc: 0.9142857142857143\n",
      "epoch:2300, loss: 0.2354000903099989, acc: 0.9142857142857143\n",
      "epoch:2400, loss: 0.2353997915598764, acc: 0.9142857142857143\n",
      "epoch:2500, loss: 0.23539958315187165, acc: 0.9142857142857143\n",
      "epoch:2600, loss: 0.23539943768234017, acc: 0.9142857142857143\n",
      "epoch:2700, loss: 0.2353993360973157, acc: 0.9142857142857143\n",
      "epoch:2800, loss: 0.23539926513176246, acc: 0.9142857142857143\n",
      "epoch:2900, loss: 0.23539921554168602, acc: 0.9142857142857143\n"
     ]
    }
   ],
   "source": [
    "#模型训练流程 ---梯度下降法\n",
    "for i in range(epochs):\n",
    "    #向前计算\n",
    "    y_hat = forward(X_train, theta, bias)\n",
    "    #计算损失\n",
    "    loss_val = loss(y_train, y_hat)\n",
    "    #计算梯度\n",
    "    delta_theta, delta_bias = calc_gradient(X_train, y_train, y_hat)\n",
    "    #更新参数\n",
    "    theta = theta - lr * delta_theta\n",
    "    bias = bias - lr * delta_bias\n",
    "\n",
    "    if i % 100 == 0:#每100次打印一次\n",
    "        #计算准确率\n",
    "        acc = np.mean(np.round(y_hat) == y_train) \n",
    "        #[FALSE,TRUE,...,FALSE...]-->[0,1,....,0]\n",
    "        print(f\"epoch:{i}, loss: {np.mean(loss_val)}, acc: {acc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:0, predict: [0.]\n"
     ]
    }
   ],
   "source": [
    "#模型推理\n",
    "idx = np.random.randint(len(X_test))#随机取一个测试样本\n",
    "x = X_test[idx]\n",
    "y = y_test[idx]\n",
    "#使用 numpy 的 random.randint 函数从 0 到 len(X_test) - 1 的范围内随机生成一个整数 idx，作为测试样本的索引。\n",
    "predict = np.round(forward(x, theta, bias))\n",
    "#forward(x, theta, bias)：调用之前定义的 forward 函数，对选取的测试样本 x 进行前向传播计算，得到预测概率。\n",
    "#np.round(...)：使用 numpy 的 round 函数对预测概率进行四舍五入，将其转换为 0 或 1 的预测标签，赋值给变量 predict。\n",
    "print(f\"y:{y}, predict: {predict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#作业\n",
    "#使用sklearn数据集训练逻辑回归模型\n",
    "#调整学习率，观察训练结果\n",
    "#把模型训练参数保存到文件，在另一个代码中加载参数实现预测功能"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
