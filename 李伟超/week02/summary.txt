逻辑回归模型是基于线性模型的分类算法，适用于二分类问题。
核心是使用 Sigmoid 函数（或 Softmax 函数）将线性回归的输出转化为概率值。
模型的损失函数是交叉熵损失（cross-entropy loss），通过最大化似然估计（MLE）来进行参数优化。
训练过程主要通过梯度下降优化模型的参数，使得损失函数最小化。
学习率lr：lr 越小，正则化越强，模型越简单；lr 越大，正则化越弱，模型越复杂。

手写逻辑回归主要过程：
1、导入数据集（对数据集切割划分为训练集和测试集，权重参数theta、bias,超参数lr、epochs）
2、模型计算函数
    线性运算：z = np.dot(theta, x.T) + bias
    sigmoid：y_hat = 1 / (1 + np.exp(-z))
3、计算损失函数
     e = 1e-8
    return - y * np.log(y_hat + e) - (1 - y) * np.log(1 - y_hat + e)
4、计算梯度
   theta梯度计算
   bias梯度计算
5、模型训练
   前向计算、计算损失、计算梯度、更新参数、计算准确率
6、模型实现预测
   直接实现预测或者外部调用

