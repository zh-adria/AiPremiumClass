{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\nclass TorchNN(nn.Module):\n    def __init__(self):  \n        super().__init__()\n\n        self.linear1 = nn.Linear(4096, 512)  # 输入改为 4096\n        self.bn1 = nn.BatchNorm1d(512)\n        self.linear2 = nn.Linear(512, 512)\n        self.bn2 = nn.BatchNorm1d(512)\n        self.linear3 = nn.Linear(512, 40)  # Olivetti Faces 有 40 个类别\n\n        self.drop = nn.Dropout(p=0.3)\n        self.act = nn.ReLU()\n\n    def forward(self, input_tensor):\n        out = self.linear1(input_tensor)\n        out = self.bn1(out)\n        out = self.act(out)\n        out = self.drop(out)\n        out = self.linear2(out)\n        out = self.bn2(out)\n        out = self.act(out)\n        out = self.drop(out)\n        final = self.linear3(out)\n\n        return final\n\n# 设备检测\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n#  加载 Olivetti Faces 数据集\ndata = fetch_olivetti_faces(shuffle=True)\nX, y = data.images, data.target  # X.shape = (400, 64, 64), y.shape = (400,)\n\n#  数据预处理（归一化 & 转换）\nscaler = StandardScaler()\nX = scaler.fit_transform(X.reshape(len(X), -1))  # (400, 4096)\n\n# 转换为 PyTorch 张量\nX_tensor = torch.tensor(X, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.long)\n\n#  创建 DataLoader（减少 BATCH_SIZE 以防内存溢出）\nBATCH_SIZE = 16  \ndataset = TensorDataset(X_tensor, y_tensor)\ntrain_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n\n# 训练不同优化器并对比 Loss\nEPOCHS = 100\nloss_fn = nn.CrossEntropyLoss()\n\noptimizers = {\n    \"SGD\": optim.SGD,\n    \"Adam\": optim.Adam,\n    \"RMSprop\": optim.RMSprop\n}\n\nresults = {}\n\nfor opt_name, opt_class in optimizers.items():\n    print(f\"\\nTraining with {opt_name} optimizer...\")\n    model = TorchNN().to(device)  # 发送至 GPU\n    optimizer = opt_class(model.parameters(), lr=0.001)\n\n    for epoch in range(EPOCHS):\n        total_loss = 0\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n\n            optimizer.zero_grad()\n            output = model(batch_X)\n            loss = loss_fn(output, batch_y)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n\n    results[opt_name] = avg_loss\n\n# 输出最终 Loss 结果\nprint(\"\\nFinal Loss Comparison:\")\nfor opt_name, loss in results.items():\n    print(f\"{opt_name}: {loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T11:28:39.275262Z","iopub.execute_input":"2025-03-19T11:28:39.275563Z","iopub.status.idle":"2025-03-19T11:28:57.859377Z","shell.execute_reply.started":"2025-03-19T11:28:39.275541Z","shell.execute_reply":"2025-03-19T11:28:57.858422Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nTraining with SGD optimizer...\nEpoch 1/100, Loss: 3.7204\nEpoch 2/100, Loss: 3.4838\nEpoch 3/100, Loss: 3.2458\nEpoch 4/100, Loss: 3.0208\nEpoch 5/100, Loss: 2.9030\nEpoch 6/100, Loss: 2.8162\nEpoch 7/100, Loss: 2.6547\nEpoch 8/100, Loss: 2.5118\nEpoch 9/100, Loss: 2.4519\nEpoch 10/100, Loss: 2.3327\nEpoch 11/100, Loss: 2.2483\nEpoch 12/100, Loss: 2.2029\nEpoch 13/100, Loss: 2.0843\nEpoch 14/100, Loss: 2.0237\nEpoch 15/100, Loss: 1.9924\nEpoch 16/100, Loss: 1.9107\nEpoch 17/100, Loss: 1.8328\nEpoch 18/100, Loss: 1.8260\nEpoch 19/100, Loss: 1.7013\nEpoch 20/100, Loss: 1.6427\nEpoch 21/100, Loss: 1.6053\nEpoch 22/100, Loss: 1.5767\nEpoch 23/100, Loss: 1.5330\nEpoch 24/100, Loss: 1.4766\nEpoch 25/100, Loss: 1.4283\nEpoch 26/100, Loss: 1.3606\nEpoch 27/100, Loss: 1.3489\nEpoch 28/100, Loss: 1.2939\nEpoch 29/100, Loss: 1.2666\nEpoch 30/100, Loss: 1.2201\nEpoch 31/100, Loss: 1.2130\nEpoch 32/100, Loss: 1.1687\nEpoch 33/100, Loss: 1.1175\nEpoch 34/100, Loss: 1.0990\nEpoch 35/100, Loss: 1.0966\nEpoch 36/100, Loss: 1.0653\nEpoch 37/100, Loss: 1.0108\nEpoch 38/100, Loss: 0.9668\nEpoch 39/100, Loss: 0.9862\nEpoch 40/100, Loss: 0.9212\nEpoch 41/100, Loss: 0.9323\nEpoch 42/100, Loss: 0.9087\nEpoch 43/100, Loss: 0.8457\nEpoch 44/100, Loss: 0.8425\nEpoch 45/100, Loss: 0.8395\nEpoch 46/100, Loss: 0.7925\nEpoch 47/100, Loss: 0.8068\nEpoch 48/100, Loss: 0.7710\nEpoch 49/100, Loss: 0.7800\nEpoch 50/100, Loss: 0.7427\nEpoch 51/100, Loss: 0.7214\nEpoch 52/100, Loss: 0.6976\nEpoch 53/100, Loss: 0.6977\nEpoch 54/100, Loss: 0.6807\nEpoch 55/100, Loss: 0.6543\nEpoch 56/100, Loss: 0.6271\nEpoch 57/100, Loss: 0.6439\nEpoch 58/100, Loss: 0.6094\nEpoch 59/100, Loss: 0.5703\nEpoch 60/100, Loss: 0.5894\nEpoch 61/100, Loss: 0.5805\nEpoch 62/100, Loss: 0.5512\nEpoch 63/100, Loss: 0.5332\nEpoch 64/100, Loss: 0.5371\nEpoch 65/100, Loss: 0.5372\nEpoch 66/100, Loss: 0.4879\nEpoch 67/100, Loss: 0.5287\nEpoch 68/100, Loss: 0.4931\nEpoch 69/100, Loss: 0.4941\nEpoch 70/100, Loss: 0.4903\nEpoch 71/100, Loss: 0.4773\nEpoch 72/100, Loss: 0.4778\nEpoch 73/100, Loss: 0.4424\nEpoch 74/100, Loss: 0.4401\nEpoch 75/100, Loss: 0.4227\nEpoch 76/100, Loss: 0.4183\nEpoch 77/100, Loss: 0.3926\nEpoch 78/100, Loss: 0.4206\nEpoch 79/100, Loss: 0.3967\nEpoch 80/100, Loss: 0.4077\nEpoch 81/100, Loss: 0.4238\nEpoch 82/100, Loss: 0.4067\nEpoch 83/100, Loss: 0.3749\nEpoch 84/100, Loss: 0.3632\nEpoch 85/100, Loss: 0.3788\nEpoch 86/100, Loss: 0.3644\nEpoch 87/100, Loss: 0.3639\nEpoch 88/100, Loss: 0.3389\nEpoch 89/100, Loss: 0.3625\nEpoch 90/100, Loss: 0.3550\nEpoch 91/100, Loss: 0.3364\nEpoch 92/100, Loss: 0.3123\nEpoch 93/100, Loss: 0.3280\nEpoch 94/100, Loss: 0.3001\nEpoch 95/100, Loss: 0.3394\nEpoch 96/100, Loss: 0.3143\nEpoch 97/100, Loss: 0.3147\nEpoch 98/100, Loss: 0.2927\nEpoch 99/100, Loss: 0.3038\nEpoch 100/100, Loss: 0.2964\n\nTraining with Adam optimizer...\nEpoch 1/100, Loss: 2.9799\nEpoch 2/100, Loss: 1.6039\nEpoch 3/100, Loss: 0.8689\nEpoch 4/100, Loss: 0.4334\nEpoch 5/100, Loss: 0.2632\nEpoch 6/100, Loss: 0.1724\nEpoch 7/100, Loss: 0.1126\nEpoch 8/100, Loss: 0.0659\nEpoch 9/100, Loss: 0.0542\nEpoch 10/100, Loss: 0.0457\nEpoch 11/100, Loss: 0.0392\nEpoch 12/100, Loss: 0.0357\nEpoch 13/100, Loss: 0.0299\nEpoch 14/100, Loss: 0.0218\nEpoch 15/100, Loss: 0.0232\nEpoch 16/100, Loss: 0.0191\nEpoch 17/100, Loss: 0.0169\nEpoch 18/100, Loss: 0.0146\nEpoch 19/100, Loss: 0.0116\nEpoch 20/100, Loss: 0.0125\nEpoch 21/100, Loss: 0.0114\nEpoch 22/100, Loss: 0.0111\nEpoch 23/100, Loss: 0.0146\nEpoch 24/100, Loss: 0.0100\nEpoch 25/100, Loss: 0.0127\nEpoch 26/100, Loss: 0.0093\nEpoch 27/100, Loss: 0.0066\nEpoch 28/100, Loss: 0.0058\nEpoch 29/100, Loss: 0.0057\nEpoch 30/100, Loss: 0.0048\nEpoch 31/100, Loss: 0.0046\nEpoch 32/100, Loss: 0.0048\nEpoch 33/100, Loss: 0.0039\nEpoch 34/100, Loss: 0.0045\nEpoch 35/100, Loss: 0.0053\nEpoch 36/100, Loss: 0.0031\nEpoch 37/100, Loss: 0.0056\nEpoch 38/100, Loss: 0.0043\nEpoch 39/100, Loss: 0.0033\nEpoch 40/100, Loss: 0.0026\nEpoch 41/100, Loss: 0.0040\nEpoch 42/100, Loss: 0.0042\nEpoch 43/100, Loss: 0.0036\nEpoch 44/100, Loss: 0.0039\nEpoch 45/100, Loss: 0.0031\nEpoch 46/100, Loss: 0.0028\nEpoch 47/100, Loss: 0.0026\nEpoch 48/100, Loss: 0.0023\nEpoch 49/100, Loss: 0.0019\nEpoch 50/100, Loss: 0.0013\nEpoch 51/100, Loss: 0.0015\nEpoch 52/100, Loss: 0.0021\nEpoch 53/100, Loss: 0.0019\nEpoch 54/100, Loss: 0.0017\nEpoch 55/100, Loss: 0.0020\nEpoch 56/100, Loss: 0.0016\nEpoch 57/100, Loss: 0.0015\nEpoch 58/100, Loss: 0.0012\nEpoch 59/100, Loss: 0.0020\nEpoch 60/100, Loss: 0.0032\nEpoch 61/100, Loss: 0.0014\nEpoch 62/100, Loss: 0.0020\nEpoch 63/100, Loss: 0.0017\nEpoch 64/100, Loss: 0.0019\nEpoch 65/100, Loss: 0.0023\nEpoch 66/100, Loss: 0.0012\nEpoch 67/100, Loss: 0.0009\nEpoch 68/100, Loss: 0.0012\nEpoch 69/100, Loss: 0.0061\nEpoch 70/100, Loss: 0.1392\nEpoch 71/100, Loss: 0.3164\nEpoch 72/100, Loss: 0.2949\nEpoch 73/100, Loss: 0.1763\nEpoch 74/100, Loss: 0.0940\nEpoch 75/100, Loss: 0.1174\nEpoch 76/100, Loss: 0.0511\nEpoch 77/100, Loss: 0.0353\nEpoch 78/100, Loss: 0.0287\nEpoch 79/100, Loss: 0.0435\nEpoch 80/100, Loss: 0.0453\nEpoch 81/100, Loss: 0.0229\nEpoch 82/100, Loss: 0.0137\nEpoch 83/100, Loss: 0.0216\nEpoch 84/100, Loss: 0.0049\nEpoch 85/100, Loss: 0.0045\nEpoch 86/100, Loss: 0.0031\nEpoch 87/100, Loss: 0.0035\nEpoch 88/100, Loss: 0.0035\nEpoch 89/100, Loss: 0.0041\nEpoch 90/100, Loss: 0.0071\nEpoch 91/100, Loss: 0.0046\nEpoch 92/100, Loss: 0.0034\nEpoch 93/100, Loss: 0.0022\nEpoch 94/100, Loss: 0.0041\nEpoch 95/100, Loss: 0.0018\nEpoch 96/100, Loss: 0.0040\nEpoch 97/100, Loss: 0.0066\nEpoch 98/100, Loss: 0.0097\nEpoch 99/100, Loss: 0.0259\nEpoch 100/100, Loss: 0.0365\n\nTraining with RMSprop optimizer...\nEpoch 1/100, Loss: 2.6972\nEpoch 2/100, Loss: 1.0299\nEpoch 3/100, Loss: 0.4508\nEpoch 4/100, Loss: 0.2255\nEpoch 5/100, Loss: 0.1246\nEpoch 6/100, Loss: 0.1078\nEpoch 7/100, Loss: 0.0640\nEpoch 8/100, Loss: 0.0871\nEpoch 9/100, Loss: 0.0523\nEpoch 10/100, Loss: 0.0303\nEpoch 11/100, Loss: 0.0193\nEpoch 12/100, Loss: 0.0237\nEpoch 13/100, Loss: 0.0404\nEpoch 14/100, Loss: 0.0380\nEpoch 15/100, Loss: 0.0958\nEpoch 16/100, Loss: 0.0524\nEpoch 17/100, Loss: 0.0560\nEpoch 18/100, Loss: 0.0483\nEpoch 19/100, Loss: 0.0233\nEpoch 20/100, Loss: 0.0101\nEpoch 21/100, Loss: 0.0121\nEpoch 22/100, Loss: 0.0111\nEpoch 23/100, Loss: 0.0128\nEpoch 24/100, Loss: 0.0104\nEpoch 25/100, Loss: 0.0041\nEpoch 26/100, Loss: 0.0056\nEpoch 27/100, Loss: 0.0341\nEpoch 28/100, Loss: 0.0064\nEpoch 29/100, Loss: 0.1268\nEpoch 30/100, Loss: 0.0652\nEpoch 31/100, Loss: 0.0153\nEpoch 32/100, Loss: 0.0074\nEpoch 33/100, Loss: 0.0084\nEpoch 34/100, Loss: 0.0063\nEpoch 35/100, Loss: 0.0062\nEpoch 36/100, Loss: 0.0066\nEpoch 37/100, Loss: 0.0057\nEpoch 38/100, Loss: 0.0017\nEpoch 39/100, Loss: 0.0361\nEpoch 40/100, Loss: 0.0289\nEpoch 41/100, Loss: 0.1152\nEpoch 42/100, Loss: 0.0397\nEpoch 43/100, Loss: 0.0251\nEpoch 44/100, Loss: 0.0193\nEpoch 45/100, Loss: 0.0062\nEpoch 46/100, Loss: 0.0113\nEpoch 47/100, Loss: 0.0081\nEpoch 48/100, Loss: 0.0039\nEpoch 49/100, Loss: 0.0033\nEpoch 50/100, Loss: 0.0042\nEpoch 51/100, Loss: 0.0025\nEpoch 52/100, Loss: 0.0281\nEpoch 53/100, Loss: 0.0340\nEpoch 54/100, Loss: 0.0112\nEpoch 55/100, Loss: 0.0032\nEpoch 56/100, Loss: 0.0076\nEpoch 57/100, Loss: 0.0039\nEpoch 58/100, Loss: 0.0335\nEpoch 59/100, Loss: 0.0280\nEpoch 60/100, Loss: 0.0148\nEpoch 61/100, Loss: 0.0291\nEpoch 62/100, Loss: 0.0201\nEpoch 63/100, Loss: 0.0162\nEpoch 64/100, Loss: 0.0034\nEpoch 65/100, Loss: 0.0036\nEpoch 66/100, Loss: 0.0034\nEpoch 67/100, Loss: 0.0120\nEpoch 68/100, Loss: 0.0085\nEpoch 69/100, Loss: 0.0032\nEpoch 70/100, Loss: 0.0042\nEpoch 71/100, Loss: 0.0043\nEpoch 72/100, Loss: 0.0025\nEpoch 73/100, Loss: 0.0059\nEpoch 74/100, Loss: 0.0304\nEpoch 75/100, Loss: 0.0148\nEpoch 76/100, Loss: 0.0085\nEpoch 77/100, Loss: 0.0105\nEpoch 78/100, Loss: 0.0304\nEpoch 79/100, Loss: 0.0149\nEpoch 80/100, Loss: 0.0121\nEpoch 81/100, Loss: 0.0191\nEpoch 82/100, Loss: 0.0209\nEpoch 83/100, Loss: 0.0095\nEpoch 84/100, Loss: 0.0022\nEpoch 85/100, Loss: 0.0050\nEpoch 86/100, Loss: 0.0031\nEpoch 87/100, Loss: 0.0013\nEpoch 88/100, Loss: 0.0130\nEpoch 89/100, Loss: 0.0507\nEpoch 90/100, Loss: 0.0070\nEpoch 91/100, Loss: 0.0033\nEpoch 92/100, Loss: 0.0008\nEpoch 93/100, Loss: 0.0042\nEpoch 94/100, Loss: 0.0026\nEpoch 95/100, Loss: 0.0019\nEpoch 96/100, Loss: 0.0103\nEpoch 97/100, Loss: 0.0423\nEpoch 98/100, Loss: 0.0190\nEpoch 99/100, Loss: 0.0137\nEpoch 100/100, Loss: 0.0029\n\nFinal Loss Comparison:\nSGD: 0.2964\nAdam: 0.0365\nRMSprop: 0.0029\n","output_type":"stream"}],"execution_count":7}]}