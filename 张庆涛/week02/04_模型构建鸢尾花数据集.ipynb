{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['setosa', 'versicolor', 'virginica']"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = load_iris()\n",
    "data.target[[10, 25, 50]]\n",
    "print(data.target_names)\n",
    "print(data.feature_names)\n",
    "list(data.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.638172410650565, acc: 0.5244444444444445\n",
      "epoch: 100, loss: 1.1614832696600752, acc: 0.5633333333333334\n",
      "epoch: 200, loss: 0.8500017375655187, acc: 0.5761111111111111\n",
      "epoch: 300, loss: 0.6729177150581023, acc: 0.6761111111111111\n",
      "epoch: 400, loss: 0.5877440031728107, acc: 0.7155555555555555\n",
      "epoch: 500, loss: 0.5336219926706838, acc: 0.7544444444444445\n",
      "epoch: 600, loss: 0.48862077533116727, acc: 0.7916666666666666\n",
      "epoch: 700, loss: 0.44828912154493356, acc: 0.8188888888888889\n",
      "epoch: 800, loss: 0.41202965641606626, acc: 0.8472222222222222\n",
      "epoch: 900, loss: 0.3795490093553071, acc: 0.8794444444444445\n",
      "epoch: 1000, loss: 0.3505516408001075, acc: 0.9011111111111111\n",
      "epoch: 1100, loss: 0.3247234169867205, acc: 0.9177777777777778\n",
      "epoch: 1200, loss: 0.30174256741461947, acc: 0.9327777777777778\n",
      "epoch: 1300, loss: 0.28129269726181094, acc: 0.9416666666666667\n",
      "epoch: 1400, loss: 0.26307351691755654, acc: 0.955\n",
      "epoch: 1500, loss: 0.24680822893081394, acc: 0.9683333333333334\n",
      "epoch: 1600, loss: 0.2322475874210952, acc: 0.9777777777777777\n",
      "epoch: 1700, loss: 0.21917117210624282, acc: 0.9838888888888889\n",
      "epoch: 1800, loss: 0.20738663541097732, acc: 0.9988888888888889\n",
      "epoch: 1900, loss: 0.19672767615273018, acc: 1.0\n",
      "epoch: 2000, loss: 0.18705136053080196, acc: 1.0\n",
      "epoch: 2100, loss: 0.17823523530460691, acc: 1.0\n",
      "epoch: 2200, loss: 0.1701745145241696, acc: 1.0\n",
      "epoch: 2300, loss: 0.16277949434527852, acc: 1.0\n",
      "epoch: 2400, loss: 0.1559732636167845, acc: 1.0\n",
      "epoch: 2500, loss: 0.14968972433028604, acc: 1.0\n",
      "epoch: 2600, loss: 0.14387190633341945, acc: 1.0\n",
      "epoch: 2700, loss: 0.1384705465819283, acc: 1.0\n",
      "epoch: 2800, loss: 0.13344289840923626, acc: 1.0\n",
      "epoch: 2900, loss: 0.12875173663499764, acc: 1.0\n",
      "epoch: 3000, loss: 0.12436452720302905, acc: 1.0\n",
      "epoch: 3100, loss: 0.1202527339058594, acc: 1.0\n",
      "epoch: 3200, loss: 0.11639123878212145, acc: 1.0\n",
      "epoch: 3300, loss: 0.11275785654647201, acc: 1.0\n",
      "epoch: 3400, loss: 0.10933292675208671, acc: 1.0\n",
      "epoch: 3500, loss: 0.10609897024599346, acc: 1.0\n",
      "epoch: 3600, loss: 0.10304039887649803, acc: 1.0\n",
      "epoch: 3700, loss: 0.10014326939793522, acc: 1.0\n",
      "epoch: 3800, loss: 0.09739507414861666, acc: 1.0\n",
      "epoch: 3900, loss: 0.09478456241014666, acc: 1.0\n",
      "epoch: 4000, loss: 0.09230158744194882, acc: 1.0\n",
      "epoch: 4100, loss: 0.0899369750686378, acc: 1.0\n",
      "epoch: 4200, loss: 0.08768241041741884, acc: 1.0\n",
      "epoch: 4300, loss: 0.08553033998910392, acc: 1.0\n",
      "epoch: 4400, loss: 0.08347388672499945, acc: 1.0\n",
      "epoch: 4500, loss: 0.08150677612343764, acc: 1.0\n",
      "epoch: 4600, loss: 0.07962327178072838, acc: 1.0\n",
      "epoch: 4700, loss: 0.07781811899517442, acc: 1.0\n",
      "epoch: 4800, loss: 0.07608649529029969, acc: 1.0\n",
      "epoch: 4900, loss: 0.07442396689324113, acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "X,y = load_iris(return_X_y=True)\n",
    "\n",
    "# 分别取X,y 后100个数据作为测试集\n",
    "X = X[:100]\n",
    "y = y[:100]\n",
    "# print(X.shape,y.shape)\n",
    "# 1.准备数据\n",
    "# 将数据集拆分为训练集和测试集，测试集占30%，随机种子为123以保证可重复性\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.4, random_state=2020,)\n",
    "# print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "\n",
    "# 初始化模型参数集合 theta 为权重参数\n",
    "theta = np.random.randn(30,4)\n",
    "# print(theta.shape)\n",
    "# print(X.shape[-1])\n",
    "\n",
    "# bias 初始化\n",
    "bias = 0\n",
    "# 设置学习率\n",
    "lr = 1e-4\n",
    "\n",
    "# 设置迭代次数\n",
    "epochs = 5000\n",
    "\n",
    "# 2.模型计算\n",
    "def forward(X,theta,bias):\n",
    "  # 前向传播 线性回归\n",
    "  z = np.dot(theta,X.T) + bias\n",
    "  # print(z)\n",
    "  # sigmoid 函数\n",
    "  y_hat = 1 / (1 + np.exp(-z))\n",
    "  # print(y_hat)\n",
    "  return y_hat\n",
    "\n",
    "  \n",
    "# 3.计算损失\n",
    "def loss_function(y,y_hat):\n",
    "  # 交叉熵损失函数\n",
    "  e = 1e-2\n",
    "  loss = -y * np.log(y_hat + e) - (1 - y) * np.log(1 - y_hat + e)\n",
    "  return loss\n",
    "\n",
    "# 4.计算梯度下降\n",
    "def clac_gradient(X,y,y_hat):\n",
    "  m = X.shape[-1] #样本数量\n",
    "  delta_w = np.dot((y_hat - y),X) / m\n",
    "  delta_b = np.mean(y_hat - y) \n",
    "  return delta_w,delta_b\n",
    "  \n",
    "# 5.更新参数\n",
    "theta_histroy =[]\n",
    "for i in range(epochs):\n",
    "  # 前向传播\n",
    "  y_hat = forward(X_train,theta,bias)\n",
    "  \n",
    "  # 计算损失\n",
    "  loss = loss_function(y_train,y_hat)\n",
    "  # 计算梯度\n",
    "  dw,db = clac_gradient(X_train,y_train,y_hat)\n",
    "  # 更新参数\n",
    "  last_theta = theta\n",
    "  \n",
    "  theta -= lr * dw\n",
    "  bias -= lr * db\n",
    "  \n",
    "  if i % 100 == 0:\n",
    "        # 计算准确率\n",
    "        acc = np.mean(np.round(y_hat) == y_train)   # [False,True,...,False] -> [0,1,...,0]\n",
    "        print(f\"epoch: {i}, loss: {np.mean(loss)}, acc: {acc}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 35\n",
      "x: [6.  3.4 4.5 1.6]\n",
      "y: 1\n",
      "y: 1\n",
      ",yuan_predict: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "predict: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# 模型推理\n",
    "idx = np.random.randint(len(X_test)) # 随机选择一个测试样本索引\n",
    "print(f\"idx: {idx}\")\n",
    "x = X_test[idx]\n",
    "print(f\"x: {x}\")\n",
    "y = y_test[idx]\n",
    "print(f\"y: {y}\")\n",
    "yuan_predict = forward(x, theta, bias)\n",
    "predict = np.round(forward(x, theta, bias))\n",
    "print(f\"y: {y}\\n,yuan_predict: {yuan_predict}\\npredict: {predict}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
