{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量（Tensor）\n",
    "张量（Tensor）是pytorch中的基本单位，也是深度学习框架构成的重要组成。  \n",
    "我们可以先把张量看做是⼀个容器，⾥⾯承载了需要运算的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = torch.tensor([[1, 2],[ 4, 5]])\n",
    "data = torch.tensor([(1, 2),(4, 5)])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [4 5]]\n",
      "\n",
      "tensor([[1, 2],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# np_array = np.array([(1, 2),(4, 5)])\n",
    "np_array = np.array(data)\n",
    "print(np_array)\n",
    "print()\n",
    "data2 = torch.tensor(np_array)\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "tensor([[0, 0],\n",
      "        [0, 0]])\n",
      "torch.int64\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 通过已知张量维度创建新张量,新张量保留参数张量的属性（形状、数据类型）。\n",
    "data3 = torch.ones_like(data2)\n",
    "print(data3)\n",
    "\n",
    "data3 = torch.zeros_like(data2)\n",
    "print(data3)\n",
    "print(data3.dtype)\n",
    "\n",
    "data3 = torch.zeros_like(data2, dtype=torch.float)\n",
    "print(data3.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape 是张量维度的元组。在下⾯的函数中，它决定了输出张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.7564, 0.5553, 0.8386],\n",
      "        [0.1841, 0.8312, 0.0739]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "<built-in method size of Tensor object at 0x000001EFC4D4F1A0>\n",
      "\n",
      "tensor([[0.1008, 0.4025, 0.4114],\n",
      "        [0.5642, 0.2037, 0.1494],\n",
      "        [0.4201, 0.2615, 0.0635],\n",
      "        [0.6632, 0.3699, 0.1394],\n",
      "        [0.5332, 0.3067, 0.8182]]) torch.float32 torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# 基于现有tensor构建，但使⽤新值填充\n",
    "m = torch.ones(5,3, dtype=torch.double)  # 生成5x3的浮点型全1矩阵\n",
    "print(m)\n",
    "print(m.size) # 获取tensor的⼤⼩\n",
    "print()\n",
    "n = torch.rand_like(m, dtype=torch.float)\n",
    "print(n,n.dtype,n.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5585, 0.1228, 0.8188],\n",
       "        [0.1822, 0.3175, 0.5424],\n",
       "        [0.3630, 0.2778, 0.8189],\n",
       "        [0.2753, 0.3020, 0.3211],\n",
       "        [0.5505, 0.4595, 0.0981]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 均匀分布\n",
    "torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3786, -0.4535, -0.5371],\n",
       "        [ 0.1719,  0.1824,  1.2695],\n",
       "        [ 0.8040, -0.2274,  1.7973],\n",
       "        [ 1.7851,  0.2618,  0.4073],\n",
       "        [-1.5485, -0.8264, -1.0368]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标准正态分布  (normal distribution 正态分布)\n",
    "torch.randn(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9606, -0.7533, -1.0049],\n",
       "        [ 1.1916, -2.6802, -0.5919],\n",
       "        [ 0.8588, -0.9147, -0.5612],\n",
       "        [-0.0407,  0.3067,  0.0169],\n",
       "        [ 0.8102, -0.4456, -1.0362]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 离散正态分布\n",
    "torch.normal(mean=.0,std=1.0,size=(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  1.4500,  1.9000,  2.3500,  2.8000,  3.2500,  3.7000,  4.1500,\n",
       "         4.6000,  5.0500,  5.5000,  5.9500,  6.4000,  6.8500,  7.3000,  7.7500,\n",
       "         8.2000,  8.6500,  9.1000,  9.5500, 10.0000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 线性间隔向量(返回⼀个1维张量，包含在区间start和end上均匀间隔的steps个点)\n",
    "torch.linspace(start=1,end=10,steps=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "size of tensor: <built-in method size of Tensor object at 0x000001EFC4D4E4D0>\n",
      "size of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"size of tensor: {tensor.size}\")\n",
    "print(f\"size of tensor: {tensor.size()}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# 是否支持GPU\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # a CUDA device object\n",
    "    tensor.to(device) # 通过tensor.to(\"cuda\") 将tensor移到GPU上\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的索引和切⽚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1., 1., 1., 1.])\n",
      "First column:  tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print('First row: ', tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1])\n",
    "tensor[:,1] = 0  # 用 0 填充第二列\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1) \n",
    "# dim=1 指定了沿 第 1 维（列方向） 进行拼接。\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.cat([tensor, tensor], dim=0) \n",
    "# dim=1 指定了沿 第 0 维（行方向） 进行拼接。\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沿着维度 0 堆叠后的形状: torch.Size([2, 2, 3])\n",
      "沿着维度 0 堆叠后的结果:\n",
      " tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n",
      "沿着维度 1 堆叠后的形状: torch.Size([2, 2, 3])\n",
      "沿着维度 1 堆叠后的结果:\n",
      " tensor([[[ 1,  2,  3],\n",
      "         [ 7,  8,  9]],\n",
      "\n",
      "        [[ 4,  5,  6],\n",
      "         [10, 11, 12]]])\n",
      "沿着维度 2 堆叠后的形状: torch.Size([2, 3, 2])\n",
      "沿着维度 2 堆叠后的结果:\n",
      " tensor([[[ 1,  7],\n",
      "         [ 2,  8],\n",
      "         [ 3,  9]],\n",
      "\n",
      "        [[ 4, 10],\n",
      "         [ 5, 11],\n",
      "         [ 6, 12]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.stack  沿着一个新的维度对张量序列进行拼接操作\n",
    "\n",
    "# 创建两个 2x3 的张量\n",
    "tensor1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "tensor2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# 沿着维度 0 进行堆叠\n",
    "stacked_tensor_0 = torch.stack([tensor1, tensor2], dim=0)\n",
    "print(\"沿着维度 0 堆叠后的形状:\", stacked_tensor_0.shape)\n",
    "print(\"沿着维度 0 堆叠后的结果:\\n\", stacked_tensor_0)\n",
    "\n",
    "# 沿着维度 1 进行堆叠\n",
    "stacked_tensor_1 = torch.stack([tensor1, tensor2], dim=1)\n",
    "print(\"沿着维度 1 堆叠后的形状:\", stacked_tensor_1.shape)\n",
    "print(\"沿着维度 1 堆叠后的结果:\\n\", stacked_tensor_1)\n",
    "\n",
    "# 沿着维度 2 进行堆叠\n",
    "stacked_tensor_2 = torch.stack([tensor1, tensor2], dim=2)\n",
    "print(\"沿着维度 2 堆叠后的形状:\", stacked_tensor_2.shape)\n",
    "print(\"沿着维度 2 堆叠后的结果:\\n\", stacked_tensor_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的算数运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.matmul 执行的是矩阵乘法，遵循矩阵乘法的规则，要求两个张量的维度满足一定的条件（例如，第一个张量的列数要等于第二个张量的行数）  \n",
    "torch.matmul(input, other, out=None)  \n",
    "input：必需参数，第一个参与矩阵乘法的张量。  \n",
    "other：必需参数，第二个参与矩阵乘法的张量。input 和 other 的形状需要满足矩阵乘法的规则或者可以通过广播机制使其满足规则。  \n",
    "out：可选参数，用于指定输出的张量。如果提供了该参数，计算结果会存储在这个张量中。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[0.6577, 0.3250, 0.3863, 0.5203],\n",
      "        [0.4745, 0.1535, 0.2023, 0.0663],\n",
      "        [0.4033, 0.0766, 0.7464, 0.3850],\n",
      "        [0.0417, 0.5896, 0.9003, 0.9706]])\n"
     ]
    }
   ],
   "source": [
    "# 计算两个张量之间矩阵乘法的⼏种⽅式。 y1, y2, y3 最后的值是⼀样的 dot\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "y3 = torch.rand_like(tensor)\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(tensor, tensor.T, out=y3) \n",
    "# out=y3 是一个可选参数，其作用是指定将矩阵乘法的结果存储到 y3 这个张量中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.mul 是 PyTorch 中的一个函数，用于执行按元素相乘（也称为逐元素相乘或哈达玛积）的操作。  \n",
    "torch.mul(input, other, out=None)  \n",
    "input：这是必需的参数，表示第一个输入的张量。  \n",
    "other：同样是必需的参数，代表第二个输入的张量或者一个标量值。如果 other 是张量，它需要和 input 能够进行广播操作；如果是标量，会将该标量与 input 中的每个元素相乘。  \n",
    "out：可选参数，用于指定输出的张量。如果提供了该参数，计算结果会存储在这个张量中。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) torch.Size([4, 4])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) torch.Size([4, 4])\n",
      "tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]]) torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "# 计算张量逐元素相乘的⼏种⽅法。 z1, z2, z3 最后的值是⼀样的。\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.ones_like(tensor)\n",
    "print(z1, z1.shape)\n",
    "print(z2, z2.shape)\n",
    "print(z3*2, z3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mul 乘法  multiplication    \n",
    "matnul 矩阵乘法  matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.mul 结果:\n",
      "tensor([[2, 4],\n",
      "        [6, 8]])\n",
      "torch.matmul 结果:\n",
      "tensor([[ 6,  6],\n",
      "        [14, 14]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor2 = torch.tensor([[2, 2], [2, 2]])\n",
    "mul_result = torch.mul(tensor1, tensor2)\n",
    "matmul_result = torch.matmul(tensor1, tensor2)\n",
    "print(\"torch.mul 结果:\")\n",
    "print(mul_result)\n",
    "print(\"torch.matmul 结果:\")\n",
    "print(matmul_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单元素张量  \n",
    "有时候需要将结果还原成传统的python数据结构，比如list，tuple，dict等,或者说还原成numpy数组，这时候就需要用到单元素张量。   \n",
    "为什么需要转换结果  \n",
    "在深度学习模型训练和推理过程中，使用 PyTorch 的张量可以利用 GPU 加速计算，提高效率。然而，当我们完成计算后，可能需要将结果展示给用户、保存到文件或者与其他不支持张量数据结构的库进行交互，这时就需要将张量转换为 Python 原生数据结构或 NumPy 数组。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "item() 方法的主要功能是将只包含一个元素的张量转换为 Python 的标量（如整数或浮点数）。  \n",
    "使用场景  \n",
    "损失值提取：在深度学习模型训练过程中，通常会计算损失值，这个损失值往往存储在一个单元素张量中。为了记录损失值、打印日志或者与其他 Python 代码交互，就需要使用 item() 方法将其转换为 Python 标量。  \n",
    "评估指标计算：在模型评估时，某些评估指标（如准确率、召回率等）可能在计算过程中以单元素张量的形式存在，使用 item() 可以方便地将其转换为可用于后续分析的 Python 标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 标量: 3.140000104904175\n",
      "数据类型: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# 单元素张量转换为 Python 标量\n",
    "import torch\n",
    "\n",
    "# 创建一个单元素张量\n",
    "single_element_tensor = torch.tensor([3.14])\n",
    "\n",
    "# 将单元素张量转换为 Python 标量\n",
    "python_scalar = single_element_tensor.item()\n",
    "print(\"Python 标量:\", python_scalar)\n",
    "print(\"数据类型:\", type(python_scalar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy() 方法用于将 PyTorch 张量转换为 NumPy 数组。这样做可以利用 NumPy 丰富的科学计算功能对张量数据进行进一步处理，同时也方便与其他依赖于 NumPy 数组的库进行交互。  \n",
    "使用场景  \n",
    "数据可视化：许多数据可视化库（如 Matplotlib）更适合处理 NumPy 数组。当需要对 PyTorch 张量中的数据进行可视化时，可以先使用 numpy() 方法将其转换为 NumPy 数组，再进行绘图操作。  \n",
    "与 NumPy 库交互：NumPy 提供了大量的数学函数和操作，在进行一些复杂的数值计算时，如果 PyTorch 没有直接对应的功能，可以将张量转换为 NumPy 数组，使用 NumPy 的函数进行计算。  \n",
    "模型评估：在模型评估阶段，可能需要使用一些基于 NumPy 实现的评估指标计算函数。将模型输出的 PyTorch 张量转换为 NumPy 数组后，就可以方便地使用这些函数进行评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy 数组:\n",
      " [[1 2 3 4 5]\n",
      " [1 2 3 4 5]\n",
      " [1 2 3 4 5]]\n",
      "数据类型: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# 多元素张量转换为 Python 列表或 NumPy 数组\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 创建一个多元素张量\n",
    "multi_element_tensor = torch.tensor([[1, 2, 3, 4, 5],[1, 2, 3, 4, 5],[1, 2, 3, 4, 5]])\n",
    "\n",
    "\n",
    "# 将多元素张量转换为 NumPy 数组\n",
    "numpy_array = multi_element_tensor.numpy()\n",
    "print(\"NumPy 数组:\\n\", numpy_array)\n",
    "print(\"数据类型:\", type(numpy_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tolist() 方法的主要功能是将 PyTorch 张量转换为嵌套的 Python 列表，转换后的列表元素的数据类型会根据原张量的数据类型进行相应转换。  \n",
    "例如，若原张量是浮点型，转换后的列表元素为 Python 的浮点数；若原张量是整型，转换后的列表元素为 Python 的整数。  \n",
    "使用场景  \n",
    "数据序列化：当需要把张量数据保存为 JSON 等格式时，由于这些格式通常要求数据为 Python 的原生数据结构，此时可以使用 tolist() 方法将张量转换为列表，方便进行序列化操作。  \n",
    "数据展示：在打印和展示张量数据时，Python 列表的格式更加直观，便于查看和调试。将张量转换为列表后，可以更清晰地看到数据的具体内容。  \n",
    "与 Python 库交互：部分 Python 库可能只支持 Python 列表作为输入，使用 tolist() 方法可以使 PyTorch 张量数据能够与这些库进行交互。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 列表:\n",
      " [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]\n",
      "数据类型: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 将多元素张量转换为 Python 列表\n",
    "python_list = multi_element_tensor.tolist()\n",
    "print(\"Python 列表:\\n\", python_list)\n",
    "print(\"数据类型:\", type(python_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“in - place” 指的是原地操作  \n",
    "直接在原数据对象上进行修改，而不额外创建新的对象来存储结果  \n",
    "优点：节省内存，提高效率  \n",
    "缺点：修改原数据对象，可能会影响其他引用该对象的地方，数据丢失的风险，代码可读性差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原地加法操作后的张量: tensor([3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个张量\n",
    "x = torch.tensor([1, 2, 3])\n",
    "\n",
    "# 原地加法操作\n",
    "x.add_(2)\n",
    "print(\"原地加法操作后的张量:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5032, -0.7186, -1.5596,  0.5872,  0.6048],\n",
      "        [-0.9052,  1.6010, -1.1181, -0.7012, -0.5280],\n",
      "        [ 2.7515, -1.1802, -0.2904, -0.6993, -0.3386],\n",
      "        [ 1.3986, -1.6924, -2.2190,  0.5322, -1.2768],\n",
      "        [ 0.1539,  0.5086,  0.4471, -0.7655, -0.7958]], requires_grad=True)\n",
      "tensor([-0.1968,  1.7218, -0.3845, -0.4743,  0.2547], requires_grad=True)\n",
      "tensor([0.1839], requires_grad=True)\n",
      "tensor([-1.0170,  0.4813, -0.1746, -0.8022,  0.2716], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "# 定义矩阵 A，向量 b 和常数 c\n",
    "# randn 生成服从标准正态分布（均值为 0，标准差为 1 的正态分布）的随机张量的函数\n",
    "A = torch.randn(5, 5,requires_grad=True) # requires_grad=True 表示要对A求导\n",
    "b = torch.randn(5,requires_grad=True)\n",
    "c = torch.randn(1,requires_grad=True)\n",
    "x = torch.randn(5, requires_grad=True)\n",
    "print(A)\n",
    "print(b)\n",
    "print(c)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'expression.png'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 计算 x^T * A + b * x + c\n",
    "result = torch.matmul(A, x.T) + torch.matmul(b, x) + c\n",
    "# ⽣成计算图节点\n",
    "dot = make_dot(result, params={'A': A, 'b': b, 'c': c, 'x': x})\n",
    "# 绘制计算图\n",
    "dot.render('expression', format='png', cleanup=True, view=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算图\n",
    "在 PyTorch 中，计算图是一个非常核心的概念，它是实现自动求导机制的基础。  \n",
    "\n",
    "定义\n",
    "计算图是一种有向无环图（DAG），用于表示数学表达式的计算过程。在 PyTorch 里，每个节点代表一个操作（如加法、乘法等），每条边代表操作之间的数据流动，即一个操作的输出作为另一个操作的输入。通过计算图，PyTorch 可以清晰地记录张量之间的计算关系，从而实现自动求导。  \n",
    "\n",
    "类型  \n",
    "动态计算图：PyTorch 使用的是动态计算图。动态计算图意味着在程序运行时动态构建计算图。这使得代码更加灵活，开发者可以根据不同的条件、循环等来动态改变计算图的结构。例如，在一个神经网络中，可以根据输入数据的不同，选择不同的网络层进行计算，计算图会根据实际的计算流程动态生成。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.], requires_grad=True)\n",
      "tensor([3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 构建过程 ：当在 PyTorch 中对张量进行操作时，计算图会自动构建。\n",
    "import torch\n",
    "from torchviz import make_dot  # torchviz 一个用于可视化 PyTorch 计算图的工具库\n",
    "\n",
    "# 创建两个需要进行梯度计算的张量\n",
    "# requires_grad=True，表示需要对它们进行梯度计算。 即求导\n",
    "x = torch.tensor([2.0], requires_grad=True) \n",
    "y = torch.tensor([3.0], requires_grad=True) \n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: tensor([5.], grad_fn=<AddBackward0>)\n",
      "w: tensor([10.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 进行计算操作\n",
    "z = x + y  # 此时 PyTorch 会自动构建一个计算图节点来表示加法操作\n",
    "w = z * 2  # 又会在计算图中添加一个乘法操作节点\n",
    "\n",
    "print(\"z:\", z)\n",
    "print(\"w:\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 工作原理\n",
    "##### 前向传播\n",
    "在进行前向传播时，程序会按照计算图中节点的顺序依次执行操作，从输入张量开始，逐步计算出最终的输出张量。  \n",
    "例如在上面的例子中，先计算 x + y 得到 z，再计算 z * 2 得到 w。\n",
    "##### 反向传播（梯度更新）\n",
    "当需要计算梯度时，会进行反向传播。反向传播从计算图的输出节点开始，根据链式法则，依次计算每个节点对应的梯度。  \n",
    "例如，要计算 w 关于 x 和 y 的梯度，会从 w 节点开始，沿着计算图反向传播，利用链式法则计算出 w 对 z 的梯度，再进一步计算出 w 对 x 和 y 的梯度。可以使用 backward() 方法来触发反向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x 的梯度: tensor([2.])\n",
      "y 的梯度: tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# 进行反向传播\n",
    "# 在调用 w.backward() 后，PyTorch 会自动计算 w 关于 x 和 y 的梯度，并将结果存储在 x.grad 和 y.grad 中。\n",
    "w.backward()\n",
    "\n",
    "# 查看梯度\n",
    "print(\"x 的梯度:\", x.grad)\n",
    "print(\"y 的梯度:\", y.grad)\n",
    "# print(\"z 的梯度:\", z.grad) 非叶子张量（non - leaf Tensor）\n",
    "# 在 PyTorch 的自动求导机制里，反向传播（autograd.backward()）时默认只会为叶子张量（leaf Tensor）填充梯度信息，非叶子张量的 .grad 属性不会被自动填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
