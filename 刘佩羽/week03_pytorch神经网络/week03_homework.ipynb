{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入需要的第三方库\n",
    "import torch\n",
    "from torchvision.datasets import KMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载\n",
    "training_data = KMNIST(root='./K_data', train=True, transform=ToTensor(), download=True)\n",
    "testing_data = KMNIST(root='./K_data', train=False, transform=ToTensor(), download=True)\n",
    "print(training_data)\n",
    "print(type(training_data))\n",
    "# 将数据集转换为字典\n",
    "data_dict = {i: (training_data.data[i], training_data.targets[i]) for i in range(len(training_data))}\n",
    "# 使用列表推导式和 break 语句输出一个元素\n",
    "result = {next((data for data, class_ in data_dict.items()), None): next((class_ for data, class_ in data_dict.items()), None)}\n",
    "# # 打印结果\n",
    "# print(result)\n",
    "# print((result[0][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "BATCH_SIZE = 1\n",
    "dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(dataloader)\n",
    "print(type(dataloader))\n",
    "# print([target for data, target in dataloader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造模型\n",
    "# 输入数据的数据特征值28*28=784个（查看数据集readme），输出层的类别数量为10\n",
    "# 设计神经元数量为num_of_neurons\n",
    "# 设计隐藏层的层数为1\n",
    "\n",
    "in_features = 28*28\n",
    "num_of_neurons = 64\n",
    "num_of_class = 10\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=in_features,\n",
    "              out_features=num_of_neurons, bias=True),  # 隐藏层1的线性函数\n",
    "    nn.Sigmoid(),                                       # 隐藏层1的激活函数\n",
    "    # nn.Linear(in_features=num_of_neurons,\n",
    "    #           out_features=num_of_neurons, bias=True),  # 隐藏层2的线性函数\n",
    "    # nn.Sigmoid(),                                       # 隐藏层2的激活函数\n",
    "    # nn.Linear(in_features=num_of_neurons,\n",
    "    #           out_features=num_of_neurons, bias=True),  # 隐藏层3的线性函数\n",
    "    # nn.Sigmoid(),                                       # 隐藏层3的激活函数\n",
    "    nn.Linear(in_features=num_of_neurons,\n",
    "              out_features=num_of_class, bias=True)   # 输出层线性函数\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定制模型损失函数和优化器\n",
    "LR = 0.7*1e-2\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    for feature_data, target in dataloader:\n",
    "        output = model(feature_data.reshape(-1, 784))   # 模型的前向计算\n",
    "        loss = loss_fn(output, target)              # 计算损失值\n",
    "        # print(output.shape)\n",
    "        # 进行梯度更新\n",
    "        optimizer.zero_grad()           # 所有的参数的梯度清零\n",
    "        loss.backward()             # 计算梯度存放在.grad里面\n",
    "        optimizer.step()                # 参数更新    \n",
    "        # break\n",
    "    print(f'epoch = {epoch}, loss = {loss.item()}') \n",
    "    # break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用测试集进行模型推理，测试模型\n",
    "test_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():    # 不计算梯度\n",
    "    for data, target in test_dataloader:\n",
    "        out_put = model(data.reshape(-1, 784))\n",
    "        _, predicted = torch.max(out_put, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted==target).sum().item()\n",
    "        # break\n",
    "# print(f'Accuracy: {correct/total*100:.2f}%')\n",
    "print(f'|{BATCH_SIZE}|{num_of_neurons}|{LR:.0e}|{epochs}|{correct/total*100:.2f}%|')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准确率记录\n",
    "\n",
    "\n",
    "|BATCH_SIZE|num_of_neurons|LR|epochs|**Accuracy**|\n",
    "|---|---|----|--------|-------------|\n",
    "|128|128|1e-03|20|48.21%+48.84%+47.38%|\n",
    "|64|64|1e-03|20|49.71%+49.61%+51.86%|\n",
    "|32|64|1e-03|20|56.80%|\n",
    "|16|64|1e-03|20|63.40%|\n",
    "|8|64|1e-03|20|68.30%|\n",
    "|4|64|1e-03|20|73.38%|\n",
    "|2|64|1e-03|20|78.08%|\n",
    "|1|128|1e-03|20|82.52%|\n",
    "|1|64|1e-03|20|82.67%|\n",
    "|1|64|1e-04|20|66.86%|\n",
    "|1|64|1e-03|40|85.61%|\n",
    "|1|64|1e-03|60|86.86%|\n",
    "|1|64|1e-02|20|86.50%|\n",
    "|1|64|5e-03|20|87.08%|\n",
    "|1|64|7e-03|20|87.26%|\n",
    "|1|64|7e-03|5|84.57%|\n",
    "|1|64|3e-03|20|85.99%|\n",
    "|1|64|1e-01|20|84.72%|\n",
    "|1|32|1e-03|40|81.66%|\n",
    "|1|32|1e-03|20|80.89%|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_put = torch.randn((3,5))\n",
    "print(out_put)\n",
    "print(out_put.size(0))\n",
    "_, predict =torch.max(out_put, 0)\n",
    "print('_, predict::',_, predict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
