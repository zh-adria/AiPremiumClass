```
逻辑回归是正态分布
损失函数和伯努利概率密度函数相关
二分类损失函数、交叉熵损失函数
计算图有自动求导的功能
超参数学习率是控制模型在每次迭代中更新权重的幅度
循环终止条件跟损失函数有关，梯度值小于某个值
连续损失函数差小于一个阈值，更新的步长非常小了
y_hat是逻辑回归的结果值,预测值
100个样本，每个本里有20个特征值。100行20列
与自变量相关的参数都统称为θ参数向量
最小二乘法适合比较简单的数据，计算比较繁琐，误差平方和越小越好
人最不合适做重复性的工作
交叉熵的假设是什么？？？？
X大x是整体的样本集合


```

# 逻辑回归

- 把**线性回归模型映射为概率模型**（把线性回归的结果值再做了一步运算，映射到0-1的区间内）。为什么要映射为概率，就是相当于说输出的结果有多大把握是正确的。

- 也就是说把模型的输出从完整的实数空间，映射到[0,1]，把输出值转换为概率值 。 

- 逻辑回归是0-1分布，伯努利分布。

- 正态分布σ（标准差），越大越扁平，越小越尖锐。

- 概率和似然的区别：概率是已知参数求预测结果，似然是已知结果去求参数。 

- 最大似然估计：找到一个最适合当前数据的概率分布。 

- 似然函数：得到的是每个数据点x在假设的概率分布中出现的可能性。也就是条件概率函数。

- 对数似然函数是最常用的损失函数

- 概率密度函数是什么样的，就把该函数写成损失函数。

  **理解：**把线性回归的预测值，通过sigmoid映射到了0-1，再用伯努利分布的概率密度函数作为损失函数，用梯度下降法去求损失函数最小时，对应的参数。

- 常用的：二分类损失函数、交叉熵损失函数（假设是什么）

# 梯度下降法

- 它是一种基于搜索的最优化方法，目标是用于优化一个目标函数，作用就是最小化一个损失函数。
- 两步：梯度值和测量频率(学习率取值在1e-2,-1e-3)。即方向和步长（每次迭代时梯度更新的幅度）
- 高维的超平面
- 对每个数据点的损失求平均值
- 函数在某一点的梯度就是一个向量
- 动态学习率可以用函数控制
- 有些模型的损失函数根据目标来确定

## 似然函数和损失函数的关系

- 似然函数取对数，再取负，就是损失函数
- 也即负对数似然函数就是交叉熵损失函数
- 本质上是统一的，一个是最大化，一个是最小化