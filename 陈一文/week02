# 导入数据库
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# 建立数据集
X, y = load_iris(return_X_y= True)
y = (y == 0).astype(int)

# 拆分数据
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 42)

# 参数初始化
theta = np.random.randn(1, 4)
bias = 0
lr = 0.1
epoch = 1000
e = 1e-7

# 前向传播模型
def forward(x, theta, bias):
    # 线性运算
    z = np.dot(theta, x.T) + bias
    # 激活函数sigmoid
    y_pred = 1 / (1 + np.exp(-z))
    return y_pred

# 计算损失函数
def loss_func(y, y_pred):
    return -(y * np.log(y_pred + e) + (1 - y) * np.log(1 - y_pred + e))

# 计算梯度
def gradient(x, y, y_pred):
    m = x.shape[0]
    delta_theta = np.dot(y_pred - y, x) / m
    delta_b = np.mean(y_pred - y)
    return delta_theta, delta_b

# 模型训练
for i in range(epoch):
    # 前向
    y_pred = forward(X_train, theta, bias)

    # 损失
    loss_val = loss_func(y_train, y_pred)
    # 梯度
    delta_theta, delta_bias = gradient(X_train, y_train, y_pred)
    # 更新参数
    theta -= lr * delta_theta
    bias -= lr * delta_bias

    # 计算准确率
    if epoch % 100 == 0:
        accuracy = np.mean(np.round(y_pred) == y_train)
        print(f'lossfunction:{np.mean(loss_val)}, accuracy:{accuracy}')

# 保存参数
np.savez('model_params.npz', theta=theta, bias=bias)
