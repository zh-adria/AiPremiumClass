{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#逻辑回归模型---代码与test01文件一样，只是修改了学习率\n",
    "test01文件中学习率为1e-1,这里修改为1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 4)\n"
     ]
    }
   ],
   "source": [
    "#导入sklearn包\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "iris=datasets.load_iris()\n",
    "\n",
    "##1、准备数据\n",
    "x=iris.data[0:100,:]\n",
    "y=iris.target[0:100]\n",
    "\n",
    "#1.1、拆分训练和测试集\n",
    "#当 shuffle=True 时，生成的样本会按照随机顺序排列；\n",
    "# #当 shuffle=False 时，生成的样本会按照类别顺序依次排列。\n",
    "train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.2,shuffle=True)\n",
    "\n",
    "##1.2、初始化模型参数\n",
    "print(train_x.shape)\n",
    "theta=np.random.randn(1,4)\n",
    "bias=0 \n",
    "##1.3、学习率\n",
    "lr=1e-3\n",
    "##1.4、模型训练的轮数\n",
    "epoch=5000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###2、前向计算\n",
    "def forward(x,theta,bias):\n",
    "    z=np.dot(theta,x.T)+bias \n",
    "    ##sigmoid\n",
    "    y_hat=1/(1+np.exp(-z))\n",
    "    return y_hat\n",
    "\n",
    "#3、损失函数\n",
    "def loss_function(y,y_hat):\n",
    "    e=1e-8  ###防止y_hat=0\n",
    "    return -y*np.log(y_hat+e)-(1-y)*np.log(1-y_hat+e)\n",
    "\n",
    "#4、计算梯度\n",
    "def calc_gradient(x,y,y_hat):\n",
    "    m=x.shape[-1]\n",
    "    delta_w=np.dot(y_hat-y,x)/m\n",
    "    delta_b=np.mean(y_hat-y)\n",
    "    return delta_w,delta_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0  loss:  0.008091967200222202\n",
      "step:  200  loss:  0.007802129073213697\n",
      "step:  400  loss:  0.00753328079220722\n",
      "step:  600  loss:  0.007283193580681158\n",
      "step:  800  loss:  0.007049945087690879\n",
      "step:  1000  loss:  0.006831868331983462\n",
      "step:  1200  loss:  0.006627510541322962\n",
      "step:  1400  loss:  0.006435599717377893\n",
      "step:  1600  loss:  0.0062550172840461216\n",
      "step:  1800  loss:  0.00608477556451638\n",
      "step:  2000  loss:  0.005923999119855046\n",
      "step:  2200  loss:  0.005771909197321107\n",
      "step:  2400  loss:  0.005627810699485257\n",
      "step:  2600  loss:  0.005491081209433806\n",
      "step:  2800  loss:  0.005361161702813929\n",
      "step:  3000  loss:  0.00523754865142591\n",
      "step:  3200  loss:  0.005119787280753752\n",
      "step:  3400  loss:  0.005007465789128272\n",
      "step:  3600  loss:  0.004900210372023223\n",
      "step:  3800  loss:  0.00479768092345626\n",
      "step:  4000  loss:  0.004699567309238151\n",
      "step:  4200  loss:  0.004605586125123205\n",
      "step:  4400  loss:  0.004515477867715413\n",
      "step:  4600  loss:  0.004429004458008756\n",
      "step:  4800  loss:  0.00434594706725354\n",
      "[[-1.10580217 -1.75058319  3.45292523  2.52430891]]\n",
      "-0.022389982878365588\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    ##正向\n",
    "    y_hat=forward(train_x,theta,bias)\n",
    "    ##计算损失\n",
    "    loss=np.mean(loss_function(train_y,y_hat))\n",
    "    if i%200==0:\n",
    "        print('step: ',i,' loss: ',loss)\n",
    "    ###梯度下降\n",
    "    dw,db=calc_gradient(train_x,train_y,y_hat)\n",
    "    ##更新参数\n",
    "    theta-=lr*dw\n",
    "    bias-=lr*db\n",
    "print(theta)\n",
    "print(bias)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.022389982878365588\n",
      "[1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "##保存模型参数\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(bias)\n",
    "##保存模型参数 theta 、bias\n",
    "df_theta=pd.DataFrame(theta);\n",
    "df_theta.to_csv(\"theta.csv\",index=False)\n",
    "\n",
    "df_bias=pd.DataFrame([bias]);\n",
    "df_bias.to_csv(\"bias.csv\",index=False)\n",
    "\n",
    "print(test_y)\n",
    " \n",
    "###测试数据导出\n",
    "df=pd.DataFrame(test_y)\n",
    "df.to_csv(\"test_y.csv\",index=False)\n",
    "\n",
    "dfx=pd.DataFrame(test_x)\n",
    "dfx.to_csv(\"test_x.csv\",index=False)\n",
    "    \n",
    "     \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测值：[1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0]\n",
      " 真实值：[1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##测试模型\n",
    "def predict(x):\n",
    "    \n",
    "    pred=forward(x,theta,bias)\n",
    "    return [1 if i>0.5 else 0 for i in pred[0]]\n",
    "    \n",
    " \n",
    "sum=0\n",
    "pred=predict(test_x)\n",
    "print(f\"预测值：{pred}\\n 真实值：{test_y}\")\n",
    " \n",
    "sum=np.sum(pred-test_y)\n",
    "sum\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
