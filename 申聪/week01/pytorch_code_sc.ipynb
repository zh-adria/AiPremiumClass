import torch

data = torch.tensor([[1,2],[3,4]], dtype=torch.float32)
data
import numpy as np

np_array = np.array([[1,2],[3,4]])
data2 = torch.from_numpy(np_array)
data2
data2.dtype
# 通过已知张量维度，创建新张量
data3 = torch.rand_like(data2, dtype=torch.float)   #data2原本为整型，rand输出为浮点型，like时要变换类型
data3
shape = (2,3,1)
rand_tensor = torch.rand(shape)
ones_tensor = torch.ones(shape)
zeros_tensor = torch.zeros(shape)

print(f"Random Tensor: \n {rand_tensor} \n")
print(f"Ones Tensor: \n {ones_tensor} \n")
print(f"Zeros Tensor: \n {zeros_tensor}")
# 基于现有tensor构建，但使用新值填充
m = torch.ones(5,3, dtype=torch.double)    #五行三列
n = torch.rand_like(m, dtype=torch.float)  #like   m      

# 获取tensor的大小
print(m.size()) # torch.Size([5,3])

# 均匀分布
print(torch.rand(5,3))
# 标准正态分布
print(torch.randn(5,3))
# 离散正态分布
print(torch.normal(mean=.0,std=1.0,size=(5,3)))
# 线性间隔向量(返回一个1维张量，包含在区间start和end上均匀间隔的steps个点)
print(torch.linspace(start=1,end=10,steps=21))          #等差数列
tensor = torch.rand(3,4)

print(f"Shape of tensor: {tensor.shape}")               #维度   print(f"Shape of tensor: {tensor.size()}")同等效果    shaoe间接调用size()
print(f"Datatype of tensor: {tensor.dtype}")            #创建张量时，不主动强调类型，默认是浮点类型 
print(f"Device tensor is stored on: {tensor.device}")   #设备    默认创建在cpu端，内存中
print(torch.cuda.is_available())    #判断是否能在gpu端运行
# 检查pytorch是否支持GPU
if torch.cuda.is_available():
    device = torch.device("cuda")
    tensor = tensor.to(device)

print(tensor)
print(tensor.device)

# mac上没有GPU，使用M系列芯片
if torch.backends.mps.is_available():
    device = torch.device("mps")
    tensor = tensor.to(device)

print(tensor)
print(tensor.device)
tensor = torch.ones(4, 4)
print('First row: ', tensor[0])                #输出第一行
print('First column: ', tensor[:, 0])          #输出第一列
print('Last column:', tensor[..., -1])         #输出最后一维，“...”忽略前面所有维度
tensor[:,1] = 0                                #第一列置为0
print(tensor)
t1 = torch.cat([tensor, tensor, tensor], dim=1)    #相同的张量三份，在第一维上拼接（ dim=1）
print(t1 * 3)
print(t1.shape)
t2 = torch.tensor([[(1,2),(2,3)],[(5,6),(7,8)]])
print(t2)
print(t2.shape)
t3 = torch.cat([t2, t2, t2], dim=0)    #相同的张量三份，在第二维上拼接（ dim=2）
print(t3)
print(t3 * 3)
print(t3.shape)
import torch
tensor = torch.arange(1,10, dtype=torch.float32).reshape(3, 3)

# 计算两个张量之间矩阵乘法的几种方式。 y1, y2, y3 最后的值是一样的 dot
y1 = tensor @ tensor.T
y2 = tensor.matmul(tensor.T)

# print(y1)
# print(y2)

y3 = torch.rand_like(tensor)
torch.matmul(tensor, tensor.T, out=y3)   #将计算结构填充到y3中（ out=y3），确保y3能够放下结果
# print(y3)


# 计算张量逐元素相乘的几种方法。 z1, z2, z3 最后的值是一样的。
z1 = tensor * tensor
z2 = tensor.mul(tensor)

z3 = torch.rand_like(tensor)
torch.mul(tensor, tensor, out=z3)

print(z1)
print(z3)
agg = tensor.sum()  #求和
print(agg)
agg_item = agg.item()   #tensor  ---->  python基础类型
print(agg_item, type(agg_item))
np_arr = z1.numpy()  #变为数组
np_arr
print(tensor, "\n")
tensor.add_(5)
# tensor = tensor + 5
# tensor += 5
print(tensor)
tensor
import torch
from torchviz import make_dot

# 定义矩阵 A，向量 b 和常数 c
A = torch.randn(10, 10,requires_grad=True)  # requires_grad=True 表示我们要对 A 求导
b = torch.randn(10,requires_grad=True)
c = torch.randn(1,requires_grad=True)
x = torch.randn(10, requires_grad=True)


# 计算 x^T * A + b * x + c
result = torch.matmul(A, x.T) + torch.matmul(b, x) + c

# 生成计算图节点
dot = make_dot(result, params={'A': A, 'b': b, 'c': c, 'x': x})
# 绘制计算图
dot.render('expression', format='png', cleanup=True, view=False)
