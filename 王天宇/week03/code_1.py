
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

# 定义输入层大小、隐藏层大小、输出层大小
n_in, n_h, n_out = 10, 5, 1

# 创建虚拟输入数据和目标数据
# x = torch.randn(10, 10) 
y = torch.tensor([[1.0], [0.0], [0.0], 
                 [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]])  # 目标输出数据
x = torch.tensor([[ 0.9913, -1.2754, -0.6440, -0.7147,  1.0668, -0.1711,  0.8180, -0.7659,
         -0.9032,  0.4671],
        [-0.8037,  0.4822,  1.5859,  0.1388, -1.3084,  1.6657,  0.6918, -0.0205,
          0.5614, -0.2263],
        [-0.1598,  0.5890,  0.3677,  1.0580,  0.6470,  0.2831,  1.5518,  0.9883,
          0.6613, -0.8833],
        [-1.6441, -1.2772,  0.0237,  1.1458,  2.0018, -1.3466,  1.3389,  2.1080,
         -0.2711, -0.2306],
        [-0.6314, -0.9136, -0.0395,  0.2317, -0.4872, -0.6342, -0.8857,  1.1337,
         -0.1144,  0.9097],
        [ 0.3319,  0.2681, -1.0642, -0.0539,  1.8322,  1.2153, -0.0845,  0.3035,
          0.1304,  0.2218],
        [ 0.5094,  2.8553,  1.2010,  0.2682, -0.0272,  0.6394, -1.3639,  0.0826,
          0.1004,  0.1589],
        [-1.3733,  0.4849,  1.2640, -0.9239,  1.1468,  0.0070,  0.5036,  1.4834,
         -0.2303, -0.2439],
        [ 0.9419, -0.2708, -1.8153, -0.7438,  0.8633,  0.2806, -0.9276, -0.0817,
         -0.2975,  0.0453],
        [-0.3657,  1.2744,  1.0678, -0.9937, -1.0450,  0.3796,  1.4830,  0.7431,
          1.1771, -0.0613]])
# print(x)
# 创建顺序模型，包含线性层、ReLU激活函数和Sigmoid激活函数
model = nn.Sequential(
   nn.Linear(n_in, n_h),  # 输入层到隐藏层的线性变换
   nn.ReLU(),            # 隐藏层的ReLU激活函数
   nn.Linear(n_h, n_out),  # 隐藏层到输出层的线性变换
   nn.Sigmoid()           # 输出层的Sigmoid激活函数
)
#在GPU上训练
model.to('cuda')
# 定义均方误差损失函数和随机梯度下降优化器
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.05)  # 学习率为0.01

# 执行梯度下降算法进行模型训练
loss_arr = []
for epoch in range(500):  
   y_pred = model(x)  # 前向传播，计算预测值
   loss = criterion(y_pred, y)  # 计算损失
   loss_arr.append(loss.item())
   if epoch % 10 == 0:  # 每10次迭代打印一次损失值
      print('epoch: ', epoch, 'loss: ', loss.item())

   optimizer.zero_grad()  # 清零梯度
   loss.backward()  # 反向传播，计算梯度
   optimizer.step()  # 更新模型参数
   
#可视化损失值的变化
plt.figure(figsize=(8, 5))
plt.plot(range(1, 501), loss_arr, label='Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Over Epochs')
plt.legend()
plt.grid()
plt.show()
   
"""
观察：
    学习率不变的情况下：
    通过设置不同数量的隐藏层节点数，可以得到不同的结果。
    当隐藏层 = 输入层的数量时，epoch:  490 loss:  0.042290233075618744
    当隐藏层 > 输出层的数量时，epoch:  490 loss:  0.026063192635774612
    当隐藏层 < 输出层的数量时，epoch:  490 loss:  0.06166771054267883
"""