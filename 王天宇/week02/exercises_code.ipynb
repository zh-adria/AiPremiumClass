{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.9,\n",
       " 1.92,\n",
       " 1.936,\n",
       " 1.9487999999999999,\n",
       " 1.95904,\n",
       " 1.9672319999999999,\n",
       " 1.9737855999999998,\n",
       " 1.9790284799999998,\n",
       " 1.9832227839999998,\n",
       " 1.9865782271999999,\n",
       " 1.9892625817599998,\n",
       " 1.9914100654079998,\n",
       " 1.9931280523264,\n",
       " 1.99450244186112,\n",
       " 1.995601953488896,\n",
       " 1.9964815627911168,\n",
       " 1.9971852502328935,\n",
       " 1.9977482001863147,\n",
       " 1.9981985601490517,\n",
       " 1.9985588481192413]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用梯度下降法，寻找一个一元二次方程的最小值\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def f(x): # 定义函数\n",
    "    return (x - 2)**2 -1\n",
    "\n",
    "#1、确定x的取值范围\n",
    "x = np.linspace(-5,5,100)\n",
    "#2、设置学习率、学习次数、精度、截距初始参数\n",
    "eta = 0.1 #学习率\n",
    "learning_nums = 100 #学习次数\n",
    "epsilon = 1e-6 #10的-6次方\n",
    "theta = 1.9 #初始参数(这个参数的初始值如何设置！？！也就是我们的观测值，最后和真实值作比较)\n",
    "#3、求取方程的导数\n",
    "def grad(theta): # 求导数\n",
    "    return 2*(theta - 2)\n",
    "#4、损失函数\n",
    "def loss(theta): # 损失函数\n",
    "    return (theta - 2)**2 -1\n",
    "#4、梯度下降函数体\n",
    "def gradient_descent(theta,eta,learning_nums,epsilon):\n",
    "    times = 0 #记录次数\n",
    "    theta_history = []\n",
    "    while times < learning_nums:\n",
    "        last_theta = theta #记录上一次的theta\n",
    "        theta_history.append(theta) #记录历史的theta\n",
    "        gradient = grad(theta)  # 参数值对应的梯度\n",
    "        theta = theta - eta * gradient  # 更新参数\n",
    "        if abs(loss(theta) - loss(last_theta)) < epsilon: # 满足条件则退出\n",
    "            break\n",
    "        times += 1 \n",
    "    return theta_history\n",
    "\n",
    "gradient_descent(theta,eta,learning_nums,epsilon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
