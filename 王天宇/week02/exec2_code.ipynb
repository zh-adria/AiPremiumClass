{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逻辑回归\n",
    "\n",
    "import numpy as np\n",
    "# 1、准备训练数据\n",
    "# 2、初始化模型参数\n",
    "# 3、定义损失函数\n",
    "# 4、梯度下降\n",
    "# 5、预测结果\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.1, num_iterations=10000):\n",
    "        self.learning_rate = learning_rate #学习率\n",
    "        self.num_iterations = num_iterations #迭代次数\n",
    "        self.weights = None #权重\n",
    "        self.bias = None #偏置（初始预测值）\n",
    "    #sigmoid函数\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z)) \n",
    "    #损失函数\n",
    "    def _loss(self, y, y_pred):\n",
    "        e = 1e-8 #添加一个极小值，防止出现除数为0的情况\n",
    "        return (-y * np.log(y_pred) - (1 - y + e) * np.log(1 - y_pred + e)).mean() \n",
    "    #训练函数\n",
    "    def fit(self, X, y):\n",
    "        # 初始化权重和初始预测值\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "        # 梯度下降\n",
    "        for i in range(self.num_iterations):\n",
    "            y_pred = self._sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "            loss_value = self._loss(y,y_pred)\n",
    "            dw = (1 / X.shape[0]) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / X.shape[0]) * np.sum(y_pred - y)\n",
    "            self.weights -= self.learning_rate * dw #更新权重和偏置\n",
    "            self.bias -= self.learning_rate * db    #更新偏置\n",
    "            if i % 100 == 0:\n",
    "                # 计算准确率\n",
    "                acc = np.mean(np.round(y_pred) == y)  \n",
    "                print(f\"epoch: {i}, loss: {np.mean(loss_value)}, acc: {acc}\")\n",
    "    #预测函数\n",
    "    def predict(self, X):\n",
    "        y_pred = self._sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "        return y_pred\n",
    " \n",
    "# 上面定义了一个 LogisticRegression 的类，用于实现逻辑回归算法\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 数据拆分\n",
    "# 只取其中一部分数据进行训练，防止出现过拟合，泛化能力差的问题\n",
    "X,y = make_classification(n_samples=150, n_features=10)  # shape (150, 10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.6931471768247505, acc: 0.5333333333333333\n",
      "epoch: 100, loss: 0.6394413987536876, acc: 0.9142857142857143\n",
      "epoch: 200, loss: 0.5950160479116745, acc: 0.9142857142857143\n",
      "epoch: 300, loss: 0.5578026742107259, acc: 0.9238095238095239\n",
      "epoch: 400, loss: 0.526202964809406, acc: 0.9428571428571428\n",
      "epoch: 500, loss: 0.49901201862509575, acc: 0.9523809523809523\n",
      "epoch: 600, loss: 0.47532783790654104, acc: 0.9523809523809523\n",
      "epoch: 700, loss: 0.4544728052264443, acc: 0.9523809523809523\n",
      "epoch: 800, loss: 0.4359330540970329, acc: 0.9523809523809523\n",
      "epoch: 900, loss: 0.41931401817698344, acc: 0.9523809523809523\n",
      "epoch: 1000, loss: 0.4043085470065155, acc: 0.9523809523809523\n",
      "epoch: 1100, loss: 0.3906741776687898, acc: 0.9523809523809523\n",
      "epoch: 1200, loss: 0.3782168938228054, acc: 0.9619047619047619\n",
      "epoch: 1300, loss: 0.36677943529722606, acc: 0.9619047619047619\n",
      "epoch: 1400, loss: 0.35623279838329314, acc: 0.9619047619047619\n",
      "epoch: 1500, loss: 0.3464699846884678, acc: 0.9619047619047619\n",
      "epoch: 1600, loss: 0.3374013476440944, acc: 0.9619047619047619\n",
      "epoch: 1700, loss: 0.3289510855532222, acc: 0.9619047619047619\n",
      "epoch: 1800, loss: 0.32105456646515057, acc: 0.9619047619047619\n",
      "epoch: 1900, loss: 0.3136562634244182, acc: 0.9619047619047619\n",
      "epoch: 2000, loss: 0.30670814274017755, acc: 0.9619047619047619\n",
      "epoch: 2100, loss: 0.3001683923039801, acc: 0.9619047619047619\n",
      "epoch: 2200, loss: 0.29400040798314975, acc: 0.9619047619047619\n",
      "epoch: 2300, loss: 0.2881719779740023, acc: 0.9714285714285714\n",
      "epoch: 2400, loss: 0.2826546205623294, acc: 0.9714285714285714\n",
      "epoch: 2500, loss: 0.27742304193110034, acc: 0.9714285714285714\n",
      "epoch: 2600, loss: 0.2724546887850994, acc: 0.9714285714285714\n",
      "epoch: 2700, loss: 0.2677293765255871, acc: 0.9714285714285714\n",
      "epoch: 2800, loss: 0.26322897812468693, acc: 0.9714285714285714\n",
      "epoch: 2900, loss: 0.258937162151064, acc: 0.9714285714285714\n",
      "epoch: 3000, loss: 0.254839170889535, acc: 0.9714285714285714\n",
      "epoch: 3100, loss: 0.2509216313932191, acc: 0.9714285714285714\n",
      "epoch: 3200, loss: 0.24717239376218303, acc: 0.9714285714285714\n",
      "epoch: 3300, loss: 0.24358039206877394, acc: 0.9714285714285714\n",
      "epoch: 3400, loss: 0.24013552422826556, acc: 0.9714285714285714\n",
      "epoch: 3500, loss: 0.236828547803698, acc: 0.9714285714285714\n",
      "epoch: 3600, loss: 0.23365098928011374, acc: 0.9714285714285714\n",
      "epoch: 3700, loss: 0.23059506477872463, acc: 0.9714285714285714\n",
      "epoch: 3800, loss: 0.22765361053071698, acc: 0.9714285714285714\n",
      "epoch: 3900, loss: 0.22482002171219906, acc: 0.9714285714285714\n",
      "epoch: 4000, loss: 0.22208819847056158, acc: 0.9714285714285714\n",
      "epoch: 4100, loss: 0.2194524981592849, acc: 0.9714285714285714\n",
      "epoch: 4200, loss: 0.2169076929515066, acc: 0.9714285714285714\n",
      "epoch: 4300, loss: 0.2144489321291053, acc: 0.9714285714285714\n",
      "epoch: 4400, loss: 0.21207170844886303, acc: 0.9714285714285714\n",
      "epoch: 4500, loss: 0.2097718280745339, acc: 0.9714285714285714\n",
      "epoch: 4600, loss: 0.2075453836366321, acc: 0.9714285714285714\n",
      "epoch: 4700, loss: 0.20538873004303831, acc: 0.9714285714285714\n",
      "epoch: 4800, loss: 0.20329846271519308, acc: 0.9714285714285714\n",
      "epoch: 4900, loss: 0.2012713979683738, acc: 0.9714285714285714\n",
      "epoch: 5000, loss: 0.19930455529168012, acc: 0.9714285714285714\n",
      "epoch: 5100, loss: 0.19739514131499974, acc: 0.9714285714285714\n",
      "epoch: 5200, loss: 0.19554053527727783, acc: 0.9714285714285714\n",
      "epoch: 5300, loss: 0.1937382758336118, acc: 0.9714285714285714\n",
      "epoch: 5400, loss: 0.19198604905864597, acc: 0.9714285714285714\n",
      "epoch: 5500, loss: 0.19028167752095354, acc: 0.9714285714285714\n",
      "epoch: 5600, loss: 0.1886231103179787, acc: 0.9714285714285714\n",
      "epoch: 5700, loss: 0.18700841397401738, acc: 0.9714285714285714\n",
      "epoch: 5800, loss: 0.18543576411493906, acc: 0.9714285714285714\n",
      "epoch: 5900, loss: 0.18390343784312735, acc: 0.9714285714285714\n",
      "epoch: 6000, loss: 0.18240980674465887, acc: 0.9714285714285714\n",
      "epoch: 6100, loss: 0.1809533304682128, acc: 0.9714285714285714\n",
      "epoch: 6200, loss: 0.17953255082176375, acc: 0.9714285714285714\n",
      "epoch: 6300, loss: 0.17814608633886983, acc: 0.9714285714285714\n",
      "epoch: 6400, loss: 0.17679262727145034, acc: 0.9714285714285714\n",
      "epoch: 6500, loss: 0.1754709309704187, acc: 0.9714285714285714\n",
      "epoch: 6600, loss: 0.17417981761950563, acc: 0.9714285714285714\n",
      "epoch: 6700, loss: 0.17291816629110524, acc: 0.9714285714285714\n",
      "epoch: 6800, loss: 0.17168491129609656, acc: 0.9714285714285714\n",
      "epoch: 6900, loss: 0.17047903880235102, acc: 0.9714285714285714\n",
      "epoch: 7000, loss: 0.16929958369910286, acc: 0.9714285714285714\n",
      "epoch: 7100, loss: 0.16814562668654826, acc: 0.9714285714285714\n",
      "epoch: 7200, loss: 0.16701629157200226, acc: 0.9714285714285714\n",
      "epoch: 7300, loss: 0.16591074275569087, acc: 0.9714285714285714\n",
      "epoch: 7400, loss: 0.16482818289082551, acc: 0.9714285714285714\n",
      "epoch: 7500, loss: 0.16376785070401212, acc: 0.9714285714285714\n",
      "epoch: 7600, loss: 0.16272901896331274, acc: 0.9714285714285714\n",
      "epoch: 7700, loss: 0.16171099258240748, acc: 0.9714285714285714\n",
      "epoch: 7800, loss: 0.16071310685033235, acc: 0.9714285714285714\n",
      "epoch: 7900, loss: 0.15973472577718612, acc: 0.9714285714285714\n",
      "epoch: 8000, loss: 0.15877524054703251, acc: 0.9714285714285714\n",
      "epoch: 8100, loss: 0.15783406806997327, acc: 0.9714285714285714\n",
      "epoch: 8200, loss: 0.15691064962604973, acc: 0.9714285714285714\n",
      "epoch: 8300, loss: 0.1560044495942439, acc: 0.9714285714285714\n",
      "epoch: 8400, loss: 0.15511495426040847, acc: 0.9714285714285714\n",
      "epoch: 8500, loss: 0.1542416706984621, acc: 0.9714285714285714\n",
      "epoch: 8600, loss: 0.1533841257196453, acc: 0.9714285714285714\n",
      "epoch: 8700, loss: 0.15254186488505256, acc: 0.9714285714285714\n",
      "epoch: 8800, loss: 0.15171445157703348, acc: 0.9714285714285714\n",
      "epoch: 8900, loss: 0.15090146612540797, acc: 0.9714285714285714\n",
      "epoch: 9000, loss: 0.15010250498475294, acc: 0.9714285714285714\n",
      "epoch: 9100, loss: 0.14931717995930807, acc: 0.9714285714285714\n",
      "epoch: 9200, loss: 0.14854511747231516, acc: 0.9714285714285714\n",
      "epoch: 9300, loss: 0.14778595787684112, acc: 0.9714285714285714\n",
      "epoch: 9400, loss: 0.14703935480536418, acc: 0.9714285714285714\n",
      "epoch: 9500, loss: 0.14630497455559704, acc: 0.9714285714285714\n",
      "epoch: 9600, loss: 0.14558249551021366, acc: 0.9714285714285714\n",
      "epoch: 9700, loss: 0.1448716075883114, acc: 0.9714285714285714\n",
      "epoch: 9800, loss: 0.14417201172660232, acc: 0.9714285714285714\n",
      "epoch: 9900, loss: 0.14348341938846546, acc: 0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "# 训练模型函数执行，可调整学习率，优化模型\n",
    "model = LogisticRegression(0.001) \n",
    "# 问题记录：学习率的调整 0.001 这个值是如何调整来的\n",
    "#   设置大：出现先达损失函数最小值，然后又开始上升的情况，即震荡情况\n",
    "#   设置小时，会出现不能收敛到最小值的情况\n",
    "\n",
    "#print(np.shape(X_train)) #(105, 10)\n",
    "#print(np.shape(y_train)) #(105,)\n",
    "#print(y_train)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: 1, predict: 0.7999502694344001\n"
     ]
    }
   ],
   "source": [
    "# 模型训练结果检验，随机选择一个样本的x，y值，进行预测   \n",
    "idx = np.random.randint(len(X_test))\n",
    "x = X_test[idx]\n",
    "y = y_test[idx]\n",
    "y_pred = model.predict(x)\n",
    "print(f\"y: {y}, predict: {y_pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
