{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#生成训练数据\n",
    "X,y = make_classification(n_samples=150,n_features=10)\n",
    "# print(X.shape)\n",
    "#把训练数据进行拆分\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)     #调整拆分比例0.2、0.3、0.4、0.5，不同的数据拆分比例对训练结果会产生影响，调整拆分比例可对训练数据准确率进行优化\n",
    "# print(X_train.shape)\n",
    "# print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.90844118 -0.66285787  0.41531736 -0.69281076 -0.73913848 -0.80900627\n",
      "  -2.00031681  2.60892435  0.68360914  1.52866812]]\n"
     ]
    }
   ],
   "source": [
    "#权重参数\n",
    "theta = np.random.randn(1,10)\n",
    "bias = 0\n",
    "#超参数\n",
    "lr = 0.2    #调整学习率0.1、0.2、0.3、0.4、0.5，不同的学习率对训练结果训练结果准确率会产生影响\n",
    "epochs = 3000\n",
    "# print(theta.shape)\n",
    "print(theta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.31666666666666665 loss： 2.5213154547841117\n",
      "100 0.9666666666666667 loss： 0.14112123615786726\n",
      "200 0.9666666666666667 loss： 0.1363217912007271\n",
      "300 0.9583333333333334 loss： 0.1341929908540359\n",
      "400 0.9583333333333334 loss： 0.13310138120755652\n",
      "500 0.9583333333333334 loss： 0.13248880913656644\n",
      "600 0.9583333333333334 loss： 0.13212385669456284\n",
      "700 0.9583333333333334 loss： 0.13189717875390555\n",
      "800 0.9583333333333334 loss： 0.131752075210168\n",
      "900 0.9583333333333334 loss： 0.131657076645151\n",
      "1000 0.9583333333333334 loss： 0.13159380336279022\n",
      "1100 0.9583333333333334 loss： 0.1315510929257448\n",
      "1200 0.9583333333333334 loss： 0.13152195635466554\n",
      "1300 0.9583333333333334 loss： 0.13150191100009023\n",
      "1400 0.9583333333333334 loss： 0.1314880257746921\n",
      "1500 0.9583333333333334 loss： 0.13147835404476624\n",
      "1600 0.9583333333333334 loss： 0.13147158650110297\n",
      "1700 0.9583333333333334 loss： 0.13146683331427164\n",
      "1800 0.9583333333333334 loss： 0.13146348455184179\n",
      "1900 0.9583333333333334 loss： 0.13146111917217865\n",
      "2000 0.9583333333333334 loss： 0.131459444817232\n",
      "2100 0.9583333333333334 loss： 0.131458257489307\n",
      "2200 0.9583333333333334 loss： 0.13145741426442703\n",
      "2300 0.9583333333333334 loss： 0.13145681466594863\n",
      "2400 0.9583333333333334 loss： 0.13145638785600428\n",
      "2500 0.9583333333333334 loss： 0.13145608377276075\n",
      "2600 0.9583333333333334 loss： 0.13145586696569275\n",
      "2700 0.9583333333333334 loss： 0.13145571228848515\n",
      "2800 0.9583333333333334 loss： 0.1314556018785046\n",
      "2900 0.9583333333333334 loss： 0.13145552303180735\n"
     ]
    }
   ],
   "source": [
    "#模型计算函数\n",
    "def forward(x, theta, bias):\n",
    "    # 线性运算\n",
    "    z = np.dot(theta, x.T) + bias # shape (105,10)\n",
    "    # sigmoid\n",
    "    y_hat = 1 / (1 + np.exp(-z))  # shape (105,10)\n",
    "    return y_hat\n",
    "\n",
    "#计算损失函数\n",
    "def loss(y,y_hat):\n",
    "    e = 1e-8\n",
    "    return -y * np.log(y_hat + e) - (1 - y) * np.log(1 - y_hat + e)\n",
    "\n",
    "#计算梯度\n",
    "def calc_gradient(x, y, y_hat):\n",
    "    #计算梯度\n",
    "    m = x.shape[-1]\n",
    "    # theta计算梯度\n",
    "    delta_theta = np.dot((y_hat - y), x) / m\n",
    "    #计算bias的梯度\n",
    "    delta_bias = np.mean(y_hat - y)\n",
    "    #返回梯度\n",
    "    return delta_theta,delta_bias\n",
    "\n",
    "#训练模型\n",
    "for i in range(epochs):\n",
    "    #向前计算\n",
    "    y_hat = forward(X_train,theta,bias)\n",
    "    #计算损失\n",
    "    loss_v = loss(y_train,y_hat)\n",
    "    #更新梯度\n",
    "    delta_theta,delta_bias = calc_gradient(X_train, y_train, y_hat)\n",
    "    #更新参数\n",
    "    theta = theta - lr * delta_theta\n",
    "    bias = bias - lr * delta_bias\n",
    "    acc = 0\n",
    "    if i % 100 == 0:\n",
    "        #计算准确率\n",
    "        acc = np.mean(np.round(y_hat) == y_train)\n",
    "        print(i,acc,\"loss：\",np.mean(loss_v))\n",
    "#保存模型数据\n",
    "modle_theta = np.save(\"theta.npy\",theta)\n",
    "modle_bias = np.save(\"bias.npy\",bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
