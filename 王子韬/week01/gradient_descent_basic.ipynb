{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降（Gradient Descent）是一种用于优化函数的算法，目标是**找到函数的最小值**。在机器学习中，我们用它来**最小化损失函数**，从而优化模型参数。\n",
    "\n",
    "1. 梯度下降依赖于微分求导\n",
    "\n",
    "梯度下降是一种优化算法，用于最小化一个函数（通常是损失函数）。它的核心思想是沿着函数梯度的负方向迭代更新参数，以找到局部最优或全局最优解。\n",
    "\n",
    "2. 梯度下降的核心操作是计算导数\n",
    "\n",
    "在梯度下降过程中，求导的作用主要体现在：\n",
    "\t•\t计算梯度方向：导数（或偏导数）告诉我们函数在当前点的变化趋势，梯度下降沿着负梯度方向移动，以减少损失。\n",
    "\t•\t确定最优更新步长：梯度的大小影响每次参数更新的幅度，若导数过大，可能导致过冲；若导数过小，下降速度慢。\n",
    "\n",
    "通常模型是多参数的，因此不能满足于在1-2个参数方向上寻找全局最优解。\n",
    "在高维空间中，局部最小值、最大值和鞍点大量存在。因此不同步长可能会导致发现局部最优解而不是全局最优解\n",
    "在标准梯度下降中，我们根据梯度来更新参数。\n",
    "\n",
    "梯度下降的三种主要形式\n",
    "\n",
    "根据计算梯度的方式，梯度下降可以分为：\n",
    "\n",
    "\t•\t批量梯度下降（Batch Gradient Descent, BGD）\n",
    "\t•\t每次计算整个训练集的梯度并更新参数。\n",
    "\t•\t优点：收敛稳定，适用于凸函数优化。\n",
    "\t•\t缺点：计算开销大，特别是数据量庞大时。\n",
    "\n",
    "\t•\t随机梯度下降（Stochastic Gradient Descent, SGD）\n",
    "\t•\t每次随机选择一个样本计算梯度并更新参数。\n",
    "\t•\t优点：计算速度快，能跳出局部最优点。\n",
    "\t•\t缺点：收敛路径震荡较大，不稳定。\n",
    "\n",
    "\t•\t小批量梯度下降（Mini-batch Gradient Descent, MBGD）\n",
    "\t•\t介于两者之间，每次使用小批量数据计算梯度。\n",
    "\t•\t优点：兼顾计算效率和稳定性，广泛应用于深度学习。\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
