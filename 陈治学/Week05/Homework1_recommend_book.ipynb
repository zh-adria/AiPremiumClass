{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推荐系统：根据输入的关键词推荐5本最相关的书籍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现基于豆瓣top250图书评论的简单推荐系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整体思路：\n",
    "1. 数据清洗并过滤掉停用词，减少计算量 cn_stopwords.txt\n",
    "2. 计算TF：每本书相当于一个“文档”，计算每本书的词频；计算总词频\n",
    "3. 计算IDF：lg(总书本量/（1+包含该词的书本量）)\n",
    "4. 计算TF-IDF，排序取前5本值最高的书：TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理：评论存在换行，造成数据没有对齐列的信息\n",
    "def process_comment_file(file_path):\n",
    "    # 读取文件并处理换行问题\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()[1:]\n",
    "    \n",
    "    # 预处理：合并被换行分割的评论\n",
    "    processed = []\n",
    "    last_record = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split('\\t')\n",
    "        # 如果满足条件就直接添加，如果不满足就添加到上一条\n",
    "        if len(parts) == 6:  # 应该有6个字段\n",
    "            if last_record:\n",
    "                processed.append(last_record)\n",
    "            last_record = parts\n",
    "        else:\n",
    "            if last_record:\n",
    "                last_record[5] += ' ' + line.strip()\n",
    "    \n",
    "    # 添加最后一条满足条件的记录\n",
    "    if last_record:\n",
    "        processed.append(last_record)\n",
    "    \n",
    "    # 转换为DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        processed,\n",
    "        columns=[\"book\", \"id\", \"star\", \"time\", \"likenum\", \"body\"]\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>id</th>\n",
       "      <th>star</th>\n",
       "      <th>time</th>\n",
       "      <th>likenum</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>96253</td>\n",
       "      <td>96253</td>\n",
       "      <td>96253</td>\n",
       "      <td>96253</td>\n",
       "      <td>96253</td>\n",
       "      <td>96253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>233</td>\n",
       "      <td>29381</td>\n",
       "      <td>6</td>\n",
       "      <td>4470</td>\n",
       "      <td>603</td>\n",
       "      <td>92139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>嫌疑人X的献身</td>\n",
       "      <td>xyws</td>\n",
       "      <td>allstar50</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>0</td>\n",
       "      <td>经典</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>970</td>\n",
       "      <td>95</td>\n",
       "      <td>34099</td>\n",
       "      <td>132</td>\n",
       "      <td>78378</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           book     id       star        time likenum   body\n",
       "count     96253  96253      96253       96253   96253  96253\n",
       "unique      233  29381          6        4470     603  92139\n",
       "top     嫌疑人X的献身   xyws  allstar50  2010-02-09       0     经典\n",
       "freq        970     95      34099         132   78378     63"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = process_comment_file(r'F:\\NLP算法课程\\正式课\\0319\\语言模型及词向量相关知识\\doubanbook_top250_comments.txt')\n",
    "comment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\czx\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.907 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 1. 预处理：合并同一本书的所有评论\n",
    "book_contents = comment.groupby('book')['body'].apply(lambda x: ' '.join(x)).to_dict()\n",
    "\n",
    "# 停用词表\n",
    "with open(r'F:\\NLP算法课程\\正式课\\0319\\语言模型及词向量相关知识\\cn_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(line.strip() for line in f)\n",
    "\n",
    "# 2. 计算每本书每个词的词频（TF）\n",
    "tf_dict = {}\n",
    "for book, content in book_contents.items():\n",
    "    words = jieba.lcut(content) # 得到分完词的list\n",
    "    # 添加停用词和中文过滤\n",
    "    filtered = [w for w in words \n",
    "               if w not in stopwords \n",
    "               and len(w) > 1  # 新增长度过滤\n",
    "               and '\\u4e00' <= w <= '\\u9fff']  # 仅保留中文\n",
    "    word_count = Counter(filtered)\n",
    "    total_words = sum(word_count.values())\n",
    "    tf_dict[book] = {word: count/total_words for word, count in word_count.items()}\n",
    "\n",
    "# 3. 计算IDF\n",
    "df_count = defaultdict(int) #遇到新词自动设置为0\n",
    "total_books = len(book_contents)\n",
    "\n",
    "for book in tf_dict:\n",
    "    for word in tf_dict[book]:\n",
    "        df_count[word] += 1\n",
    "\n",
    "idf_dict = {word: math.log(total_books / (1 + count)) for word, count in df_count.items()}\n",
    "\n",
    "# 4. 计算TF-IDF并推荐书籍\n",
    "def recommend_by_tfidf(keyword):\n",
    "    book_scores = []\n",
    "    for book in tf_dict:\n",
    "        tf = tf_dict[book].get(keyword, 0)\n",
    "        idf = idf_dict.get(keyword, 0)\n",
    "        book_scores.append((book, tf * idf))\n",
    "    \n",
    "    return sorted(book_scores, key=lambda x: x[1], reverse=True)[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关键词：推荐，基于TF-IDF的推荐结果：\n",
      "1995-2005夏至未至（TF-IDF值：-0.0000）\n",
      "1Q84 BOOK 1（TF-IDF值：-0.0000）\n",
      "1Q84 BOOK 3（TF-IDF值：-0.0000）\n",
      "一个陌生女人的来信（TF-IDF值：-0.0000）\n",
      "一個人住第5年（TF-IDF值：-0.0000）\n"
     ]
    }
   ],
   "source": [
    "#使用示例\n",
    "key_word = input('请输入关键词')\n",
    "recommendations = recommend_by_tfidf(key_word)\n",
    "print(f\"关键词：{key_word}，基于TF-IDF的推荐结果：\")\n",
    "for book, score in recommendations:\n",
    "    print(f\"{book}（TF-IDF值：{score:.4f}）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "心得：dict真好用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置超参数\n",
    "k = 1.5\n",
    "b = 0.75\n",
    "# 计算BM25得分并推荐书籍\n",
    "doc_lengths = {book: sum(word_count.values()) for book, word_count in tf_dict.items()} #总词数量化文档长度\n",
    "avg_dl = sum(doc_lengths.values()) / len(doc_lengths) #平均文档长度\n",
    "\n",
    "# 计算IDF\n",
    "df_count = defaultdict(int)\n",
    "total_books = len(book_contents)\n",
    "for book in tf_dict:\n",
    "    for word in tf_dict[book]:\n",
    "        df_count[word] += 1\n",
    "\n",
    "# 新公式：log[(N - n_t + 0.5) / (n_t + 0.5)]\n",
    "idf_dict = {word: math.log( (total_books - count + 0.5) / (count + 0.5) ) \n",
    "            for word, count in df_count.items()}\n",
    "\n",
    "def recommend_by_BM25(keyword):\n",
    "    book_scores = []\n",
    "    idf = idf_dict.get(keyword, 0)\n",
    "    for book in tf_dict:\n",
    "        tf = tf_dict[book].get(keyword, 0)\n",
    "        dl = doc_lengths[book]\n",
    "        \n",
    "        # BM25计算公式\n",
    "        numerator = tf * (k + 1)\n",
    "        denominator = tf + k * (1 - b + b * (dl / avg_dl))\n",
    "        bm25_score = idf * (numerator / (denominator + 1e-8))  # 防止除零\n",
    "        \n",
    "        book_scores.append((book, bm25_score))\n",
    "    \n",
    "    return sorted(book_scores, key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1995-2005夏至未至', -0.0),\n",
       " ('1Q84 BOOK 1', -0.0),\n",
       " ('1Q84 BOOK 3', -0.0),\n",
       " ('一个陌生女人的来信', -0.0),\n",
       " ('一個人住第5年', -0.0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_recommendations = recommend_by_BM25(key_word)\n",
    "bm25_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_prj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
