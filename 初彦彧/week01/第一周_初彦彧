import numpy as np

arr = np.array([1, 2, 3, 4, 5])
arr
arr = np.array([[1, 2, 3]] , float)
arr

a = np.array([(1,2,3) , (4,5,6) , (7,8,9)])
a

a = np.array([(1,2,3) , (4,5,6) , (7,8,9)] , dtype = np.float64)
a

a = np.array([(1,2,3) , (4,5,6) , (7,8,9)] , dtype = np.complex64)
a

a1 = np.zeros((2,3) , dtype = float)
a1

a1 = np.zeros((2,3) , dtype = int)
a1

a1 = np.zeros((2,3) , dtype = np.complex64)
a1

a = np.ones((3,3))
a

a = np.ones((3,3) , dtype = int)
a

a2 = np.arange(1 , 6 , 0.3)
a2

a3 = np.eye(3)
a3   #one-hot编码

a4 = np.random.random(5)   # 模型参数初始化
a4

mu , sigma = 0 , 0.1
a5 = np.random.normal(mu , sigma , 5)   # 模型参数初始化
a5

a6 = np.array([(1,2) , (3,4) , (5,6)])
print(a6[: , 1])

a6 = np.array([(1,2) , (3,4) , (5,6)])
print(a6[1:])

a6 = np.array([(1,2) , (3,4) , (5,6)])
print(a6[1][1])

a7 = np.array([(1,2) , (3,4) , (5,6)])
i , j = a7[0]
print(i , j)

a7 = np.array([(1,2) , (3,4) , (5,6)])
for i , j in a7:
    print(i , j)

a = np.array([(1,2,3), (4,5,6), (7,8,9)])
print("ndim:", a.ndim)
print("shape:", a.shape)
print("size", a.size)
print("dtype", a.dtype)

if 3 in a:
    print("3 in a")

print(3 in a)
print((1,2,3) in a)

a = np.zeros((2,3,4))
a

a7 = np.arange(1 , 10)
print(a7)
print(a7.shape)

a7 = a7.reshape((3,3))   # 维度大小的乘积 == 元素个数
print(a7)
print(a7.shape)

a7 = a7.reshape((3,3,1))   # 维度大小的乘积 == 元素个数
print(a7)
print(a7.shape)   # 高维矩阵，每个维度都有含义

# 加载图像数据
# img.shape [1,3,120,120]  1个样本，3颜色特征通道，120*120像素

a = np.array([(1,2,3), (4,5,6), (7,8,9)])
print(a)

a = a.transpose()
print(a)

a = a.T
print(a)

a = a.flatten()
print(a)

a8 = np.array([(1,2) , (3,4) , (5,6)])
a8 = a8[: , np.newaxis , :]
a8.shape

a = np.ones((2,2))
b = np.array([(-1,1),(-1,1)])
print(a)
print(b)
print()
print(a+b)
print()
print(a-b)

a.sum()
a.prod()

# 数组，矩阵，维度
# 数组，矩阵：数据组织的结构形式
# 维度：通讯地址

# 平均数，⽅差，标准差，最⼤值，最⼩值
a = np.array([5,3,1])
print("mean:",a.mean())
print("var:", a.var())
print("std:", a.std())
print("max:", a.max())
print("min:", a.min())

# 最⼤与最⼩值对应的索引值：argmax,argmin:
# 取元素值上限，下限，四舍五⼊：ceil, floor, rint
a = np.array([1.2, 3.8, 4.9])
print("argmax:", a.argmax())
print("argmin:", a.argmin())
print("ceil:", np.ceil(a))
print("floor:", np.floor(a))
print("rint:", np.rint(a))

a = np.array([16,31,12,28,22,31,48])
a.sort() # 内部排序
a

import numpy as np
# 定义两个简单的矩阵
m1 = np.array([[1, 2], [3, 4]] , dtype=np.float32)
m2 = np.array([[5, 6], [7, 8]] , dtype=np.float32)
# 使⽤ np.dot 进⾏矩阵乘法
result_dot = np.dot(m1, m2)
# 使⽤ @ 运算符进⾏矩阵乘法
result_at = m1 @ m2
print("矩阵 1:")
print(m1)
print("矩阵 2:")
print(m2)
print("使⽤ np.dot 得到的矩阵乘法结果:")
print(result_dot)

print("使⽤ @ 运算符得到的矩阵乘法结果:")
print(result_at)
# 创建⼀个全零矩阵，⽤于存储⼿动推演的结果
# 结果矩阵的⾏数等于 matrix1 的⾏数，列数等于 matrix2 的列数
manual_result = np.zeros((m1.shape[0], m2.shape[1]), dtype=np.float32)
# 外层循环：遍历 matrix1 的每⼀⾏
# i 表⽰结果矩阵的⾏索引
for i in range(m1.shape[0]):
 # 中层循环：遍历 matrix2 的每⼀列
 # j 表⽰结果矩阵的列索引
 for j in range(m2.shape[1]):
 # 初始化当前位置的结果为 0
    manual_result[i, j] = 0
 # 内层循环：计算 matrix1 的第 i ⾏与 matrix2 的第 j 列对应元素的乘积之和
 # k 表⽰参与乘法运算的元素索引
 for k in range(m1.shape[1]):
 # 打印当前正在计算的元素
    print(f"{m1[i, k]} * {m2[k, j]} = {m1[i, k] * m2[k, j]}")
 # 将 matrix1 的第 i ⾏第 k 列元素与 matrix2 的第 k ⾏第 j 列元素相乘，并累
 manual_result[i, j] += m1[i, k] * m2[k, j]
 # 打印当前位置计算完成后的结果
 print(f"结果矩阵[{i+1},{j+1}]:{manual_result[i, j]}\n")
print("⼿动推演结果:")
print(manual_result)

a9 = np.array([[1, 2 ,3], [4, 5, 6]])
b9 = np.array([(4, 5, 6), (7, 8, 9)])

np.save('result.npy', manual_result)

result_np = np.load('result.npy')
result_np

a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
a + b

a = np.array([(1,2), (2,2), (3,3), (4,4)]) # shape(4,2)
b = np.array([-1,1]) # shape(2)-> shape(1,2) -> shape(4,2) [ -1,1],[-1,1],[-1,1],[-1
a + b

import torch
data = torch.tensor([[1, 2] , [3, 4]])
data

np_array = np.array([[1, 2] , [3, 4]])
data2 = torch.from_numpy(np_array)
data2

# 通过已知的张量维度，创建新张量
data3 = torch.ones_like(data2)
data3

data3 = torch.zeros_like(data2)
data3

data3 = torch.rand_like(data2 , dtype=torch.float32)
data3

shape = (2,3,)
rand_tensor = torch.rand(shape)
ones_tensor = torch.ones(shape)
zeros_tensor = torch.zeros(shape)

print(f"Random Tensor: \n {rand_tensor} \n")
print(f"Ones Tensor: \n {ones_tensor} \n")
print(f"Zeros Tensor: \n {zeros_tensor}")

# 基于现有tensor构建，但使⽤新值填充
m = torch.ones(5,3, dtype=torch.double)
n = torch.rand_like(m, dtype=torch.float)
# 获取tensor的⼤⼩
print(m.size()) # torch.Size([5,3])
print()
# 均匀分布
print(torch.rand(5,3))
print()
# 标准正态分布
print(torch.randn(5,3))
print()
# 离散正态分布
print(torch.normal(mean=.0,std=1.0,size=(5,3)))
print()
# 线性间隔向量(返回⼀个1维张量，包含在区间start和end上均匀间隔的steps个点)
print(torch.linspace(start=1,end=10,steps=21))

tensor = torch.rand(3,4)
print(f"Shape of tensor: {tensor.shape}")
print(f"Datatype of tensor: {tensor.dtype}")
print(f"Device tensor is stored on: {tensor.device}")

# 检查 pytorch 是否支持CPU
print(torch.cuda.is_available())

tensor = torch.ones(4, 4)
print('First row: ', tensor[0])
print('First column: ', tensor[:, 0])
print('Last column:', tensor[..., -1])
tensor[:,1] = 0
print(tensor)

tensor = torch.zeros(4, 4)
print('First row: ', tensor[0])
print('First column: ', tensor[:, 0])
print('Last column:', tensor[..., -1])
tensor[:,1] = 1
print(tensor)

