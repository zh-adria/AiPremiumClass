{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量创建及基础属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张量： tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "形状shape： torch.Size([2, 2])\n",
      "维度ndim： 2\n",
      "张量元素类型： torch.float32\n",
      "张量类型： <class 'torch.Tensor'>\n",
      "张量的大小： torch.Size([2, 2])\n",
      "False\n",
      "张量 device： cpu\n"
     ]
    }
   ],
   "source": [
    "# 张量创建及基础属性\n",
    "# mac m芯片设置device mps\n",
    "# tensor_data = torch.tensor([[1,2],[3,4]], dtype=torch.float32,device=\"mps:0\")\n",
    "tensor_data = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "print(\"张量：\", tensor_data)\n",
    "print(\"形状shape：\", tensor_data.shape)\n",
    "print(\"维度ndim：\", tensor_data.ndim)\n",
    "print(\"张量元素类型：\", tensor_data.dtype)\n",
    "print(\"张量类型：\", type(tensor_data))\n",
    "print(\"张量的大小：\", tensor_data.size())\n",
    "\n",
    "# 检测是否支持GPU\n",
    "print(torch.cuda.is_available())\n",
    "print(\"张量 device：\", tensor_data.device)\n",
    "\n",
    "# 设置张量在GPU上运算\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor_data.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# gpu_check\n",
    "# mac m1\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 英伟达GPU\n",
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量 inner_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_data tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "zeros_data: tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "rand_data: tensor([[0.1062, 0.0055, 0.6118],\n",
      "        [0.2911, 0.2108, 0.7279],\n",
      "        [0.3048, 0.1158, 0.1178],\n",
      "        [0.3861, 0.0075, 0.7253]])\n",
      "zeros_like_data: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "ones_like: tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "rand_tensor: {tensor([[0.4198, 0.5538, 0.9499],\n",
      "        [0.2020, 0.5514, 0.2098]])}\n",
      "ones_tensor: {tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])}\n",
      "zeros_tensor: {tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])}\n",
      "tensor([[0.7313, 0.7580, 0.1760, 0.4285, 0.2038],\n",
      "        [0.2887, 0.4110, 0.5904, 0.0834, 0.9666],\n",
      "        [0.6025, 0.2957, 0.6950, 0.2874, 0.5752],\n",
      "        [0.0681, 0.6712, 0.1162, 0.5863, 0.3596],\n",
      "        [0.9188, 0.2104, 0.4927, 0.1536, 0.3140],\n",
      "        [0.3622, 0.4018, 0.8119, 0.2579, 0.5102]])\n",
      "tensor([[ 0.2518, -0.0799, -0.4877,  1.9289,  1.3142],\n",
      "        [-1.2557, -1.2761,  1.5569,  0.2042, -1.2636],\n",
      "        [ 0.8817,  0.1045, -0.4667,  0.7233, -0.5095],\n",
      "        [ 0.3875,  0.5345, -1.3731,  1.1772, -0.7101],\n",
      "        [ 0.8774,  0.6655, -0.0115,  0.6248, -2.0445],\n",
      "        [ 0.3354,  0.6020,  1.8114,  0.3793, -0.9231]])\n",
      "tensor([[-0.3784, -0.2435,  0.3994, -0.0634, -1.5251],\n",
      "        [-1.4687, -0.4265, -0.5772,  1.7540,  0.8478],\n",
      "        [-0.0614,  0.9234,  1.2596,  1.0883, -0.7482],\n",
      "        [ 0.0668, -0.3279,  1.4113, -0.0774,  1.8653],\n",
      "        [ 0.0387, -1.5066,  0.3574, -1.0697, -0.4688],\n",
      "        [ 1.6545, -0.2468,  0.5150, -0.0487,  0.7539]])\n",
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "tensor([1, 2, 3]) \n",
      "[1:]: tensor([2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]]) \n",
      "[1]: tensor([1, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]]) \n",
      "[1][1]: tensor(5)\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]]) \n",
      "[1,][1,]: tensor([3, 6, 9])\n"
     ]
    }
   ],
   "source": [
    "# torch.ones\n",
    "tensor_data = torch.ones(3, 3, dtype=torch.int32)\n",
    "print(\"tensor_data\", tensor_data)\n",
    "\n",
    "# torch.zeros\n",
    "zeros_data = torch.zeros(3, 4, dtype=torch.float32)\n",
    "print(\"zeros_data:\", zeros_data)\n",
    "\n",
    "# zero_like\n",
    "rand_data = torch.rand(4, 3)\n",
    "print(\"rand_data:\", rand_data)\n",
    "zeros_like_data = torch.zeros_like(rand_data)\n",
    "print(\"zeros_like_data:\", zeros_like_data)\n",
    "\n",
    "# ones_like\n",
    "ones_like_data = torch.ones_like(rand_data, dtype=torch.int32)\n",
    "print(\"ones_like:\", ones_like_data)\n",
    "\n",
    "# 使用张量维度元组\n",
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "print(\"rand_tensor:\", {rand_tensor})\n",
    "ones_tensor = torch.ones(shape)\n",
    "print(\"ones_tensor:\", {ones_tensor})\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(\"zeros_tensor:\", {zeros_tensor})\n",
    "\n",
    "# 均匀分布\n",
    "print(torch.rand(6, 5))\n",
    "# 标准正态分布\n",
    "print(torch.randn(6, 5))\n",
    "# 离散正态分布\n",
    "print(torch.normal(mean=.0, std=1.0, size=(6, 5)))\n",
    "# 线性间隔向量(返回⼀个1维张量，包含在区间start和end上均匀间隔的steps个点)\n",
    "print(torch.linspace(start=1, end=10, steps=10, dtype=torch.float32))\n",
    "\n",
    "tensor_1 = torch.tensor([1, 2, 3])\n",
    "print(tensor_1, \"\\n[1:]:\", tensor_1[1:])\n",
    "\n",
    "tensor_2 = torch.tensor([(1, 2, 3), (4, 5, 6), (7, 8, 9)])\n",
    "print(tensor_2, \"\\n[1]:\", tensor_2[0])\n",
    "print(tensor_2, \"\\n[1][1]:\", tensor_2[1][1])\n",
    "print(tensor_2, \"\\n[1,][1,]:\", tensor_2[..., 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量拼接 tensor_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
      "        [4, 5, 6, 4, 5, 6, 4, 5, 6],\n",
      "        [7, 8, 9, 7, 8, 9, 7, 8, 9]])\n",
      "tensor([1, 2, 3, 1, 2, 3, 1, 2, 3])\n",
      "tensor([[[1, 1, 1],\n",
      "         [2, 2, 2],\n",
      "         [3, 3, 3]],\n",
      "\n",
      "        [[4, 4, 4],\n",
      "         [5, 5, 5],\n",
      "         [6, 6, 6]],\n",
      "\n",
      "        [[7, 7, 7],\n",
      "         [8, 8, 8],\n",
      "         [9, 9, 9]]])\n",
      "tensor([[1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "# torch.cat 横向拼接\n",
    "tensor_1 = torch.tensor([(1, 2, 3), (4, 5, 6), (7, 8, 9)])\n",
    "t1 = torch.cat([tensor_1, tensor_1, tensor_1], dim=1)\n",
    "print(t1)\n",
    "\n",
    "tensor_2 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.cat([tensor_2, tensor_2, tensor_2])\n",
    "print(t2)\n",
    "\n",
    "# torch.stack 纵向拼接\n",
    "tensor_1 = torch.tensor([(1, 2, 3), (4, 5, 6), (7, 8, 9)])\n",
    "t1 = torch.stack([tensor_1, tensor_1, tensor_1], dim=2)\n",
    "print(t1)\n",
    "\n",
    "tensor_2 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.stack([tensor_2, tensor_2, tensor_2])\n",
    "print(t2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量算术操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张量相加+: tensor([[ 2,  4,  6],\n",
      "        [ 8, 10, 12]])\n",
      "张量相加add: tensor([[ 2,  4,  6],\n",
      "        [ 8, 10, 12]])\n",
      "---\n",
      "张量相减-: tensor([[0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "张量相减sub: tensor([[0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "---\n",
      "张量相乘×: tensor([[ 1,  4,  9],\n",
      "        [16, 25, 36]])\n",
      "张量相乘mul: tensor([[ 1,  4,  9],\n",
      "        [16, 25, 36]])\n",
      "---\n",
      "张量相除÷: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "张量相除div: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "---\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "matmul: tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "---\n",
      "tensor_2.T tensor([[5, 7],\n",
      "        [6, 8]])\n",
      "matmul: tensor([[17, 23],\n",
      "        [39, 53]])\n"
     ]
    }
   ],
   "source": [
    "tensor_1 = torch.tensor([(1, 2, 3), (4, 5, 6)])\n",
    "tensor_2 = torch.tensor([(1, 2, 3), (4, 5, 6)])\n",
    "\n",
    "result = tensor_1 + tensor_2\n",
    "print(\"张量相加+:\", result)\n",
    "print(\"张量相加add:\", tensor_1.add(tensor_2))\n",
    "\"\"\"\n",
    " tensor([[ 2,  4,  6],\n",
    "         [ 8, 10, 12]])\n",
    " \"\"\"\n",
    "print(\"---\")\n",
    "\n",
    "result = tensor_1 - tensor_2\n",
    "print(\"张量相减-:\", result)\n",
    "print(\"张量相减sub:\", tensor_1.sub(tensor_2))\n",
    "\"\"\"\n",
    " tensor([[0, 0, 0],\n",
    "         [0, 0, 0]])\n",
    "\n",
    " \"\"\"\n",
    "print(\"---\")\n",
    "\n",
    "result = tensor_1 * tensor_2\n",
    "print(\"张量相乘×:\", result)\n",
    "print(\"张量相乘mul:\", tensor_1.mul(tensor_2))\n",
    "\"\"\"\n",
    " tensor([[1, 4, 9],\n",
    "         [16, 25, 36]])\n",
    " \"\"\"\n",
    "print(\"---\")\n",
    "\n",
    "result = tensor_1 / tensor_2\n",
    "print(\"张量相除÷:\", result)\n",
    "print(\"张量相除div:\", tensor_1.div(tensor_2))\n",
    "\"\"\"\n",
    " tensor([[1., 1., 1.],\n",
    "         [1., 1., 1.]])\n",
    " \"\"\"\n",
    "print(\"---\")\n",
    "# 张量乘积\n",
    "tensor_1 = torch.tensor([(1, 2), (3, 4)])\n",
    "tensor_2 = torch.tensor([(5, 6), (7, 8)])\n",
    "#  mat1 and mat2 shapes cannot be multiplied (2x3 and 2x3)\n",
    "result = tensor_1 @ tensor_2\n",
    "\n",
    "\"\"\"\n",
    " tensor_1=a\n",
    " tensor_2=b\n",
    "\n",
    " a11 * b11 + a12 * b21 = 1*5+2*7=19\n",
    " a11 * b12 + a12 * b22 = 1*6+2*8=22\n",
    " a21 * b11 + a22 * b21 = 3*5+4*7=43\n",
    " a21 * b12 + a22 * b22 = 3*6+4*8=50\n",
    "\n",
    " [[19 22]\n",
    "  [43 50]]\n",
    " \"\"\"\n",
    "\n",
    "print(result)\n",
    "print(\"matmul:\", tensor_1.matmul(tensor_2))\n",
    "print(\"---\")\n",
    "print(\"tensor_2.T\", tensor_2.T)\n",
    "print(\"matmul:\", tensor_1.matmul(tensor_2.T))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量点积运算matmul_operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape = (n, n)\n",
      " tensor([[ 1.0187,  0.2180,  0.0788],\n",
      "        [-3.5104,  0.4484,  1.7732],\n",
      "        [-1.7275,  1.0122,  1.8561]])\n",
      "(n,) 和 (n,)->()\n",
      " tensor(-1.1169)\n",
      "(b, n, m) 和 (m, p)->(b, n, p)\n",
      " tensor([[[-3.3895,  0.2287, -3.8240, -1.3790,  3.2079],\n",
      "         [-2.3341,  0.5266, -2.9751, -0.7259, -1.4459],\n",
      "         [-3.4885, -3.2068, -3.0551, -1.8515,  4.9465]],\n",
      "\n",
      "        [[-1.2576, -0.7226, -0.6934, -0.8730,  1.0366],\n",
      "         [ 1.9472,  0.6652,  0.1430,  1.8569, -2.9506],\n",
      "         [ 2.0295, -1.1105,  1.1411,  1.4524, -3.4229]]])\n",
      "tensor([[[-0.1066, -1.7289,  0.7936, -2.7563],\n",
      "         [-3.4221, -0.0489, -3.3897,  1.5342],\n",
      "         [ 0.8609,  1.5287,  0.2507,  2.1559]],\n",
      "\n",
      "        [[-3.5977, -0.0588, -2.8756,  1.6796],\n",
      "         [ 0.4532,  2.2091, -0.3632,  2.3016],\n",
      "         [-0.6149,  1.9719,  0.2055,  2.8289]]])\n",
      "(n, m) 和 (m,)->(n,)\n",
      " tensor([2.9674, 1.3146, 1.7601])\n",
      "(b, n, m) 和 (b, m, p)->(b, n, p)\n",
      " tensor([[[-4.1905, -1.9987, -1.9771, -0.6651,  1.7034],\n",
      "         [-0.8600, -0.8620, -0.9890, -3.1519,  0.5358],\n",
      "         [ 0.5894,  0.2526,  0.6232, -2.2356,  0.4833]],\n",
      "\n",
      "        [[ 1.4862, -0.3942, -1.9679,  0.7100, -0.3225],\n",
      "         [ 0.7003, -1.8610, -1.2284, -1.3106, -2.7603],\n",
      "         [-1.0147,  1.4219,  1.5193, -1.5139, -0.1949]]])\n",
      "tensor([[11, 21],\n",
      "        [12, 22],\n",
      "        [13, 23]])\n"
     ]
    }
   ],
   "source": [
    "# Shape = (n, n)\n",
    "tensor_a = torch.randn(3, 3)  # 形状 (3, 3)\n",
    "tensor_b = torch.randn(3, 3)  # 形状 (3, 3)\n",
    "tensor_c = torch.matmul(tensor_a, tensor_b)  # 形状 (3, 3)\n",
    "print(\"Shape = (n, n)\\n\", tensor_c)\n",
    "\n",
    "# 一维向量点积 (n,) 和 (n,)->()\n",
    "tensor_a = torch.randn(4)  # 形状 (4,)\n",
    "tensor_b = torch.randn(4)  # 形状 (4,)\n",
    "tensor_c = torch.matmul(tensor_a, tensor_b)  # 形状 ()\n",
    "print(\"(n,) 和 (n,)->()\\n\", tensor_c)\n",
    "\n",
    "# 广播机制(b, n, m) 和 (m, p)->(b, n, p)\n",
    "tensor_a = torch.randn(2, 3, 4)  # 形状 (2, 3, 4)\n",
    "tensor_b = torch.randn(4, 5)  # 形状 (4, 5)\n",
    "tensor_c = torch.matmul(tensor_a, tensor_b)  # 形状 (2, 3, 5)\n",
    "print(\"(b, n, m) 和 (m, p)->(b, n, p)\\n\", tensor_c)\n",
    "\n",
    "tensor_a = torch.randn(2, 3, 4)  # 形状 (2, 3, 4)\n",
    "tensor_b = torch.randn(4, 4)  # 形状 (2, 4)\n",
    "tensor_c = torch.matmul(tensor_a, tensor_b)  # 形状 (2, 3)\n",
    "print(tensor_c)\n",
    "\n",
    "# 矩阵与向量相乘(n, m) 和 (m,)->(n,)\n",
    "tensor_a = torch.randn(3, 4)  # 形状 (3, 4)\n",
    "tensor_b = torch.randn(4)  # 形状 (4,)\n",
    "tensor_c = torch.matmul(tensor_a, tensor_b)  # 形状 (3,)\n",
    "print(\"(n, m) 和 (m,)->(n,)\\n\", tensor_c)\n",
    "\n",
    "# (b, n, m) 和 (b, m, p)->(b, n, p)\n",
    "tensor_a = torch.randn(2, 3, 4)  # 形状 (2, 3, 4)\n",
    "tensor_b = torch.randn(2, 4, 5)  # 形状 (2, 4, 5)\n",
    "tensor_c = torch.matmul(tensor_a, tensor_b)  # 形状 (2, 3, 5)\n",
    "print(\"(b, n, m) 和 (b, m, p)->(b, n, p)\\n\", tensor_c)\n",
    "\n",
    "tensor_a = torch.tensor([[1], [2], [3]])  # 形状(1,3)\n",
    "tensor_b = torch.tensor([10, 20])  # 形状(2,2)\n",
    "print(tensor_a + tensor_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor_numpy_switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "arr: [1. 1. 1. 1. 1.]\n",
      "arr type: <class 'numpy.ndarray'>\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "tensor type: <class 'torch.Tensor'>\n",
      "tensor([[123., 456.],\n",
      "        [  1.,   2.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# tensor to ndarray\n",
    "tensor = torch.ones(5)\n",
    "print(tensor)\n",
    "arr = tensor.numpy()\n",
    "print(\"arr:\", arr)\n",
    "print(\"arr type:\", type(arr))\n",
    "\n",
    "# ndarray to tensor\n",
    "arr = np.ones(5)\n",
    "print(arr)\n",
    "tensor = torch.from_numpy(arr)\n",
    "print(\"tensor:\", tensor)\n",
    "print(\"tensor type:\", type(tensor))\n",
    "\n",
    "# torch.from_numpy 张量可以从 NumPy 数组创建\n",
    "arr = np.array([(123, 456), (1, 2)], dtype=float)\n",
    "tensor_data = torch.from_numpy(arr)\n",
    "print(tensor_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
