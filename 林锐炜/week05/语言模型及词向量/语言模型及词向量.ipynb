{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、实现基于豆瓣top250图书评论的简单推荐系统（TF-IDF及BM25两种算法实现）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98175\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# 数据清洗\n",
    "file_path = \"doubanbook_top250_comments.txt\"\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileExistsError(f\"can not found file {file_path}\")\n",
    "\n",
    "infos_list=[]\n",
    "with open(file=file_path,mode='r',encoding='utf-8') as fs:\n",
    "    csv_datas = csv.DictReader(fs,delimiter=\"\\t\")\n",
    "    for row in csv_datas:\n",
    "        infos_list.append(row)\n",
    "\n",
    "print(len(infos_list))\n",
    "\n",
    "# 将txt转为csv\n",
    "df = pd.DataFrame(infos_list)\n",
    "df.to_csv(\"doubanbook_top250_comments.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理book评论信息，依赖stopword进行拆分\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "csv_file = \"doubanbook_top250_comments.csv\"\n",
    "stop_words_path = \"stopwords_full.txt\"\n",
    "\n",
    "if not os.path.exists(csv_file):\n",
    "    raise FileExistsError(f\"can not found file {csv_file}\")\n",
    "\n",
    "if not os.path.exists(stop_words_path):\n",
    "    raise FileExistsError(f\"can not found file {stop_words_path}\")\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "df['body'] = df['body'].fillna('')\n",
    "book_infos = df.to_dict('records')\n",
    "\n",
    "book_comments = {}\n",
    "for book_info in book_infos:\n",
    "    book = book_info['book']\n",
    "    if book == '' : continue\n",
    "\n",
    "    comment = book_info['body']\n",
    "    # print(comment)\n",
    "    comment_words = jieba.lcut(comment)\n",
    "\n",
    "    book_comments[book] = book_comments.get(book,[])\n",
    "    book_comments[book].extend(comment_words)\n",
    "\n",
    "stop_words_ = []\n",
    "with open(file=stop_words_path,mode='r',encoding='utf-8') as sw_fs:\n",
    "    stop_words_ = [line.strip() for line in sw_fs if line.strip()]\n",
    "\n",
    "book_names=list(book_comments.keys())\n",
    "book_cts = list(book_comments.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py311/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['articl', 'daren', 'itse', 'lex', 'mayn', 'mon', 'myse', 'oughtn', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《鬼吹灯之精绝古城》 相似度：「0.6717」\n",
      "《鬼吹灯之云南虫谷》 相似度：「0.5673」\n",
      "《盗墓笔记3》 相似度：「0.5351」\n",
      "《盗墓笔记4》 相似度：「0.4690」\n",
      "《盗墓笔记2》 相似度：「0.4114」\n",
      "《达·芬奇密码》 相似度：「0.2869」\n",
      "《西决》 相似度：「0.2629」\n",
      "《何以笙箫默》 相似度：「0.2589」\n",
      "《华胥引（全二册）》 相似度：「0.2448」\n",
      "《穆斯林的葬礼》 相似度：「0.2384」\n"
     ]
    }
   ],
   "source": [
    "# 计算TF-IDF并通过计算余弦相似度\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 构建TF-IDF特征矩阵\n",
    "vectorize = TfidfVectorizer(stop_words=stop_words_)\n",
    "tfidf_matrix = vectorize.fit_transform([' '.join(comments_) for comments_ in book_cts])\n",
    "\n",
    "# 计算图书之间余弦相似度\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "input_book_name =\"盗墓笔记\"\n",
    "\n",
    "if input_book_name not in book_names:\n",
    "    raise ValueError(\"input book name not found.\")\n",
    "book_idx = book_names.index(input_book_name)\n",
    "\n",
    "# 获取与输入图书最相似的图书\n",
    "recomment_book_indexs = np.argsort(-similarity_matrix[book_idx])[1:11]\n",
    "\n",
    "# 输出推荐书籍\n",
    "for idx in recomment_book_indexs:\n",
    "    print(f\"《{book_names[idx]}》 相似度：「{similarity_matrix[book_idx][idx]:.4f}」\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《鬼吹灯之精绝古城》 相似度：「0.6717」\n",
      "《鬼吹灯之云南虫谷》 相似度：「0.5673」\n",
      "《盗墓笔记3》 相似度：「0.5351」\n",
      "《盗墓笔记4》 相似度：「0.4690」\n",
      "《盗墓笔记2》 相似度：「0.4114」\n",
      "《达·芬奇密码》 相似度：「0.2869」\n",
      "《西决》 相似度：「0.2629」\n",
      "《何以笙箫默》 相似度：「0.2589」\n",
      "《华胥引（全二册）》 相似度：「0.2448」\n",
      "《穆斯林的葬礼》 相似度：「0.2384」\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "preprocessed_corpus = []  # 存储过滤停用词后的所有书评\n",
    "for words in book_comments.values():\n",
    "    filtered_words = [word for word in words if word not in stop_words_ and word.strip() != '']\n",
    "    preprocessed_corpus.append(filtered_words)\n",
    "\n",
    "bm25 = BM25Okapi(preprocessed_corpus)  # 初始化 BM25 模型\n",
    "\n",
    "input_book_name =\"盗墓笔记\"\n",
    "\n",
    "if input_book_name not in book_names:\n",
    "    raise ValueError(\"input book name not found.\")\n",
    "book_idx = book_names.index(input_book_name)\n",
    "\n",
    "query = preprocessed_corpus[book_idx]  # 获取这本书的词列表\n",
    "scores = bm25.get_scores(query)  # 计算相似度分数\n",
    "\n",
    "sorted_indices = np.argsort(scores)[::-1]  # 按相似度降序排序\n",
    "recommended_indices = [idx for idx in sorted_indices if idx != book_idx][1:11] \n",
    "\n",
    "# 输出推荐书籍\n",
    "for idx in recomment_book_indexs:\n",
    "    print(f\"《{book_names[idx]}》 相似度：「{similarity_matrix[book_idx][idx]:.4f}」\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、使用自定义的文档文本，通过fasttext训练word2vec训练词向量模型，并计算词汇间的相关度。（选做：尝试tensorboard绘制词向量可视化图）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "data_src_path = 'data_src.txt'\n",
    "data_path = 'data.txt'\n",
    "\n",
    "if not os.path.exists(data_src_path):\n",
    "    raise FileExistsError(f\"can not found file {data_src_path}\")\n",
    "\n",
    "texts = []\n",
    "with open(file=data_src_path,mode='r',encoding='utf-8') as fs:\n",
    "    for line in fs:\n",
    "        words = jieba.lcut(re.sub(r'[^\\w\\s]', '', line.strip()))\n",
    "        texts.append(' '.join(words))\n",
    "\n",
    "with open(file=data_path, mode='w', encoding='utf-8') as fs:\n",
    "    for text in texts:\n",
    "        fs.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  2\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    6136 lr:  0.000000 avg.loss:  4.108504 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "科技 vs 创新: -0.0538\n",
      "手机 vs 电脑: 0.0140\n",
      "艺术 vs 科学: 0.0640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fasttext_vis/embedding.ckpt-1'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "data_path = 'data.txt'\n",
    "word_pairs = [(\"科技\", \"创新\"), (\"手机\", \"电脑\"), (\"艺术\", \"科学\")]\n",
    "\n",
    "# 无监督学习,使用skipgram（跳字模型）模型\n",
    "model_skipgram = fasttext.train_unsupervised(\n",
    "    input=data_path,\n",
    "    model='skipgram',\n",
    "    dim=100,ws=5,\n",
    "    minCount=5,\n",
    "    epoch=50)\n",
    "\n",
    "# 使用cbow（连续词袋模型）\n",
    "# model_cbow = fasttext.train_unsupervised('data.txt',model='cbow',dim=100,ws=5,minCount=5,epoch=50)\n",
    "\n",
    "model = model_skipgram\n",
    "\n",
    "model.save_model('fasttext_model.bin')\n",
    "\n",
    "# 计算词语相似度\n",
    "for w1, w2 in word_pairs:\n",
    "    vec1 = model.get_word_vector(w1).reshape(1, -1)\n",
    "    vec2 = model.get_word_vector(w2).reshape(1, -1)\n",
    "    sim = cosine_similarity(vec1, vec2)[0][0]\n",
    "    print(f\"{w1} vs {w2}: {sim:.4f}\")\n",
    "\n",
    "words = model.words[:500]\n",
    "vectors = np.array([model.get_word_vector(w) for w in words])\n",
    "    \n",
    "# 创建TF变量\n",
    "embedding_var = tf.Variable(vectors, name=f'fasttext_{len(words)}d')\n",
    "    \n",
    "# 保存模型和metadata\n",
    "log_dir = 'fasttext_vis'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "with open(f'{log_dir}/metadata.tsv', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Word\\tFrequency\\n\")\n",
    "    for i, word in enumerate(words):\n",
    "        f.write(f\"{word}\\t{i+1}\\n\")  # 用序号模拟词频\n",
    "\n",
    "# 保存embedding\n",
    "checkpoint = tf.train.Checkpoint(embedding=embedding_var)\n",
    "checkpoint.save(os.path.join(log_dir, 'embedding.ckpt'))\n",
    "\n",
    "# print(f\"\\n启动TensorBoard查看:\\ntensorboard --logdir {log_dir}\\n\")\n",
    "# os.system(\"tensorboard --logdir fasttext_vis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "data_path = 'data.txt'\n",
    "word_pairs = [(\"科技\", \"创新\"), (\"手机\", \"电脑\"), (\"艺术\", \"科学\")]\n",
    "\n",
    "# 从文件加载分词后的数据（每行已分词，用空格分隔）\n",
    "sentences = []\n",
    "with open(data_path, 'r', encoding='utf-8') as fs:\n",
    "    sentences = [line.strip().split() for line in fs]\n",
    "\n",
    "# 训练Word2Vec模型\n",
    "model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,    # 向量维度\n",
    "    window=5,           # 上下文窗口\n",
    "    min_count=5,        # 最小词频\n",
    "    sg=1,               # 1=skipgram, 0=CBOW\n",
    "    epochs=50           # 迭代次数\n",
    ")\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# 计算词语相似度（与FastText API一致）\n",
    "for w1, w2 in word_pairs:\n",
    "    sim = model.wv.similarity(w1, w2)  # 直接调用gensim的相似度方法\n",
    "    print(f\"{w1} vs {w2}: {sim:.4f}\")\n",
    "\n",
    "# 获取词表和向量\n",
    "words = list(model.wv.index_to_key)[:500]\n",
    "vectors = np.array([model.wv[word] for word in words])\n",
    "\n",
    "# 保存模型和metadata\n",
    "log_dir = 'fasttext_vis'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "# 1. 保存词向量为TF变量\n",
    "embedding_var = tf.Variable(vectors, name='word2vec_embedding')\n",
    "# 2. 生成Metadata（词+自定义属性）\n",
    "with open(os.path.join(log_dir, 'metadata.tsv'), 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Word\\tFrequency\\n\")\n",
    "    for i, word in enumerate(words):\n",
    "        f.write(f\"{word}\\t{i+1}\\n\")\n",
    "# 3. 保存向量和配置\n",
    "checkpoint = tf.train.Checkpoint(embedding=embedding_var)\n",
    "checkpoint.save(os.path.join(log_dir, 'embedding.ckpt'))\n",
    "\n",
    "print(f\"\\n启动TensorBoard查看:\\ntensorboard --logdir {log_dir}\\n\")\n",
    "os.system(\"tensorboard --logdir fasttext_vis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、使用课堂示例cooking.stackexchange.txt，使用fasttext训练文本分类模型。（选做：尝试使用Kaggle中的Fake News数据集训练文本分类模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "\n",
    "input_file = \"cooking.stackexchange.txt\"\n",
    "output_file = \"cooking.fasttext.txt\"\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileExistsError(f\"can not found file {input_file}\")\n",
    "\n",
    "texts = []\n",
    "with open(file=input_file,mode='r',encoding='utf-8') as fs:\n",
    "    for line in fs:\n",
    "        words = jieba.lcut(re.sub(r'[^\\w\\s]', '', line.strip()))\n",
    "        texts.append(' '.join(words))\n",
    "\n",
    "with open(file=output_file, mode='w', encoding='utf-8') as fs:\n",
    "    for text in texts:\n",
    "        fs.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# 训练模型（监督学习）\n",
    "model = fasttext.train_supervised(\n",
    "    input=\"cooking.stackexchange.txt\",\n",
    "    lr=0.1, \n",
    "    epoch=25,         \n",
    "    wordNgrams=2,     \n",
    "    dim=100,          \n",
    "    loss='softmax' \n",
    ")\n",
    "\n",
    "# 保存模型\n",
    "model.save_model(\"cooking_model.bin\")\n",
    "\n",
    "# 加载模型\n",
    "model = fasttext.load_model(\"cooking_model.bin\")\n",
    "\n",
    "# 预测单条文本\n",
    "text = \"Which backing dish is best to bake a banana bread ?\"\n",
    "print(model.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
