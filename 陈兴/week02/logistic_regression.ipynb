{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑回归:\n",
    "- 逻辑回归是一种二分类算法\n",
    "- 逻辑回归的假设函数是sigmoid函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2 0. ]\n",
      " [4.9 3.  1.4 0.2 0. ]\n",
      " [4.7 3.2 1.3 0.2 0. ]\n",
      " [4.6 3.1 1.5 0.2 0. ]\n",
      " [5.  3.6 1.4 0.2 0. ]\n",
      " [5.4 3.9 1.7 0.4 0. ]\n",
      " [4.6 3.4 1.4 0.3 0. ]\n",
      " [5.  3.4 1.5 0.2 0. ]\n",
      " [4.4 2.9 1.4 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [5.4 3.7 1.5 0.2 0. ]\n",
      " [4.8 3.4 1.6 0.2 0. ]\n",
      " [4.8 3.  1.4 0.1 0. ]\n",
      " [4.3 3.  1.1 0.1 0. ]\n",
      " [5.8 4.  1.2 0.2 0. ]\n",
      " [5.7 4.4 1.5 0.4 0. ]\n",
      " [5.4 3.9 1.3 0.4 0. ]\n",
      " [5.1 3.5 1.4 0.3 0. ]\n",
      " [5.7 3.8 1.7 0.3 0. ]\n",
      " [5.1 3.8 1.5 0.3 0. ]\n",
      " [5.4 3.4 1.7 0.2 0. ]\n",
      " [5.1 3.7 1.5 0.4 0. ]\n",
      " [4.6 3.6 1.  0.2 0. ]\n",
      " [5.1 3.3 1.7 0.5 0. ]\n",
      " [4.8 3.4 1.9 0.2 0. ]\n",
      " [5.  3.  1.6 0.2 0. ]\n",
      " [5.  3.4 1.6 0.4 0. ]\n",
      " [5.2 3.5 1.5 0.2 0. ]\n",
      " [5.2 3.4 1.4 0.2 0. ]\n",
      " [4.7 3.2 1.6 0.2 0. ]\n",
      " [4.8 3.1 1.6 0.2 0. ]\n",
      " [5.4 3.4 1.5 0.4 0. ]\n",
      " [5.2 4.1 1.5 0.1 0. ]\n",
      " [5.5 4.2 1.4 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.2 0. ]\n",
      " [5.  3.2 1.2 0.2 0. ]\n",
      " [5.5 3.5 1.3 0.2 0. ]\n",
      " [4.9 3.6 1.4 0.1 0. ]\n",
      " [4.4 3.  1.3 0.2 0. ]\n",
      " [5.1 3.4 1.5 0.2 0. ]\n",
      " [5.  3.5 1.3 0.3 0. ]\n",
      " [4.5 2.3 1.3 0.3 0. ]\n",
      " [4.4 3.2 1.3 0.2 0. ]\n",
      " [5.  3.5 1.6 0.6 0. ]\n",
      " [5.1 3.8 1.9 0.4 0. ]\n",
      " [4.8 3.  1.4 0.3 0. ]\n",
      " [5.1 3.8 1.6 0.2 0. ]\n",
      " [4.6 3.2 1.4 0.2 0. ]\n",
      " [5.3 3.7 1.5 0.2 0. ]\n",
      " [5.  3.3 1.4 0.2 0. ]\n",
      " [7.  3.2 4.7 1.4 1. ]\n",
      " [6.4 3.2 4.5 1.5 1. ]\n",
      " [6.9 3.1 4.9 1.5 1. ]\n",
      " [5.5 2.3 4.  1.3 1. ]\n",
      " [6.5 2.8 4.6 1.5 1. ]\n",
      " [5.7 2.8 4.5 1.3 1. ]\n",
      " [6.3 3.3 4.7 1.6 1. ]\n",
      " [4.9 2.4 3.3 1.  1. ]\n",
      " [6.6 2.9 4.6 1.3 1. ]\n",
      " [5.2 2.7 3.9 1.4 1. ]\n",
      " [5.  2.  3.5 1.  1. ]\n",
      " [5.9 3.  4.2 1.5 1. ]\n",
      " [6.  2.2 4.  1.  1. ]\n",
      " [6.1 2.9 4.7 1.4 1. ]\n",
      " [5.6 2.9 3.6 1.3 1. ]\n",
      " [6.7 3.1 4.4 1.4 1. ]\n",
      " [5.6 3.  4.5 1.5 1. ]\n",
      " [5.8 2.7 4.1 1.  1. ]\n",
      " [6.2 2.2 4.5 1.5 1. ]\n",
      " [5.6 2.5 3.9 1.1 1. ]\n",
      " [5.9 3.2 4.8 1.8 1. ]\n",
      " [6.1 2.8 4.  1.3 1. ]\n",
      " [6.3 2.5 4.9 1.5 1. ]\n",
      " [6.1 2.8 4.7 1.2 1. ]\n",
      " [6.4 2.9 4.3 1.3 1. ]\n",
      " [6.6 3.  4.4 1.4 1. ]\n",
      " [6.8 2.8 4.8 1.4 1. ]\n",
      " [6.7 3.  5.  1.7 1. ]\n",
      " [6.  2.9 4.5 1.5 1. ]\n",
      " [5.7 2.6 3.5 1.  1. ]\n",
      " [5.5 2.4 3.8 1.1 1. ]\n",
      " [5.5 2.4 3.7 1.  1. ]\n",
      " [5.8 2.7 3.9 1.2 1. ]\n",
      " [6.  2.7 5.1 1.6 1. ]\n",
      " [5.4 3.  4.5 1.5 1. ]\n",
      " [6.  3.4 4.5 1.6 1. ]\n",
      " [6.7 3.1 4.7 1.5 1. ]\n",
      " [6.3 2.3 4.4 1.3 1. ]\n",
      " [5.6 3.  4.1 1.3 1. ]\n",
      " [5.5 2.5 4.  1.3 1. ]\n",
      " [5.5 2.6 4.4 1.2 1. ]\n",
      " [6.1 3.  4.6 1.4 1. ]\n",
      " [5.8 2.6 4.  1.2 1. ]\n",
      " [5.  2.3 3.3 1.  1. ]\n",
      " [5.6 2.7 4.2 1.3 1. ]\n",
      " [5.7 3.  4.2 1.2 1. ]\n",
      " [5.7 2.9 4.2 1.3 1. ]\n",
      " [6.2 2.9 4.3 1.3 1. ]\n",
      " [5.1 2.5 3.  1.1 1. ]\n",
      " [5.7 2.8 4.1 1.3 1. ]\n",
      " [6.3 3.3 6.  2.5 2. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [7.1 3.  5.9 2.1 2. ]\n",
      " [6.3 2.9 5.6 1.8 2. ]\n",
      " [6.5 3.  5.8 2.2 2. ]\n",
      " [7.6 3.  6.6 2.1 2. ]\n",
      " [4.9 2.5 4.5 1.7 2. ]\n",
      " [7.3 2.9 6.3 1.8 2. ]\n",
      " [6.7 2.5 5.8 1.8 2. ]\n",
      " [7.2 3.6 6.1 2.5 2. ]\n",
      " [6.5 3.2 5.1 2.  2. ]\n",
      " [6.4 2.7 5.3 1.9 2. ]\n",
      " [6.8 3.  5.5 2.1 2. ]\n",
      " [5.7 2.5 5.  2.  2. ]\n",
      " [5.8 2.8 5.1 2.4 2. ]\n",
      " [6.4 3.2 5.3 2.3 2. ]\n",
      " [6.5 3.  5.5 1.8 2. ]\n",
      " [7.7 3.8 6.7 2.2 2. ]\n",
      " [7.7 2.6 6.9 2.3 2. ]\n",
      " [6.  2.2 5.  1.5 2. ]\n",
      " [6.9 3.2 5.7 2.3 2. ]\n",
      " [5.6 2.8 4.9 2.  2. ]\n",
      " [7.7 2.8 6.7 2.  2. ]\n",
      " [6.3 2.7 4.9 1.8 2. ]\n",
      " [6.7 3.3 5.7 2.1 2. ]\n",
      " [7.2 3.2 6.  1.8 2. ]\n",
      " [6.2 2.8 4.8 1.8 2. ]\n",
      " [6.1 3.  4.9 1.8 2. ]\n",
      " [6.4 2.8 5.6 2.1 2. ]\n",
      " [7.2 3.  5.8 1.6 2. ]\n",
      " [7.4 2.8 6.1 1.9 2. ]\n",
      " [7.9 3.8 6.4 2.  2. ]\n",
      " [6.4 2.8 5.6 2.2 2. ]\n",
      " [6.3 2.8 5.1 1.5 2. ]\n",
      " [6.1 2.6 5.6 1.4 2. ]\n",
      " [7.7 3.  6.1 2.3 2. ]\n",
      " [6.3 3.4 5.6 2.4 2. ]\n",
      " [6.4 3.1 5.5 1.8 2. ]\n",
      " [6.  3.  4.8 1.8 2. ]\n",
      " [6.9 3.1 5.4 2.1 2. ]\n",
      " [6.7 3.1 5.6 2.4 2. ]\n",
      " [6.9 3.1 5.1 2.3 2. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [6.8 3.2 5.9 2.3 2. ]\n",
      " [6.7 3.3 5.7 2.5 2. ]\n",
      " [6.7 3.  5.2 2.3 2. ]\n",
      " [6.3 2.5 5.  1.9 2. ]\n",
      " [6.5 3.  5.2 2.  2. ]\n",
      " [6.2 3.4 5.4 2.3 2. ]\n",
      " [5.9 3.  5.1 1.8 2. ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 作业:\n",
    "# 使用 sklearn 数据集训练逻辑逻辑回归模型  \n",
    "# 用datasets中的 iris(鸢尾花) 数据集合 使用numpy进行模型运算\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 第一步: 数据生成\n",
    "# X 的含义是: 花萼长度 花萼宽度 花瓣长度 花瓣宽度\n",
    "# y 的含义是: 花的种类, 取值 0 1 2 分别代表 山鸢尾 变色鸢尾 维吉尼亚鸢尾\n",
    "# 数据集中有 150 个样本, 每个样本有 4 个特征(X), 共 150 个标签\n",
    "X,y = load_iris(return_X_y = True) # return_X_y = True时，直接返回特征矩阵X和标签向量y的元组（均为numpy数组）\n",
    "\n",
    "# 方便查看: 把X的行和y的列合并成一个矩阵打印出来, 前50行是山鸢尾, 50-100行是变色鸢尾, 100-150行是维吉尼亚鸢尾\n",
    "print(np.column_stack((X,y)))\n",
    "\n",
    "# 只取前100个样本, 因为只有前100个样本的标签是 山鸢尾 变色鸢尾. 这样就变成了一个二分类问题\n",
    "X = X[:100]\n",
    "y = y[:100]\n",
    "# 将数据集按 7:3 的比例划分为 训练集和测试集\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.47356101 -0.07828081  0.14770477  1.50071132]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#第二步: 模型参数设定\n",
    "\n",
    "# 逻辑回归模型的权重初始化操作\n",
    "# 权重参数: 随机初始化 4个特征 4个权重\n",
    "theta = np.random.randn(1,4) \n",
    "print(theta)\n",
    "\n",
    "# 偏差参数: 初始化为0\n",
    "bias = 0\n",
    "# 超参数 \n",
    "# 学习率: 控制模型参数更新的步长\n",
    "learn_rate = 1e-2 \n",
    "# 训练轮数: 控制模型训练的次数\n",
    "epochs = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 第三步: 模型的运算 逻辑回归模型的运算 两步得到预测值y_hat\n",
    "\n",
    "# 前向传播: 计算预测值y_hat\n",
    "# X是输入的特征矩阵, theta是权重向量, bias是偏置\n",
    "def forward(X,theta,bias):\n",
    "    # 计算线性组合, 作为sigmoid函数的z值\n",
    "    # z = torch.matmul(theta,X.T) + bias\n",
    "    z = np.dot(theta,X.T) + bias \n",
    "    \n",
    "    # 用sigmoid函数转换为概率\n",
    "    # y_hat 是预测值\n",
    "    y_hat = 1 / (1 + np.exp(-z))\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:3.939567606389347， acc:0.4857142857142857\n",
      "epoch:100, loss:0.2651995931993564， acc:1.0\n",
      "epoch:200, loss:0.1961759457180512， acc:1.0\n",
      "epoch:300, loss:0.15475586067638344， acc:1.0\n",
      "epoch:400, loss:0.12747229677217214， acc:1.0\n",
      "epoch:500, loss:0.10825944197720956， acc:1.0\n",
      "epoch:600, loss:0.09404431863474316， acc:1.0\n",
      "epoch:700, loss:0.08312201762375289， acc:1.0\n",
      "epoch:800, loss:0.07447703138787294， acc:1.0\n",
      "epoch:900, loss:0.06746931689037737， acc:1.0\n",
      "epoch:1000, loss:0.061676554333899236， acc:1.0\n",
      "epoch:1100, loss:0.05680934324753699， acc:1.0\n",
      "epoch:1200, loss:0.05266298355988233， acc:1.0\n",
      "epoch:1300, loss:0.04908873812114755， acc:1.0\n",
      "epoch:1400, loss:0.04597600698880324， acc:1.0\n",
      "epoch:1500, loss:0.043240883515267454， acc:1.0\n",
      "epoch:1600, loss:0.04081858490956803， acc:1.0\n",
      "epoch:1700, loss:0.03865831291692113， acc:1.0\n",
      "epoch:1800, loss:0.0367196829809176， acc:1.0\n",
      "epoch:1900, loss:0.03497019177080126， acc:1.0\n",
      "epoch:2000, loss:0.03338338785570653， acc:1.0\n",
      "epoch:2100, loss:0.03193752829048335， acc:1.0\n",
      "epoch:2200, loss:0.03061457719672984， acc:1.0\n",
      "epoch:2300, loss:0.029399449075908513， acc:1.0\n",
      "epoch:2400, loss:0.028279429919844994， acc:1.0\n",
      "epoch:2500, loss:0.02724372928797771， acc:1.0\n",
      "epoch:2600, loss:0.026283130086850737， acc:1.0\n",
      "epoch:2700, loss:0.025389712092555864， acc:1.0\n",
      "epoch:2800, loss:0.024556631736192394， acc:1.0\n",
      "epoch:2900, loss:0.023777945247243165， acc:1.0\n",
      "模型训练完毕:  [[ 0.04374485 -1.93143241  1.59697795  2.19810631]] -0.42629930636945146\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 第四步: loss 函数\n",
    "# 损失函数的作用: 用于评估模型的预测能力, 损失函数的值越小 模型的预测能力越好\n",
    "# 损失函数: 预测值和真实值的差距越大 损失函数的值越大 \n",
    "def loss(y_hat,y): \n",
    "    #y_hat是预测值 y是真实值\n",
    "    #y_hat和y都是向量 对应元素相乘\n",
    "    \n",
    "    # 防止log(0)\n",
    "    e = 1e-8 \n",
    "    \n",
    "    # 计算损失: \n",
    "    # 这里使用交叉熵损失函数(Cross Entropy Loss)作为逻辑回归的损失函数 (最小化交叉熵等价于最大化对数似然函数)\n",
    "    return -y * np.log(y_hat + e) - (1-y) * np.log(1 - y_hat + e)\n",
    "\n",
    "# 第五步: 梯度下降\n",
    "def compute_gradients(X,y,y_hat):\n",
    "    # X是训练数据 y是真实标签 y_hat是预测标签\n",
    "    # y_hat 和 y 都是向量 对应元素相乘\n",
    "    \n",
    "    # 样本数量: 70 组数据\n",
    "    m = X.shape[0] \n",
    "    # 计算梯度:\n",
    "    # 70组数据同时求出梯度\n",
    "    delta_theta = np.dot((y_hat - y), X) / m \n",
    "    # 70组数据同时求出偏置项梯度\n",
    "    delta_bias = np.mean(y_hat - y) \n",
    "    \n",
    "    return delta_theta,delta_bias #返回梯度\n",
    "\n",
    "# 第六步: 模型训练\n",
    "for i in range(epochs):\n",
    "    #前向传播\n",
    "    y_hat = forward(X_train,theta,bias) \n",
    "    #计算损失\n",
    "    loss_val = loss(y_hat,y_train) \n",
    "    #计算梯度\n",
    "    delta_theta,delta_bias = compute_gradients(X_train,y_train,y_hat) \n",
    "    \n",
    "    \n",
    "    # 更新theta\n",
    "    theta = theta - learn_rate * delta_theta \n",
    "    # 更新偏置项\n",
    "    bias = bias - learn_rate * delta_bias \n",
    "    # 每100次迭代打印一次损失\n",
    "    if i % 100 == 0:\n",
    "        # 计算准确率\n",
    "        acc = np.mean([np.round(y_hat) == y_train])\n",
    "        # 打印损失: 损失是一个向量 取平均值\n",
    "        print(f\"epoch:{i}, loss:{np.mean(loss_val)}， acc:{acc}\")\n",
    "\n",
    "# 模型训练完毕\n",
    "#保存 模型训练好的模型参数 到文件中, 方便后续使用\n",
    "print(\"模型训练完毕: \",theta,bias)\n",
    "np.savez('logistic_regression_model.npz',theta,bias) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:5, x:[[5.1 3.4 1.5 0.2]], y_true:0, predict:0.0\n",
      "idx:5, x:[[5.1 3.4 1.5 0.2]\n",
      " [4.6 3.6 1.  0.2]], y_true:[0 0], predict:[[0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 测试上面训练的模型\n",
    "#模型推理\n",
    "\n",
    "# 修改后的测试代码\n",
    "# 重新加载原始数据（确保数据维度正确）\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X = X[:100]  # 保持二维结构 (100,4)\n",
    "y = y[:100]   # 保持一维结构 (100,)\n",
    "\n",
    "# 重新划分数据集\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=42)\n",
    "\n",
    "# 加载模型参数\n",
    "weights = np.load('logistic_regression_model.npz')\n",
    "\n",
    "# 测试单个样本时需要保持二维结构\n",
    "idx = np.random.randint(len(X_test))\n",
    "x = X_test[idx:idx+1] \n",
    "y_true = y_test[idx:idx+1]\n",
    "\n",
    "# weights['arr_0'] 是 theta, weights['arr_1'] 是 bias\n",
    "theta, bias = weights['arr_0'], weights['arr_1']\n",
    "predict = np.round(forward(x, theta, bias)) \n",
    "\n",
    "print(f'idx:{idx}, x:{x}, y_true:{y_true[0]}, predict:{predict[0][0]}')\n",
    "\n",
    "# 测试多个样本时\n",
    "x = X_test[idx:idx+2] \n",
    "y_true = y_test[idx:idx+2]\n",
    "theta, bias = weights['arr_0'], weights['arr_1']\n",
    "predict = np.round(forward(x, theta, bias)) \n",
    "print(f'idx:{idx}, x:{x}, y_true:{y_true}, predict:{predict}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
