#Pytorch实现逻辑回归
#1.数据准备，初始化参数
#2.前向计算
#3.计算损失
#4.计算梯度
#5.更新参数
#6.重复2-5步骤，观察损失函数值，调整学习率

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np
import pickle
# 读取iris数据集
X,y = load_iris(return_X_y=True)
# 截取2分类数据
X = X[:100]
y = y[:100]
# 拆分训练和测试集
train_X,test_X, train_y,test_y = train_test_split(X,y, test_size=0.2, shuffle=True)
print(train_X)
print(train_y)
# 初始化参数模型参数
# theta = np.zeros((1,4))
theta = np.random.randn(1, 4)
# theta = np.random.randn(4,1)
bias = 0
# 学习率
lr = 1e-3
# 模型训练的轮数
epoch = 5000
# 前向计算
def forward(x, theta, bias):
 # linear
 z = np.dot(theta,x.T) + bias
 # z = np.dot(theta.T,x.T) + bias
 # sigmoid
 y_hat = 1 / (1 + np.exp(-z))
 return y_hat
# 损失函数
def loss_function(y, y_hat):
 e = 1e-8 # 防⽌y_hat计算值为0，添加的极⼩值epsilon
 return - y * np.log(y_hat + e) - (1 - y) * np.log(1 - y_hat + e)
# 计算梯度
def calc_gradient(x,y,y_hat):
 m = x.shape[-1]
 delta_w = np.dot(y_hat-y,x)/m
 delta_b = np.mean(y_hat-y)
 return delta_w, delta_b
for i in range(epoch):
 #正向
 y_hat = forward(train_X, theta, bias)
 # 计算损失
 loss = np.mean(loss_function(train_y, y_hat))
 if i % 100 == 0:
     print('step:', i, 'loss:', loss)
 # 梯度下降
 dw, db = calc_gradient(train_X, train_y, y_hat)
 # 更新参数
 # theta -= lr * dw.T
 theta -= lr * dw
 bias -= lr * db
# 测试模型
idx = np.random.randint(len(test_X))
x = test_X[idx]
y = test_y[idx]


def predict(x):
    pred = forward(x, theta, bias)[0]
    if pred > 0.5:
        return 1
    else:
        return 0


pred = predict(x)
print(f'预测值：{pred} 真实值：{y}')

# 输出值
# [[7.  3.2 4.7 1.4]
#  [6.  2.2 4.  1. ]
#  [4.9 3.1 1.5 0.2]
#  [6.3 2.5 4.9 1.5]
#  [4.8 3.  1.4 0.3]
#  [5.4 3.4 1.7 0.2]
#  [5.  3.4 1.6 0.4]
#  [5.6 3.  4.1 1.3]
#  [4.5 2.3 1.3 0.3]
#  [5.9 3.  4.2 1.5]
#  [5.  3.5 1.3 0.3]
#  [5.4 3.4 1.5 0.4]
#  [5.5 2.3 4.  1.3]
#  [5.7 2.6 3.5 1. ]
#  [4.4 2.9 1.4 0.2]
#  [5.2 3.4 1.4 0.2]
#  [4.9 2.4 3.3 1. ]
#  [5.2 4.1 1.5 0.1]
#  [5.1 3.5 1.4 0.2]
#  [6.2 2.9 4.3 1.3]
#  [5.4 3.9 1.3 0.4]
#  [5.  3.6 1.4 0.2]
#  [6.6 2.9 4.6 1.3]
#  [5.8 4.  1.2 0.2]
#  [6.7 3.  5.  1.7]
#  [4.8 3.  1.4 0.1]
#  [4.7 3.2 1.6 0.2]
#  [5.1 3.4 1.5 0.2]
#  [4.9 3.6 1.4 0.1]
#  [5.5 3.5 1.3 0.2]
#  [5.5 2.5 4.  1.3]
#  [6.3 3.3 4.7 1.6]
#  [5.7 3.8 1.7 0.3]
#  [5.8 2.6 4.  1.2]
#  [6.6 3.  4.4 1.4]
#  [6.9 3.1 4.9 1.5]
#  [6.4 3.2 4.5 1.5]
#  [5.4 3.  4.5 1.5]
#  [4.6 3.6 1.  0.2]
#  [5.9 3.2 4.8 1.8]
#  [5.  3.3 1.4 0.2]
#  [4.6 3.1 1.5 0.2]
#  [5.1 2.5 3.  1.1]
#  [5.1 3.8 1.5 0.3]
#  [4.7 3.2 1.3 0.2]
#  [6.7 3.1 4.7 1.5]
#  [6.5 2.8 4.6 1.5]
#  [5.7 4.4 1.5 0.4]
#  [5.5 2.4 3.7 1. ]
#  [5.3 3.7 1.5 0.2]
#  [6.2 2.2 4.5 1.5]
#  [4.6 3.4 1.4 0.3]
#  [5.  3.5 1.6 0.6]
#  [4.3 3.  1.1 0.1]
#  [5.  2.3 3.3 1. ]
#  [5.8 2.7 4.1 1. ]
#  [5.6 2.5 3.9 1.1]
#  [5.7 2.8 4.1 1.3]
#  [5.  3.4 1.5 0.2]
#  [4.9 3.1 1.5 0.1]
#  [5.7 2.9 4.2 1.3]
#  [5.1 3.8 1.9 0.4]
#  [5.4 3.7 1.5 0.2]
#  [6.7 3.1 4.4 1.4]
#  [6.1 3.  4.6 1.4]
#  [6.  2.9 4.5 1.5]
#  [5.8 2.7 3.9 1.2]
#  [4.8 3.4 1.6 0.2]
#  [5.  3.2 1.2 0.2]
#  [5.4 3.9 1.7 0.4]
#  [5.6 2.7 4.2 1.3]
#  [5.5 2.6 4.4 1.2]
#  [6.1 2.8 4.  1.3]
#  [5.6 2.9 3.6 1.3]
#  [6.  2.7 5.1 1.6]
#  [4.4 3.2 1.3 0.2]
#  [5.5 4.2 1.4 0.2]
#  [4.8 3.1 1.6 0.2]
#  [5.1 3.5 1.4 0.3]
#  [4.9 3.  1.4 0.2]]
# [1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1
#  1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 1 1 1 1
#  1 0 0 0 0 0]
# step: 0 loss: 1.4239137918172902
# step: 100 loss: 0.13500737012389866
# step: 200 loss: 0.09834347627482261
# step: 300 loss: 0.0773863647127121
# step: 400 loss: 0.06386664386309163
# step: 500 loss: 0.054430835867394944
# step: 600 loss: 0.047472381237139115
# step: 700 loss: 0.042128021389122625
# step: 800 loss: 0.037893338979970735
# step: 900 loss: 0.034454139778034805
# step: 1000 loss: 0.03160460980787736
# step: 1100 loss: 0.029204330209064037
# step: 1200 loss: 0.027154227628140893
# step: 1300 loss: 0.025382403109136753
# step: 1400 loss: 0.023835412507703164
# step: 1500 loss: 0.022472700696082783
# step: 1600 loss: 0.021262935078322408
# step: 1700 loss: 0.020181523244340575
# step: 1800 loss: 0.019208891506094134
# step: 1900 loss: 0.018329265496385114
# step: 2000 loss: 0.017529789934873207
# step: 2100 loss: 0.016799882378760216
# step: 2200 loss: 0.016130751467259896
# step: 2300 loss: 0.015515032792987882
# step: 2400 loss: 0.014946510196602297
# step: 2500 loss: 0.014419899978056056
# step: 2600 loss: 0.013930682049403158
# step: 2700 loss: 0.013474966527995963
# step: 2800 loss: 0.013049387380953553
# step: 2900 loss: 0.01265101692746599
# step: 3000 loss: 0.012277296575121704
# step: 3100 loss: 0.01192598030225146
# step: 3200 loss: 0.01159508822952601
# step: 3300 loss: 0.011282868238843054
# step: 3400 loss: 0.010987764056754673
# step: 3500 loss: 0.010708388565875946
# step: 3600 loss: 0.010443501370966007
# step: 3700 loss: 0.010191989848193337
# step: 3800 loss: 0.009952853062014835
# step: 3900 loss: 0.009725188055429539
# step: 4000 loss: 0.009508178114433044
# step: 4100 loss: 0.009301082682469385
# step: 4200 loss: 0.009103228660163923
# step: 4300 loss: 0.00891400287310018
# step: 4400 loss: 0.008732845528508181
# step: 4500 loss: 0.008559244512475424
# step: 4600 loss: 0.008392730404222141
# step: 4700 loss: 0.008232872104294856
# step: 4800 loss: 0.008079272990159694
# step: 4900 loss: 0.007931567526346738
# 预测值：0 真实值：0
