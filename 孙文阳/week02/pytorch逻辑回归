# %%
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd

# %%
#1.加载花的数据集，return_X_y=True只返回X和y的集合
X,y = load_iris(return_X_y=True)

X=X[:100]
y=y[:100]
# print(X)
# print(y)
# print(X.shape)
# print(y.shape)

#数据拆分把X,y分成两份测试集和训练集,(X,y,test_size=0.2)%20做测试样本
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)
print(X_train.shape)
print(y_train.shape)

# %%
#权重参数 
theta = np.random.randn(1,4)#对应样本（load_iris）4个特征
bias=0#偏差
#超参数
lr=0.1  #学习率
epochs = 2000 #训练次数

# %%
#2.模型运算函数
def forward(x,theta,bias):       #带入参数
    #线性运算
    z = np.dot(theta,x.T)+bias  #样本所有的值shape（80，4）
    #sigmoid
    y_hat = 1/(1+np.exp(-z))    #概率值
    return y_hat

#3.计算损失函数
def loss(y,y_hat):
    e = 1e-5
    return - y * np.log (y_hat + e) - (1-y) * np.log (1-y_hat + e)

#4.计算梯度
def calc_gradient(x,y,y_hat):
    m = x.shape[-1]
    delta_theta = np.dot((y_hat -y),x) /m
    delta_bias= np.mean(y_hat-y)
    return delta_theta,delta_bias

#5.模型循环训练
for i in range (epochs):
    #前向计算
    y_hat = forward(X_train,theta,bias)
    #计算损失
    loss_val = loss(y_train,y_hat)
    #计算梯度
    delta_theta,delta_bias = calc_gradient(X_train,y_train,y_hat)
    #更新参数
    theta = theta - lr * delta_theta
    bias = bias - lr * delta_bias 
    if i % 100 == 0:
    # 计算准确率
        acc = np.mean(np.round(y_hat) == y_train)
        print(f"epoch:{i},loss:{np.mean(loss_val)},acc:{acc}")
# print(theta)
# print(bias)
