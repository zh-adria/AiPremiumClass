# 神经网络与线性回归基础

## 神经网络概述

人工神经网络模仿生物神经网络结构和功能，是深度学习的基础与主流架构。学习神经网络常从逻辑回归入手，进而认识神经元和网络训练。

## 线性回归

### 简单线性回归

旨在找到线性函数拟合数据，使预测值与真实值误差最小。简单线性回归模型为：
\[ y = \beta_{0} + \beta_{1}x + \epsilon \]
用最小二乘法估计参数，通过对误差平方和求偏导并令其为 0，得出参数估计值。

### 多元线性回归

多元线性回归模型表达式为：
\[ y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \cdots + \beta_{p}x_{p} + \epsilon \]
同样用最小二乘法，借助矩阵运算得到参数估计值。

## 逻辑回归核心知识

### 逻辑回归原理

将线性回归模型输出映射到 [0, 1] 区间，转化为概率值，使用 sigmoid 函数进行转换。
假设函数：
\[ h_{\theta}(X) = \frac{1}{1 + e^{-z}} = P(Y = 1|X;\theta) \]
\[ P(Y = 0|X;\theta) = 1 - h_{\theta}(X) \]

### 最大似然估计与损失函数

最大似然估计用于寻找最符合观测数据的概率分布。逻辑回归通过最大似然估计，结合伯努利分布概率质量函数定义似然函数，取对数并添加负号得到损失函数：
\[ J(\theta) = -\sum_{i=1}^{m} \left[ Y \log(\hat{Y}) + (1 - Y) \log(1 - \hat{Y}) \right] \]

## 梯度下降法及其应用

### 梯度下降原理

梯度下降是基于搜索的优化方法，用于最小化损失函数。其思想类似下山，沿梯度反方向（函数下降最快方向）迭代更新参数，逐步逼近损失函数最小值，但可能陷入局部最小值。

### 梯度与学习率

梯度是函数在某点的向量，方向与最大方向导数一致，模为方向导数最大值。对于多元函数，需对各参数求偏导确定梯度方向。学习率是超参数，影响梯度下降步长，合适的学习率可使损失函数在合适时间收敛到局部最小值，过大或过小都会影响收敛效果。

### 梯度下降模拟与可视化

通过代码模拟梯度下降过程，绘制损失函数图像，观察不同学习率下参数更新情况，直观展示学习率对梯度下降的影响。

## 逻辑回归模型构建与训练

### Numpy 实现

- **数据准备**：使用 scikit-learn 生成数据集并拆分训练集和测试集，初始化权重参数和超参数。
- **模型运算（前向传播）**：计算预测值。
- **计算损失**：使用负对数损失函数。
- **计算梯度**：依据公式。
- **更新参数**：按梯度下降规则，不断迭代直至损失函数收敛。

### PyTorch 实现

- **数据准备**：创建张量，初始化参数。
- **前向运算**：借助 PyTorch 的线性层和 sigmoid 函数实现。
- **计算损失**：使用 `binary_cross_entropy` 函数。
- **计算梯度**：利用自动求导功能。
- **参数更新**：在关闭梯度计算跟踪下进行，代码相较于 Numpy 实现更简洁。