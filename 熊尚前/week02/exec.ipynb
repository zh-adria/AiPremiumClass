{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、使用sklearn数据集训练逻辑回归模型；\n",
    "# 2、调整学习率，样本数据拆分比率，观察训练结果；\n",
    "# 3、训练后模型参数保存到文件，在另一个代码中加载参数实现预测功能；\n",
    "# 4、总结逻辑回归运算及训练相关知识点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练结束，epoch:100001,loss:0.040618400575697725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "x,y = load_iris(return_X_y=True)\n",
    "# 取前100个数据\n",
    "x = x[:100]\n",
    "y = y[:100]\n",
    "# 数据拆分\n",
    "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.3,shuffle=True,random_state=123)\n",
    "# train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.5,shuffle=True,random_state=123)\n",
    "# train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.6,shuffle=True,random_state=123)\n",
    "\n",
    "# 初始化模型参数\n",
    "theta = np.random.randn(1,4) # 有4个特征\n",
    "bias = 0 # 截距\n",
    "lr = 1e-5 # 学习率\n",
    "epochs = 100000 # 模型训练次数\n",
    "\n",
    "# 模型运算\n",
    "def forward(x,theta,bias):\n",
    "    z = np.dot(theta,x.T)+bias # 线性回归\n",
    "    y_hat = 1/(1+np.exp(-z)) # 线性回归的输出作为sigmoid函数的输入，转换为0-1之间的概率值\n",
    "    return y_hat\n",
    "\n",
    "# 损失函数\n",
    "def loss(y,y_hat):\n",
    "    e = 1e-8 # 防止出现log0的情况\n",
    "    return -y*np.log(y_hat + e)-(1-y)*np.log(1-y_hat+e)\n",
    "\n",
    "# 计算梯度\n",
    "def calc_gradient(x,y,y_hat):\n",
    "    m = x.shape[-1] # 样本数量\n",
    "    delta_w = np.dot(y_hat-y,x)/m\n",
    "    delta_b = np.mean(y_hat-y)\n",
    "    return delta_w,delta_b\n",
    "\n",
    "# 模型训练\n",
    "flag = True\n",
    "i = 0\n",
    "while flag:\n",
    "    y_hat = forward(train_x,theta,bias)\n",
    "    l = np.mean(loss(train_y,y_hat))\n",
    "    delta_w,delta_b = calc_gradient(train_x,train_y,y_hat)\n",
    "    theta = theta - lr*delta_w # 跟新theta\n",
    "    bias = bias - lr*delta_b # 跟新bias\n",
    "    i += 1\n",
    "    if i > epochs or l < 1e-3:\n",
    "        flag = False\n",
    "        print(f'训练结束，epoch:{i},loss:{l}')\n",
    "\n",
    "# 将训练后的theta和bias保存到本地\n",
    "np.savez('result.npz',theta=theta,bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正确数量:30\n",
      "准确率:1.0\n"
     ]
    }
   ],
   "source": [
    "# 从文件中加载参数，用来测试\n",
    "import numpy as np\n",
    "\n",
    "# 从文件中加载数据\n",
    "result = np.load(\"result.npz\")\n",
    "# 提取theta和bias\n",
    "theta = result[\"theta\"]\n",
    "bias = result[\"bias\"]\n",
    "# print(\"theta:\",theta)\n",
    "# print(\"bias:\",bias)\n",
    "\n",
    "def predict(x):\n",
    "    y_hat = forward(x,theta,bias)[0]\n",
    "    return np.round(y_hat)\n",
    "count = 0\n",
    "for i in range(len(test_x)):\n",
    "    x = test_x[i]\n",
    "    y = test_y[i]\n",
    "    pred = predict(x)\n",
    "    # print(f'index:{i}，真实值:{y}，预测值:{pred}')\n",
    "    if pred == y:\n",
    "        count += 1\n",
    "print(f'正确数量:{count}')\n",
    "print(f'准确率:{count/len(test_x)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
