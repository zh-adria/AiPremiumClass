{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendation system\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import csv\n",
    "import jieba # 分词\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99665it [00:00, 307280.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# 清洗数据，有可能有断行\n",
    "fixed_file = open(\"doubanbook_top250_comments_fixed.txt\", \"w\", encoding = \"utf-8\")\n",
    "lines = [line for line in open(\"doubanbook_top250_comments.txt\", \"r\", encoding = \"utf-8\").readlines()]\n",
    "print(len(lines))\n",
    "for i, line in tqdm(enumerate(lines)):\n",
    "    #提取书名和评论，保存标题列\n",
    "    if i == 0:\n",
    "        fixed_file.write(line)\n",
    "        prev_line = ''\n",
    "        continue\n",
    "    # 判断当前的行书名是否等于上一行的书名, 如果不是要合并\n",
    "\n",
    "    terms = line.split(\"\\t\")\n",
    "    # book_name = terms[0]\n",
    "    # comment = terms[-1]\n",
    "    if line.split(\"\\t\")[0] == prev_line.split(\"\\t\")[0]:\n",
    "        if len(prev_line.split(\"\\t\")) == 6:\n",
    "            fixed_file.write(prev_line + '\\n')\n",
    "            prev_line = line.strip()\n",
    "        else:\n",
    "            prev_line = \"\"\n",
    "    else:\n",
    "        if len(terms) == 6:\n",
    "            prev_line = line.strip()\n",
    "        else:\n",
    "            prev_line += line.strip()\n",
    "\n",
    "fixed_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停用词表\n",
    "stop_words = [word.strip() for word in open(\"stopwords.txt\", 'r', encoding = 'utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tsv_data(file_path):\n",
    "    book_comments = {} # {book_name: \"评论词1+评论词2+...\"}\n",
    "    with open(file_path, 'r') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        for item in reader:\n",
    "            book_name = item['book']\n",
    "            comment = item['body']\n",
    "            comment_words = jieba.lcut(comment)\n",
    "            if book_name == \"\":\n",
    "                continue\n",
    "            book_comments[book_name] = book_comments.get(book_name, list()) # 如果书名不存在,就是返回空列表\n",
    "            book_comments[book_name].extend(comment_words)\n",
    "    return book_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_comments = load_tsv_data(\"doubanbook_top250_comments_fixed.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = list(book_comments.keys())\n",
    "# print(len(book_list))\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "# fit： 根据所有文档（书籍评论）计算词汇表和相应的 IDF 值。\n",
    "# transform： 将每个文档转换为对应的 TF-IDF 向量。\n",
    "# 组合说明： fit_transform 是 fit 和 transform 的结合，即一步完成模型参数的学习（计算 IDF）和数据转换，适合于一次性对训练数据进行处理。\n",
    "tfidf_matrix = vectorizer.fit_transform([' '.join(book_comments[book_name]) for book_name in book_list]) # 计算tfidf\n",
    "# print(tfidf_matrix.shape)\n",
    "# 这会返回一个对称的相似度矩阵，表示每对文档之间的“内容相似程度”\n",
    "similarities = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<1995-2005夏至未至>\t similarity: 0.12349318350286932\n",
      "<苏菲的世界>\t similarity: 0.12193950370527712\n",
      "<盗墓笔记>\t similarity: 0.10750471319253747\n",
      "<霍乱时期的爱情>\t similarity: 0.10006284467308349\n",
      "<恶意>\t similarity: 0.09818539603453101\n",
      "<长安乱>\t similarity: 0.07893026228621035\n",
      "<许三观卖血记>\t similarity: 0.07839213736458249\n",
      "<悲伤逆流成河>\t similarity: 0.07575044302166914\n",
      "<Harry Potter and the Deathly Hallows>\t similarity: 0.07465265480840053\n",
      "<1Q84 BOOK 1>\t similarity: 0.03454391961036781\n"
     ]
    }
   ],
   "source": [
    "book_id = book_list.index(\"天才在左 疯子在右\")\n",
    "recommend_book_ids = np.argsort(-similarities[book_id][:11])[1:] # 取前十名,去掉第一个自己本身, 加负号降序\n",
    "for i in recommend_book_ids:\n",
    "    print(f\"<{book_list[i]}>\\t similarity: {similarities[book_id][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25(comments, k1=5, b=0.75):\n",
    "    \"\"\"\n",
    "    BM25算法计算词权重\n",
    "    comments：分词后的文本列表（例如 [['我', '喜欢', '阅读'], ['这是', '一个', '示例']]）\n",
    "    k1, b：BM25公式中的可调参数，通常取 k1=1.2~2.0，b=0.75左右\n",
    "    \"\"\"\n",
    "    # 1. 计算每篇文档的长度\n",
    "    doc_lengths = []\n",
    "    for comment in comments:\n",
    "        doc_lengths.append(len(comment))\n",
    "\n",
    "    # 2. 统计词频：每篇文档中，各词出现的次数\n",
    "    doc_term_dict_list = []\n",
    "    for comment in comments:\n",
    "        doc_term_dict = {}\n",
    "        for word in comment:\n",
    "            doc_term_dict[word] = doc_term_dict.get(word, 0) + 1\n",
    "        doc_term_dict_list.append(doc_term_dict)\n",
    "\n",
    "    # 3. 统计所有文档中出现过的单词集合\n",
    "    unique_words = set()\n",
    "    for doc_term_dict in doc_term_dict_list:\n",
    "        unique_words |= set(doc_term_dict.keys())\n",
    "\n",
    "    # 4. 构建词典，并为每个单词分配索引\n",
    "    vocabulary = list(unique_words)\n",
    "    word_idx = {}\n",
    "    for idx, word in enumerate(vocabulary):\n",
    "        word_idx[word] = idx\n",
    "\n",
    "    # 5. 计算每个词在多少篇文档中出现（文档频次）\n",
    "    doc_freq = {}\n",
    "    for doc_term_dict in doc_term_dict_list:\n",
    "        for word in doc_term_dict.keys():\n",
    "            doc_freq[word] = doc_freq.get(word, 0) + 1\n",
    "\n",
    "    # 6. 计算所有文档的平均长度\n",
    "    avg_doc_len = sum(doc_lengths) / len(doc_lengths)\n",
    "\n",
    "    # 7. 初始化 BM25 矩阵\n",
    "    bm25_matrix = []\n",
    "    for _ in range(len(doc_term_dict_list)):\n",
    "        bm25_matrix.append([0] * len(vocabulary))\n",
    "\n",
    "    # 8. 计算 BM25 值\n",
    "    for i, doc_term_dict in enumerate(doc_term_dict_list):\n",
    "        for word, freq in doc_term_dict.items():\n",
    "            # IDF\n",
    "            idf = math.log((len(comments) - doc_freq[word] + 0.5) / (doc_freq[word] + 0.5))\n",
    "            # TF\n",
    "            tf = (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * (doc_lengths[i] / avg_doc_len)))\n",
    "            # BM25\n",
    "            bm25_matrix[i][word_idx[word]] = idf * tf\n",
    "\n",
    "    # 9. 补齐成相同长度（如果后续需要以矩阵形式处理）\n",
    "    max_length = max(len(row) for row in bm25_matrix)\n",
    "    final_bm25_matrix = []\n",
    "    for row in bm25_matrix:\n",
    "        final_bm25_matrix.append(row + [0] * (max_length - len(row)))\n",
    "    final_bm25_matrix = np.array(final_bm25_matrix)\n",
    "\n",
    "    return final_bm25_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_matrix = bm25([' '.join(book_comments[book_name]) for book_name in book_list])\n",
    "\n",
    "similarities_bm = cosine_similarity(bm25_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<恶意>\t similarity: 0.9678231503468048\n",
      "<苏菲的世界>\t similarity: 0.9653268008913644\n",
      "<霍乱时期的爱情>\t similarity: 0.9630073687833041\n",
      "<1Q84 BOOK 1>\t similarity: 0.9613670534381518\n",
      "<许三观卖血记>\t similarity: 0.9606248546173871\n",
      "<盗墓笔记>\t similarity: 0.9572816073170372\n",
      "<长安乱>\t similarity: 0.9526024034737062\n",
      "<1995-2005夏至未至>\t similarity: 0.9500788818977659\n",
      "<Harry Potter and the Deathly Hallows>\t similarity: 0.949401513584823\n",
      "<悲伤逆流成河>\t similarity: 0.8917140063251158\n"
     ]
    }
   ],
   "source": [
    "book_id = book_list.index(\"天才在左 疯子在右\")\n",
    "recommend_book_ids = np.argsort(-similarities_bm[book_id][:11])[1:] # 取前十名,去掉第一个自己本身, 加负号降序\n",
    "for i in recommend_book_ids:\n",
    "    print(f\"<{book_list[i]}>\\t similarity: {similarities_bm[book_id][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import jieba\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HLM.txt', 'r', encoding='gb2312', errors='replace') as f:\n",
    "    lines = f.read()\n",
    "# 分词处理\n",
    "with open('sparse.txt', 'w') as f:\n",
    "    f.write(' '.join(jieba.cut(lines)))\n",
    "# 仅支持空格分词的文件\n",
    "model = fasttext.train_unsupervised('sparse.txt', model = 'skipgram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7684757113456726, '李婶娘'), (0.7592969536781311, '薛姨妈'), (0.7405363321304321, '贾母素'), (0.7380984425544739, '李婶'), (0.7317023873329163, '贾母才'), (0.7302117943763733, '王二夫人'), (0.7285462617874146, '邢'), (0.7238101959228516, '王夫人'), (0.7182996869087219, '彼此'), (0.7164919972419739, '劝慰')]\n"
     ]
    }
   ],
   "source": [
    "# 获取词向量的类比\n",
    "print(model.get_analogies('贾母','宝玉', '宝钗'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = fasttext.train_supervised('cooking.stackexchange.txt', lr=0.01, epoch=10, dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__baking',), array([0.03588072]))\n"
     ]
    }
   ],
   "source": [
    "print(model_class.predict('Why not put knives in the dishwasher ?'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
