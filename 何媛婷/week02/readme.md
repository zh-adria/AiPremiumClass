# Pytorch 逻辑回归
#### 几个问题？
+ 什么是回归？回归就是向“平均”不断靠拢。
+ 为什么用线性回归表达？线性能够简单表达因果关系，非线性回归可以转为线性回归并进行研究。
+ 线性回归的主要目的是？解释现有的自变量对因变量的影响， 以及预测未来。

## 1. 线性回归
```text
线性回归就是 找到一个合适的函数来描述、整理数据，使得预测值和真实数值之间的误差尽可能小。线性回归的自变量越多，那越接近真实情况，但是解释这些参数影响将是很困难的事情。
```
### 参数估计
参数估计是确定线性函数的过程。
### 1.1 参数估计—简单线性回归
+ 模型表达式为 ：y = β0 + β1 x + e 
    其中 y 是因变量， x是⾃变量， β0 是截距， β1 是斜率，e 是误差项（服从均值为 0 的正态分布）

### 1.2  参数估计—多元线性回归
```
确定几个特定的变量之间是否存在某种关系，如果存在的话，需要找到合适的数学表达式表达这类关系。
```


## 2.逻辑回归
逻辑回归就是将线性回归模型映射为概率的模型。
把 实数空间的输出 映射到取值为[0,1] 的区间, 从⽽把模型的输出值转换为概率值。
⽽这其中的转换，我们使⽤的就是sigmoid函数
```python
def sigmoid(z):
	return 1 / (1 + np.exp(-z))
```
### 2.1 概率
概率是已知一些分布参数的情况下，预测结果
+ P(x∣θ):如果θ是已知确定的，X是变量，这个函数叫做概率函数(probability function)

### 2.2 似然
对预测结果所属概率分布的参数进行估计
+ P(x∣θ): 如果X是已知确定的,θ是变量，这个函数叫做似然函数(likelihood function)
+ 最⼤似然估计的⽬的：找到⼀个最符合当前观测数据的概率分布。
+ 对数似然函数: 对似然函数取对数
### 2.3 损失函数
假设函数（hypothesis）本质上是⼀个模型，⽤于描述⾃变量和因变量之间的映射关系；假设因变量与⾃变量之间存在线性关系

+ 梯度下降法的作⽤就是：最⼩化⼀个损失函数
### 3.自我总结，理解的步骤
#### 步骤一： 收集数据样本
#### 步骤二：确定自变量和因变量，做好数据清洗工作后，切分数据集；
#### 步骤三：构建回归函数，建立学习模型，预测系数。确定自变量和因变量之间的关系，找到和实际值最接近的系数
#### 步骤四：构建逻辑回归方法，将线性回归模型转为概率模型，逻辑回归其实是解决分类问题，比如分配到某个区间的概率。

