{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d55b2a",
   "metadata": {},
   "source": [
    "## 最小二乘法（LSE）\n",
    "- 缺点：\n",
    "1. 计算复杂\n",
    "2. 只适合简单的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a385f841",
   "metadata": {},
   "source": [
    "## 梯度下降法\n",
    "实际常用的模型\n",
    "sigmoid\n",
    "#### 最大似然（MLE）\n",
    "- 由于概率是小数，计算累乘很容易导致下溢出，通常取对数\n",
    "伯努利分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff637d14",
   "metadata": {},
   "source": [
    "## 似然函数\n",
    "### **1. 似然函数在机器学习中的应用**\n",
    "在机器学习中，**似然函数（Likelihood Function）**主要用于**模型参数估计**，特别是在**监督学习、概率模型、生成模型**等任务中。  \n",
    "\n",
    "它的核心作用：\n",
    "- 估计 **模型参数**，使得训练数据在该参数下出现的概率最大（**最大似然估计，MLE**）。\n",
    "- 评估 **模型的好坏**，通过**对数似然（log-likelihood）**衡量模型对数据的拟合程度。\n",
    "- 作为 **损失函数**，许多常见的损失函数（如交叉熵）可以看作是**似然函数的负对数**。\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 似然函数用于估计哪些模型参数？**\n",
    "在不同的模型中，似然函数用于估计不同的参数：\n",
    "- **线性回归（Linear Regression）**：估计**权重 \\( w \\)** 和 **偏置 \\( b \\)**。\n",
    "- **逻辑回归（Logistic Regression）**：估计**模型权重 \\( w \\)** 以最大化数据的正确分类概率。\n",
    "- **高斯混合模型（GMM）**：估计多个高斯分布的**均值、方差、混合系数**。\n",
    "- **隐马尔可夫模型（HMM）**：估计**状态转移概率、观测概率**。\n",
    "- **神经网络（NN）**：输出层可以看作是一个概率分布（如 softmax），可以用最大似然优化网络参数。\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 机器学习中的似然函数示例**\n",
    "#### **(1) 逻辑回归的似然函数**\n",
    "逻辑回归用于二分类问题，假设输出是一个**二项分布**：\n",
    "\\[\n",
    "P(y | x; w) = p^y (1 - p)^{(1 - y)}\n",
    "\\]\n",
    "其中：\n",
    "- \\( p = \\sigma(w^T x) \\) 是 **sigmoid** 激活的概率。\n",
    "- \\( y \\in \\{0,1\\} \\) 是真实标签。\n",
    "\n",
    "**似然函数**：\n",
    "\\[\n",
    "L(w) = \\prod_{i=1}^{n} P(y_i | x_i; w) = \\prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{(1 - y_i)}\n",
    "\\]\n",
    "**对数似然函数**：\n",
    "\\[\n",
    "\\log L(w) = \\sum_{i=1}^{n} \\left[ y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right]\n",
    "\\]\n",
    "**最大化对数似然** ＝ **最小化负对数似然**（= 交叉熵损失）：\n",
    "\\[\n",
    "J(w) = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right]\n",
    "\\]\n",
    "这就是逻辑回归的**交叉熵损失函数**，所以最大似然估计直接影响了模型的损失函数设计。\n",
    "\n",
    "---\n",
    "\n",
    "#### **(2) 线性回归的似然函数**\n",
    "假设误差项服从**正态分布**：\n",
    "\\[\n",
    "y_i = w^T x_i + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2)\n",
    "\\]\n",
    "则：\n",
    "\\[\n",
    "P(y_i | x_i; w) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y_i - w^T x_i)^2}{2\\sigma^2}}\n",
    "\\]\n",
    "**对数似然函数**：\n",
    "\\[\n",
    "\\log L(w) = -\\sum_{i=1}^{n} \\frac{(y_i - w^T x_i)^2}{2\\sigma^2} - \\frac{n}{2} \\log (2\\pi\\sigma^2)\n",
    "\\]\n",
    "最大化似然等价于最小化平方误差：\n",
    "\\[\n",
    "J(w) = \\sum_{i=1}^{n} (y_i - w^T x_i)^2\n",
    "\\]\n",
    "这就是**均方误差（MSE）**，说明**线性回归的损失函数其实也是最大似然的结果！**\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 机器学习输出的概率是否就是似然？**\n",
    "不完全是！但有一定联系：\n",
    "- **分类模型（如 Softmax 回归、神经网络）**\n",
    "  - 最终输出的是一个**概率分布**，表示输入属于每个类别的概率。\n",
    "  - 但这个概率本质上是**条件概率 \\( P(y|x) \\)**，它衡量的是**给定输入后属于某类的概率**。\n",
    "  - 似然函数 \\( L(\\theta) \\) 关注的是**给定参数后，数据出现的概率**，两者不完全一样。\n",
    "\n",
    "- **贝叶斯方法（如 Naïve Bayes）**\n",
    "  - 计算 \\( P(y|x) \\) 需要用**贝叶斯公式**：\n",
    "    \\[\n",
    "    P(y|x) = \\frac{P(x|y) P(y)}{P(x)}\n",
    "    \\]\n",
    "  - 其中 \\( P(x|y) \\) 就是**似然函数**，表示在 \\( y \\) 发生的情况下，\\( x \\) 发生的可能性。\n",
    "  - 所以，在贝叶斯分类器中，最终的输出概率与似然有更直接的关系。\n",
    "\n",
    "**总结**：\n",
    "- 机器学习模型的输出概率通常是**条件概率 \\( P(y|x) \\)**，并不直接等于似然。\n",
    "- 但很多模型（如 Naïve Bayes）会在计算过程中使用似然来估计这个概率。\n",
    "- 似然函数主要用于**训练过程**，通过最大似然估计参数。\n",
    "\n",
    "---\n",
    "\n",
    "### **5. 结论**\n",
    "✅ **似然函数在机器学习中主要用于参数估计**，如：\n",
    "  - 线性回归 → 估计 \\( w \\)（得到 MSE 损失）\n",
    "  - 逻辑回归 → 估计 \\( w \\)（得到交叉熵损失）\n",
    "  - GMM/HMM → 估计分布参数（EM 算法）\n",
    "  - 神经网络 → 通过 Softmax 归一化概率，并使用对数似然优化\n",
    "\n",
    "✅ **最大化似然函数 = 选择最可能生成数据的参数**\n",
    "  - 线性回归的**最小二乘法**，其实是 MLE 在**正态分布假设**下的结果。\n",
    "  - 逻辑回归的**交叉熵损失**，其实是 MLE 在**二项分布假设**下的结果。\n",
    "\n",
    "✅ **模型的输出概率不一定是似然**\n",
    "  - 在贝叶斯分类器中，**似然函数** \\( P(x|y) \\) 直接影响最终概率。\n",
    "  - 在神经网络和逻辑回归中，输出是 \\( P(y|x) \\)，而训练时用的是**最大似然原则**来优化参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1054c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
